A pca-based change detection framework for multidimensional data streams: Change detection in multidimensional data streams
Detecting changes in multidimensional data streams is an important and challenging task. In unsupervised change detection, changes are usually detected by comparing the distribution in a current (test) window with a reference window. It is thus essential to design divergence metrics and density estimators for comparing the data distributions, which are mostly done for univariate data. Detecting changes in multidimensional data streams brings difficulties to the density estimation and comparisons. In this paper, we propose a framework for detecting changes in multidimensional data streams based on principal component analysis, which is used for projecting data into a lower dimensional space, thus facilitating density estimation and change-score calculations. The proposed framework also has advantages over existing approaches by reducing computational costs with an efficient density estimator, promoting the change-score calculation by introducing effective divergence metrics, and by minimizing the efforts required from users on the threshold parameter setting by using the Page-Hinkley test. The evaluation results on synthetic and real data show that our framework outperforms two baseline methods in terms of both detection accuracy and computational costs.

Seeping semantics: Linking datasets using word embeddings for data discovery
Employees that spend more time finding relevant data than analyzing it suffer from a data discovery problem. The large volume of data in enterprises, and sometimes the lack of knowledge of the schemas aggravates this problem. Similar to how we navigate the Web, we propose to identify semantic links that assist analysts in their discovery tasks. These links relate tables to each other, to facilitate navigating the schemas. They also relate data to external data sources, such as ontologies and dictionaries, to help explain the schema meaning. We materialize the links in an enterprise knowledge graph, where they become available to analysts. The main challenge is how to find pairs of objects that are semantically related. We propose SEMPROP, a DAG of different components that find links based on syntactic and semantic similarities. SEMPROP is commanded by a semantic matcher which leverages word embeddings to find objects that are semantically related. We introduce coherent group, a technique to combine word embeddings that works better than other state of the art combination alternatives. We implement SEMPROP as part of Aurum, a data discovery system we are building, and conduct user studies, real deployments and a quantitative evaluation to understand the benefits of links for data discovery tasks, as well as the benefits of SEMPROP and coherent groups to find those links.

Implementation and experimental performance evaluation of a hybrid interrupt-handling scheme
Improving snort performance under linux
Network intrusion detection systems (NIDS) have become vital components in securing today's computer networks. To be highly effective, NIDS must perform packet inspection of incoming traffic at or near wire speed. Failing to do so will allow malicious packets to sneak through the network undetected, and thus jeopardising network security. Snort is one of the most popular IDS and intrusion prevention system (IPS) applications. Snort is a publicly available open-source NIDS application that typically runs on Linux. In this study, the authors present and discuss the essential software components of Snort and its underlying Linux support architecture. The authors characterise Snort execution and present an analytical queuing model to give insight into understanding the kernel and Snort behaviour as well as to identify key-dominating factors that strongly influence and impact Snort performance. The authors demonstrate that the current default configurations of the packet reception mechanism of the Linux networking subsystem (a.k.a. NAPI) are not suitable for Snort performance and show that the performance of Snort can be improved significantly by tuning certain configuration parameters, specifically by having a small NAPI budge value of 2. The performance is measured in terms of throughput and packet loss. The authors also measure the packet loss encountered at the kernel level as well as the interrupt rate of incoming traffic. Performance was measured when subjecting a PC host running Snort to both normal and malicious traffic, and with different traffic load conditions.

FAHES: A robust disguised missing values detector
Missing values are common in real-world data and may seriously affect data analytics such as simple statistics and hypothesis testing. Generally speaking, there are two types of missing values: explicitly missing values (i.e. NULL values), and implicitly missing values (a.k.a. disguised missing values (DMVs)) such as "11111111" for a phone number and "Some college" for education. While detecting explicitly missing values is trivial, detecting DMVs is not; the essential challenge is the lack of standardization about how DMVs are generated. In this paper, we present FAHES, a robust system for detecting DMVs from two angles: DMVs as detectable outliers and as detectable inliers. For DMVs as outliers, we propose a syntactic outlier detection module for categorical data, and a density-based outlier detection module for numerical values. For DMVs as inliers, we propose a method that detects DMVs which follow either missing-completely-at-random or missing-at-random models. The robustness of FAHES is achieved through an ensemble technique that is inspired by outlier ensembles. Our extensive experiments using real-world data sets show that FAHES delivers better results than existing solutions.

KDE-Track: An efficient dynamic density estimator for data streams
Recent developments in sensors, global positioning system devices, and smart phones have increased the availability of spatiotemporal data streams. Developing models for mining such streams is challenged by the huge amount of data that cannot be stored in the memory, the high arrival speed, and the dynamic changes in the data distribution. Density estimation is an important technique in stream mining for a wide variety of applications. The construction of kernel density estimators is well studied and documented. However, existing techniques are either expensive or inaccurate and unable to capture the changes in the data distribution. In this paper, we present a method called KDE-Track to estimate the density of spatiotemporal data streams. KDE-Track can efficiently estimate the density function with linear time complexity using interpolation on a kernel model, which is incrementally updated upon the arrival of new samples from the stream. We also propose an accurate and efficient method for selecting the bandwidth value for the kernel density estimator, which increases its accuracy significantly. Both theoretical analysis and experimental validation show that KDE-Track outperforms a set of baseline methods on the estimation accuracy and computing time of complex density structures in data streams.

Pattern functional dependencies for data cleaning
Patterns (or regex-based expressions) are widely used to constrain the format of a domain (or a column), e.g., a Year column should contain only four digits, and thus a value like "1980-" might be a typo. Moreover, integrity constraints (ICs) defined over multiple columns, such as (conditional) functional dependencies and denial constraints, e.g., a ZIP code uniquely determines a city in the UK, have been widely used in data cleaning. However, a promising, but not yet explored, direction is to combine regex- and IC-based theories to capture data dependencies involving partial attribute values. For example, in an employee ID such as"F-9-107", "F" is sufficient to determine the finance department. Inspired by the above observation, we propose a novel class of ICs, called pattern functional dependencies (PFDs), to model fine-grained data dependencies gleaned from partial attribute values. These dependencies cannot be modeled using traditional ICs, such as (conditional) functional dependencies, which work on entire attribute values. We also present a set of axioms for the inference of PFDs, analogous to Armstrong's axioms for FDs, and study the complexity of consistency and implication analysis of PFDs. Moreover, we devise an effective algorithm to automatically discover PFDs even in the presence of errors in the data. Our extensive experiments on 15 real-world datasets show that our approach can effectively discover valid and useful PFDs over dirty data, which can then be used to detect data errors that are hard to capture by other types of ICs.

A demo of the data civilizer system
Finding relevant data for a specific task from the numerous data sources available in any organization is a daunting task. This is not only because of the number of possible data sources where the data of interest resides, but also due to the data being scattered all over the enterprise and being typically dirty and inconsistent. In practice, data scientists are routinely reporting that the majority (more than 80%) of their effort is spent finding, cleaning, integrating, and accessing data of interest to a task at hand. We propose to demonstrate DATA CIVILIZER to ease the pain faced in analyzing data "in the wild". DATA CIVILIZER is an end-to-end big data management system with components for data discovery, data integration and stitching, data cleaning, and querying data from a large variety of storage engines, running in large enterprises.

Boosting throughput of Snort NIDS under Linux
Snort is one of the most popular Network Intrusion Detection Systems (NIDS) that exist today. Snort needs to be highly effective to keep up with today's high traffic of gigabit networks. An intrusion detection system that fails to perform packet inspection at high rate will allow malicious packets to enter the network undetected. In this paper we demonstrate that the current default configuration of the Linux networking subsystem (a.k.a. NAPI) is not suitable for Snort's performance. We show that the performance of Snort can be improved significantly by tuning certain configuration parameters. In particular, we experimentally study the performance impact of choosing different NAPI budget values on Snort's throughput. We conclude that a small budget would enhance the performance significantly.

Building data civilizer pipelines with an advanced workflow engine
In order for an enterprise to gain insight into its internal business and the changing outside environment, it is essential to provide the relevant data for in-depth analysis. Enterprise data is usually scattered across departments and geographic regions and is often inconsistent. Data scientists spend the majority of their time finding, preparing, integrating, and cleaning relevant data sets. Data Civilizer is an end-to-end data preparation system. In this paper, we present the complete system, focusing on our new workflow engine, a superior system for entity matching and consolidation, and new cleaning tools. Our workflow engine allows data scientists to author, execute and retrofit data preparation pipelines of different data discovery and cleaning services. Our end-to-end demo scenario is based on data from the MIT data warehouse and e-commerce data sets.

FAHES: Detecting Disguised Missing Values.
It is well established that missing values, if not dealt with properly, may lead to poor data analytics models, misleading conclusions, and limitation in the generalization of findings. A key challenge in detecting these missing values is when they manifest themselves in a form that is otherwise valid, making it hard to distinguish them from other legitimate values. We propose to demonstrate FAHES, a system for detecting different types of disguised missing values (DMVs) which often occur in real world data. FAHES consists of several components, namely a profiler to generate rules for detecting repeated patterns, an outlier detection module, and a module to detect values that are used repeatedly in random records. Using several real world datasets, we will demonstrate how FAHES can easily catch DMVs.

Minimizing User Involvement For Learning Human Mobility Patterns From Location Traces

 
 Utilizing trajectories for modeling human mobility often involves extracting descriptive features for each individual, a procedure heavily based on experts' knowledge. In this work, our objective is to minimize human involvement and exploit the power of community in learning `features' for individuals from their location traces. We propose a probabilistic graphical model that learns distribution of latent concepts, named motifs, from anonymized sequences of user locations. To handle variation in user activity level, our model learns motif distributions from sequence-level location co-occurrence of all users. To handle the big variation in location popularity, our model uses an asymmetric prior, conditioned on per-sequence features. We evaluate the new representation in a link prediction task and compare our results to those of baseline approaches.
 


Efficient estimation of dynamic density functions with an application to outlier detection
In this paper, we propose a new method to estimate the dynamic density over data streams, named KDE-Track as it is based on a conventional and widely used Kernel Density Estimation (KDE) method. KDE-Track can efficiently estimate the density with linear complexity by using interpolation on a kernel model, which is incrementally updated upon the arrival of streaming data. Both theoretical analysis and experimental validation show that KDE-Track outperforms traditional KDE and a baseline method Cluster-Kernels on estimation accuracy of the complex density structures in data streams, computing time and memory usage. KDE-Track is also demonstrated on timely catching the dynamic density of synthetic and real-world data. In addition, KDE-Track is used to accurately detect outliers in sensor data and compared with two existing methods developed for detecting outliers and cleaning sensor data.

ANMAT: automatic knowledge discovery and error detection through pattern functional dependencies
Knowledge discovery is critical to successful data analytics. We propose a new type of meta-knowledge, namely pattern functional dependencies (PFDs), that combine patterns (or regex-like rules) and integrity constraints (ICs) to model the dependencies (or meta-knowledge) between partial values (or patterns) across different attributes in a table. PFDs go beyond the classical functional dependencies and their extensions. For instance, in an employee table, ID "F-9-107'', "F'' determines the finance department. Moreover, a key application of PFDs is to use them to identify erroneous data; tuples that violate some PFDs. In this demonstration, attendees will experience the following features: (i) PFD discovery -- automatically discover PFDs from (dirty) data in different domains; and (ii) Error detection with PFDs -- we will show errors that are detected by PFDs but cannot be captured by existing approaches.

Manipulation detection in cryptocurrency markets: An anomaly and change detection based approach
As a financial asset, cryptocurrencies innovated the financial industry in different ways. However, the lack of regulations and transparency in cryptocurrency markets is hindering the industry from reaching its full potential. There is a need for extensive technical analysis of the cryptocurrency market data to detect possible market manipulation attempts. Anomaly detection techniques can reveal information about abnormal activities in the market and provide insights on manipulation attempts. In this study, a robust unsupervised anomaly detection tool (ADT) is developed for this purpose. Experiments show that ADT outperforms a set of methods in detecting the anomalies in features extracted from the cryptocurrency exchanges data and on a set of benchmark data sets.

Prefixcdd: Effective online concept drift detection over event streams using prefix trees
Process mining focuses on applying data mining techniques over business process data. Recently, with the improvements in sensoring, collection, and storage of event data, a big demand for both shorter mining time and adaptive models of streaming process events arose. This increased the interest in streaming process mining. Some techniques within this field attempt to identify drifts (change points) from evolving process data streams. Existing work on supervised and unsupervised-learning approaches over data streams have several limitations with regards to the nature of the drifts, the excessive storage required to store and process the stream, and the performance over real-world datasets. This paper contributes PrefixCDD, an efficient unsupervised-learning novel approach for online concept drift detection (CDD) over event streams. Our proposed approach utilizes a data structure, where the data stream components are stored in a set of prefix-trees. It transforms then the discrete data into continuous one using a Principal Component Analysis (PCA) approach over the trees. Then, ADWIN is used to focus on up-to-date information, making it appealing to work with the decaying mechanism logic behind our algorithm. Using six artificial and three real-life datasets, PrefixCDD outperforms state-of-the-art techniques in terms of detecting existing drifts of different natures, discovering them shortly after they appear, and the overall execution time.

Experimental performance evaluation of a hybrid packet reception scheme for Linux networking subsystem
The heavy traffic introduced by Gigabit networks can result in a significant performance degradation of network hosts. This degradation happens as a result of the interrupt overhead associated with the high rate of packet arrivals. NAPI, a packet reception mechanism integrated into the latest version of Linux networking subsystem, was designed to improve Linux performance to suit today's Gigabit traffic. NAPI is definitely a major step up from earlier reception mechanisms; however, NAPI has shortcomings and its performance can be further enhanced. A hybrid packet reception scheme that switches between interrupt disabling-enabling (DE) and polling (NAPI) can better improve the performance of Gigabit network hosts. In this paper we prove experimentally that the hybrid scheme can boost under different traffic load conditions the performance of general-purpose network desktops or servers running network I/O-bound applications. The implementation of the hybrid scheme is done in the latest version of Linux kernel 2.6.15.

A new study of two divergence metrics for change detection in data streams
Streaming data are dynamic in nature with frequent changes. To detect such changes, most methods measure the difference between the data distributions in a current time window and a reference window. Divergence metrics and density estimation are required to measure the difference between the data distributions. Our study shows that the Kullback-Leibler (KL) divergence, the most popular metric for comparing distributions, fails to detect certain changes due to its asymmetric property and its dependence on the variance of the data. We thus consider two metrics for detecting changes in univariate data streams: a symmetric KL-divergence and a divergence metric measuring the intersection area of two distributions. The experimental results show that these two metrics lead to more accurate results in change detection than baseline methods such as Change Finder and using conventional KL-divergence.

Coscfair: Ensuring subgroup fairness through fair classification framework
Introduction: The usage of Machine Learning (ML) in a wide diversity of domains has affected everyone’s daily life. For example, machine learning algorithms are used for decision making in business and government systems, in recommending systems, advertisements, hiring systems, and so on. Machine learning algorithms have become widespread because of their high performance compared to humans in such tasks. Machine learning algorithms can handle big volumes of data for complex computational tasks in significantly shorter time compared to humans. Besides, people usually have subjective opinions and points of view, which can lead to bias in their decisions. Unfortunately, a large number of systems have been identified to show bias against specific groups of the society. For example: i) Amazon’s algorithm for free same-day delivery made racially biased decisions while choosing which neighborhoods to provide this service; ii) the COMPAS recidivism estimation tool, which is used in many courts of the United States shows significant discrimination against African-American males by predicting a higher risk for recidivism compared to white male offenders [source]. According to the automatically predicted risk level of the defendants, courts can keep the defendants in custody until the trial and consider this risk score while deciding the verdict. There are several reasons behind the bias in ML algorithms: i) the under-representation of a certain group of people in the training set of a dataset; ii) the historical bias or prejudice reflected in the decision variable (class label); iii) limited features in a dataset that could be less informative about the population; and iv) the existence of attributes that are directly related to the sensitive attributes, such as race and gender, evenwhen these sensitive attributes are not used to train the algorithms could be considered as another reason. These potential problems in a dataset cause machine learning algorithms to keep the existing bias and reflect it in their decisions, or even sometimes exacerbates the existing bias. However, in order to identify and prevent bias in machine learning, researchers have come upwith several different fairness metrics around fairness-aware machine learning. To improve the algorithmic fairness according to the fairness metrics, different algorithmic approaches have been developed to eliminate the existing bias or mitigate it under a certain level. Unfortunately, there is no consensus on which fairness metrics and mitigation algorithms are the best to ensure fairness yet. We propose COSCFair, a pre-processing framework that can handle datasets with multiple sensitive attributes by eliminating its class and group imbalance via an oversampling technique to mitigate the bias before training the classifiers. This way, the classifier will not carry on or exacerbate the existing bias in a dataset. By eliminating both class and group imbalance simultaneously and obtaining the same base rates for all subgroups in a dataset, the framework will be able to satisfy multiple fairness metrics in the literature, which cannot be satisfied otherwise. Our framework is, therefore, based on oversampling the under-represented subgroups in the dataset. We used two different oversampling techniques: I) the Synthetic Minority Over-sampling TEchnique (SMOTE) [1]; II) the Generative Adversarial Network (GANs) [2]. However, it is crucial to ensure that the generated samples are realistic and close to the original data. For this reason, when using SMOTE [1], we cluster the data before performing the oversampling to improve the quality of the synthetic data. Since the original data samples in each cluster has more similarity with each other, the SMOTE oversampling technique used on these clusters will yield better quality of synthetic samples. GANs is a deep learning model that use two main machines: a) the generator which is responsible for generating new samples from the original dataset; and b) the discriminator that tries to classify the new generated samples as either real (close to the samples from the domain), or fake samples. When the discriminator classifies a generated sample as fake, the generator adjustes its weights to produce more realistic samples. In our work, we use the Conditional GAN (CTGAN), where both the generator and the discriminator are conditioned on a set of attributes. We use the GANs to generate samples from the under-represented groups such that the final number of instances from each group (combinations of the values of the sensitive attributes and the class label) are equal. Experimental results over different datasets that are widely used as benchmarks to evaluate algorithmic fairness show that our framework yields consistent improvements compared to a set of baseline methods.

Machine Learning—Evaluation (Cross-validation, Metrics, Importance Scores...)
The proliferation of connected vehicles and the advent of New Radio (NR) technologies have ushered in a new era of intelligent transportation systems. Ensuring reliable and lowlatency communication between vehicles and their surrounding environment is of utmost importance for the success of these systems. This paper presents a novel approach to predict Quality of Service (QoS) in Vehicle-to-Everything (V2X) communications through nested cross-validation. Our methodology employs several machine learning (ML) methods to predict some QoS metrics, such as packet delivery ratio (PDR), and throughput, in NR-based V2X scenarios. In ML employment, nested cross-validation approach, unlike conventional cross-validation approach, prevents information leakage from parameter selection into hyperparameter selection, and this results in getting more robust results in terms of overfitting. The study utilizes real-world NR-V2X datasets to train and validate the proposed ML methods. Through extensive experiments, we demonstrate the efficacy of our approach in accurately predicting QoS parameters, even in dynamic and challenging vehicular environments. In summary, our research contributes to the advancement of NR-based V2X communication systems by introducing employment of ML methods with a novel approach for QoS prediction. The combination of accurate predictions through nested cross-validation not only enhances the reliability of communication in connected vehicles' landscape but also has a supportive role for stakeholders to make informed decisions for the optimization and management of vehicular networks.

DONE