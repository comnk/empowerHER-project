Infrastructure for e-government web services
In efforts to use information and communication technologies for the civil and political conduct of government, many countries have begun supporting e-government initiatives. The ultimate goal is to improve government-citizen interactions through an infrastructure built around the "life experience" of citizens. To facilitate the use of welfare applications and expeditiously satisfy citizens' needs, we wrapped these applications in modular Web services. Adopting Web services in e-government enables government agencies to provide value-added services by defining a new service that outsources from other e-government services; to uniformly handle privacy issues: and to standardize the description, discovery, and invocation of social programs. The core of our research is to develop techniques to efficiently access e-government services while preserving citizens' privacy. To that end we designed and implemented an infrastructure called Web Digital Government.

Privacy on the Web: Facts, challenges, and solutions
Despite important regulatory and technical efforts aimed at tackling aspects of the problem, privacy violation incidents on the Web continue to hit the headlines. The authors outline the salient issues and proposed solutions, focusing on generic Web user's Web privacy.

Preserving privacy in web services
Web services are increasingly being adopted as a viable means to access Web-based applications. This has been enabled by the tremendous standardization effort to describe, advertise, discover, and invoke Web services. Digital government (DG) is a major application domain for Web services. It aims at improving government-citizen interactions using information and communication technologies. Government agencies collect, store, process, and share information about millions of citizens who have different preferences regarding their privacy. This naturally raises a number of legal and technical issues that must be addressed to preserve citizens' privacy through the control of the information flow amongst different entities (users, Web services, DBMSs). Solutions addressing this issue are still in their infancy. They consist, essentially, of enforcing privacy by law or by self-regulation. In this paper, we propose a new technical approach for preserving privacy in government Web services. Our design is based on digital privacy credentials, data filters and mobile privacy preserving agents. This work aims at establishing the feasibility and provable reliability of technology-based privacy preserving solutions for Web service infrastructures.

Service-oriented sensor–actuator networks: Promises, challenges, and the road ahead
Dimming cellular networks
We propose a novel technique called dimming to improve the energy efficiency of cellular networks by reducing the capacity, services, and energy consumption of cells without turning off the cells. We define three basic methods to dim the network: coverage, frequency, and service dimming. We construct a multi-time period optimization problem to implement frequency dimming and extend it to implement both frequency and service dimming together. We illustrate the ability of dimming techniques to adapt the capacity and network services in proportion to the dynamic spatial and temporal load resulting in significant energy savings through numerical results for a sample network.

CAPRA: a comprehensive approach to product ranking using customer reviews
Utilize cloud computing to support dust storm forecasting
Abstract The simulations and potential forecasting of dust storms are of significant interest to public health and environment sciences. Dust storms have interannual variabilities and are typical disruptive events. The computing platform for a dust storm forecasting operational system should support a disruptive fashion by scaling up to enable high-resolution forecasting and massive public access when dust storms come and scaling down when no dust storm events occur to save energy and costs. With the capability of providing a large, elastic, and virtualized pool of computational resources, cloud computing becomes a new and advantageous computing paradigm to resolve scientific problems traditionally requiring a large-scale and high-performance cluster. This paper examines the viability for cloud computing to support dust storm forecasting. Through a holistic study by systematically comparing cloud computing using Amazon EC2 to traditional high performance computing (HPC) cluster, we find that cloud computing is emerging as a credible solution for (1) supporting dust storm forecasting in spinning off a large group of computing resources in a few minutes to satisfy the disruptive computing requirements of dust storm forecasting, (2) performing high-resolution dust storm forecasting when required, (3) supporting concurrent computing requirements, (4) supporting real dust storm event forecasting for a large geographic domain by using recent dust storm event in Phoniex, 05 July 2011 as example, and (5) reducing cost by maintaining low computing support when there is no dust storm events while invoking a large amount of computing resource to perform high-resolution forecasting and responding to large amount of concurrent public accesses.

Service-oriented sensor-actuator networks [Ad hoc and sensor networks]
ion Type of Application Off-Network In-Network Services Awareness Optimization Optimization [34] Node services In-network No No √ [94] Jini Services Off-network No √ No [50] Web services Off-network No No No [118] Web services Off-network √ √ No [95] Node services In-network No No No [26] Node Web services In-network √ No No [38] Web services Off-network No No No [48] .NET components Off-network √ √ No [56] OSGi-based services Off-network No No No [87] Ad hoc In-network No No √ [47] Ad hoc In-network √ √ No [72] Objects In,Off-network √ √ No TinySOA Node services In-network √ √ √ Table 3.1: TinySOA vs. Other Service-Oriented Systems associate a single service invocation with several queries. An example of the latter is TinySOA’s SARP protocol discussed in Chapter 4. – Off-Network Service-based Optimization: From the perspective of TinySOA’s clients (e.g. base station, mobile user), TinySOA streams query results in similar ways than existing sensor systems. Traditional off-network optimization techniques (caching, query rewriting, reprocessing previous results to answer new queries, etc.) are also applicable in the context of TinySOA. • Application Independence and Application Awareness: Application awareness refers to the ability to exploit the specific characteristics of a given application to improve the overall efficiency of the network. Several prior efforts have proposed application-aware solutions. However, these solutions are often too specific to the considered class of applications. In contrast, TinySOA expose “neutral” services that any application may use with the same efficiency expectations. TinySOA is therefore application independent while it is still able to exploit any specific characteristics of a given application. A. Rezgui Chapter 3. Service-Oriented Query Model and Architecture for SANETs 67 Table 3.1 summarizes the recent and ongoing research that investigates the concept of services in the context of sensor networks. It also compares TinySOA to other service-oriented sensor systems. In the Abstraction column of the table, the term node services refers to a software module running on sensor nodes. Web services refer to cases where the service is Webaccessible through standards such as XML. Node Web services refer to cases where the service is both hosted on sensor nodes and Web-accessible.

Service-oriented sensor-actuator networks [Ad hoc and sensor networks]
ion Type of Application Off-Network In-Network Services Awareness Optimization Optimization [34] Node services In-network No No √ [94] Jini Services Off-network No √ No [50] Web services Off-network No No No [118] Web services Off-network √ √ No [95] Node services In-network No No No [26] Node Web services In-network √ No No [38] Web services Off-network No No No [48] .NET components Off-network √ √ No [56] OSGi-based services Off-network No No No [87] Ad hoc In-network No No √ [47] Ad hoc In-network √ √ No [72] Objects In,Off-network √ √ No TinySOA Node services In-network √ √ √ Table 3.1: TinySOA vs. Other Service-Oriented Systems associate a single service invocation with several queries. An example of the latter is TinySOA’s SARP protocol discussed in Chapter 4. – Off-Network Service-based Optimization: From the perspective of TinySOA’s clients (e.g. base station, mobile user), TinySOA streams query results in similar ways than existing sensor systems. Traditional off-network optimization techniques (caching, query rewriting, reprocessing previous results to answer new queries, etc.) are also applicable in the context of TinySOA. • Application Independence and Application Awareness: Application awareness refers to the ability to exploit the specific characteristics of a given application to improve the overall efficiency of the network. Several prior efforts have proposed application-aware solutions. However, these solutions are often too specific to the considered class of applications. In contrast, TinySOA expose “neutral” services that any application may use with the same efficiency expectations. TinySOA is therefore application independent while it is still able to exploit any specific characteristics of a given application. A. Rezgui Chapter 3. Service-Oriented Query Model and Architecture for SANETs 67 Table 3.1 summarizes the recent and ongoing research that investigates the concept of services in the context of sensor networks. It also compares TinySOA to other service-oriented sensor systems. In the Abstraction column of the table, the term node services refers to a software module running on sensor nodes. Web services refer to cases where the service is Webaccessible through standards such as XML. Node Web services refer to cases where the service is both hosted on sensor nodes and Web-accessible.

TARP: a trust-aware routing protocol for sensor-actuator networks
Most routing protocols for sensor-actuator networks (SANETs) are built under the assumption that nodes normally cooperate in forwarding each other's messages. In practice, this assumption is not realistic; SANETs are environments where nodes may or may not cooperate. For several reasons, a node may fail to operate as planned at deployment time. As a result, when actually deployed, protocols and applications may not be as efficient as expected. In this paper, we present TARP (Trust-Aware Routing Protocol), a routing protocol for sensor-actuator networks that exploits past nodes' routing behavior and links' quality to determine efficient paths. We implemented TARP in a TinyOS-based SANET and conducted several experiments to evaluate its performance. The obtained results confirmed that TARP achieves substantial improvements in terms of energy consumption and scalability.

Predicting Intensive Care Unit Length of Stay and Mortality Using Patient Vital Signs: Machine Learning Model Development and Validation
Background Patient monitoring is vital in all stages of care. In particular, intensive care unit (ICU) patient monitoring has the potential to reduce complications and morbidity, and to increase the quality of care by enabling hospitals to deliver higher-quality, cost-effective patient care, and improve the quality of medical services in the ICU. Objective We here report the development and validation of ICU length of stay and mortality prediction models. The models will be used in an intelligent ICU patient monitoring module of an Intelligent Remote Patient Monitoring (IRPM) framework that monitors the health status of patients, and generates timely alerts, maneuver guidance, or reports when adverse medical conditions are predicted. Methods We utilized the publicly available Medical Information Mart for Intensive Care (MIMIC) database to extract ICU stay data for adult patients to build two prediction models: one for mortality prediction and another for ICU length of stay. For the mortality model, we applied six commonly used machine learning (ML) binary classification algorithms for predicting the discharge status (survived or not). For the length of stay model, we applied the same six ML algorithms for binary classification using the median patient population ICU stay of 2.64 days. For the regression-based classification, we used two ML algorithms for predicting the number of days. We built two variations of each prediction model: one using 12 baseline demographic and vital sign features, and the other based on our proposed quantiles approach, in which we use 21 extra features engineered from the baseline vital sign features, including their modified means, standard deviations, and quantile percentages. Results We could perform predictive modeling with minimal features while maintaining reasonable performance using the quantiles approach. The best accuracy achieved in the mortality model was approximately 89% using the random forest algorithm. The highest accuracy achieved in the length of stay model, based on the population median ICU stay (2.64 days), was approximately 65% using the random forest algorithm. Conclusions The novelty in our approach is that we built models to predict ICU length of stay and mortality with reasonable accuracy based on a combination of ML and the quantiles approach that utilizes only vital signs available from the patient’s profile without the need to use any external features. This approach is based on feature engineering of the vital signs by including their modified means, standard deviations, and quantile percentages of the original features, which provided a richer dataset to achieve better predictive power in our models.

Mobile cloud gaming: Issues and challenges
Using adaptively coupled models and high-performance computing for enabling the computability of dust storm forecasting
Forecasting dust storms for large geographical areas with high resolution poses great challenges for scientific and computational research. Limitations of computing power and the scalability of parallel systems preclude an immediate solution to such challenges. This article reports our research on using adaptively coupled models to resolve the computational challenges and enable the computability of dust storm forecasting by dividing the large geographical domain into multiple subdomains based on spatiotemporal distributions of the dust storm. A dust storm model (Eta-8bin) performs a quick forecasting with low resolution (22 km) to identify potential hotspots with high dust concentration. A finer model, non-hydrostatic mesoscale model (NMM-dust) performs high-resolution (3 km) forecasting over the much smaller hotspots in parallel to reduce computational requirements and computing time. We also adopted spatiotemporal principles among computing resources and subdomains to optimize parallel systems and improve the performance of high-resolution NMM-dust model. This research enabled the computability of high-resolution, large-area dust storm forecasting using the adaptively coupled execution of the two models Eta-8bin and NMM-dust.

Geoinformatics: transforming data to knowledge for geosciences
An integrative view of Earth as a system, based on multidisciplinary data, has become one of the most compelling reasons for research and education in the geosciences. It is now necessary to establish a modern infrastructure that can support the transformation of data to knowledge. Such an information infrastructure for geosciences is contained within the emerging science of geoinformatics, which seeks to promote the utilization and integration of complex, multidisciplinary data in seeking solutions to geoscience-based societal challenges.

μRACER: A reliable adaptive service-driven efficient routing protocol suite for sensor-actuator networks
We present reliable adaptive service-driven efficient routing (muRACER), a routing protocol suite based on a novel service-oriented design for sensor-actuator networks where nodes expose their capabilities to applications as a service profile. A node's service profile consists of a set of services (i.e., sensing and actuation capabilities) that it provides and the quality-of-service (QoS) parameters associated with those services (delay, accuracy, freshness, etc.). muRACER uses an efficient service-aware routing approach that aggressively reduces downstream traffic (from the sink to the network's nodes) by translating service profiles into efficient paths for queries. To support QoS, muRACER dynamically adapts each node's routing behavior and service profile according to the current context of that node, i.e., number of pending queries and number and type of messages to be routed. Finally, muRACER improves end-to-end reliability through a scalable reputation-based approach in which each node is able to locally estimate the next hop of the most reliable path to the sink. Service- and context-aware reliable routing enhances the network's efficiency and effectiveness (satisfaction of applications' QoS requirements). We implemented muRACER on top of TinyOS and conducted several experiments that confirmed muRACER's ability with regard to each of its design objectives.

What is a remote sensing change detection technique? Towards a conceptual framework
ABSTRACT Remote sensing change detection (RSCD) is the process of identifying changes between scenes of the same location acquired at different times. This is an active research area with a broad range of applications. Many change detection methods have been described in the literature, ranging from simple differencing to machine learning techniques. Faced with this large number of suggested techniques, researchers have tried to classify them into different categories to simplify and systematize the topic. A number of different categorization schemes have been proposed, based on various dimensions, aspects or features of the techniques considered. Nevertheless, using these schemes to understand the different techniques and to select suitable techniques for a specific remote sensing change detection project remains a hard task for practitioners. In this context, we provide a critical study of proposed categorization schemes in order to extract the main dimensions used for their construction. Then, based on the obtained set of dimensions, we propose a new definition for the concept of a remote sensing change detection technique, which provides greater clarity and better guidance for researchers and practitioners. We then expand upon this description to incorporate non-computational aspects of RSCD in order to propose a comprehensive conceptual framework for remote sensing change detection projects.

Change detection in urban areas from remote sensing data: A multidimensional classification scheme
ABSTRACT Change detection (CD) from remote sensing data is a very challenging research problem, especially when we analyse an urban scene. Urban scenes are composed of many different types of objects, both natural and man-made. The building class is one of the important and most complex classes to analyse, important because it is useful for so many applications and complex because it exhibits many changes due to human activity and natural catastrophes. For these reasons, we focus our study on building change detection (BCD). In this paper we propose a classification scheme for BCD research according to several important dimensions including objective, input data, temporal resolution, analysis unit, target output unit, building features, processing technique, change categories, and assessment of results. This classification scheme can guide practitioners in choosing appropriate change detection methods to achieve their goals as well as informing new research efforts. Based on this multidimensional characterisation of BCD, we offer a number of suggestions for further work to be done in this field.

ConceptOnto: An upper ontology based on Conceptnet
The exponential growth of information has prompted the introduction of new technologies such as Semantic Web and Common Sense knowledge bases. To connect the different knowledge presentations together is a primary requirement, and ontologies are central we need for this transformation. In this paper we introduce ConceptOnto which is an ontology based on the ConceptNet knowledge base with extension of some of the other properties in some of the more acclaimed upper ontologies. Our goal in the creation of ConceptOnto is readability for humans, and maximizing the functionality while saving the generality of the ontology.

Failuresim: a system for predicting hardware failures in cloud data centers using neural networks
Hardware failures in cloud data centers may cause substantial losses to cloud providers and cloud users. Therefore, the ability to accurately predict when failures occur is of paramount importance. In this paper, we present FailureSim, a simulator based on CloudSim that supports failure prediction. FailureSim obtains performance related information from the cloud and classifies the status of the hardware using a neural network. Performance information is read from hosting hardware and stored in a variable length windowing vector. At specified stages, various aggregation methods are applied to the windowing vector to obtain a single vector that is fed as input to the trained classification algorithm. Using conservative host failure behavior models, FailureSim was able to successfully predict host failure in the cloud with roughly 89% accuracy.

A Reputation-based approach to preserving privacy in Web services
DONE