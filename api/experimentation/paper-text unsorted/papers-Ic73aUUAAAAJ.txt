CULZSS: LZSS lossless data compression on CUDA
Increasing needs in efficient storage management and better utilization of network bandwidth with less data transfer have led the computing community to consider data compression as a solution. However, compression introduces extra overhead and performance can suffer. The key elements in making the decision to use compression are execution time and compression ratio. Due to negative performance impact, compression is often neglected. General purpose computing on graphic processing units (GPUs) introduces new opportunities where parallelism is available. Our work targets the use of opportunities in GPU based systems by exploiting parallelism in compression algorithms. In this paper we present an implementation of the Lempel-Ziv-Storer-Szymanski (LZSS) loss less data compression algorithm by using NVIDIA GPUs Compute Unified Device Architecture (CUDA) Framework. Our implementation of the LZSS algorithm on GPUs significantly improves the performance of the compression process compared to CPU based implementation without any loss in compression ratio. This can support GPU based clusters in solving application bandwidth problems. Our system outperforms the serial CPU LZSS implementation by up to 18x, the parallel threaded version up to 3x and the BZIP2 program by up to 6x in terms of compression time, showing the promise of CUDA systems in loss less data compression. To give the programmers an easy to use tool, our work also provides an API for in memory compression without the need for reading from and writing to files, in addition to the version involving I/O.

Pipelined parallel LZSS for streaming data compression on GPGPUs
In this paper, we present an algorithm and provide design improvements needed to port the serial Lempel-Ziv-Storer-Szymanski (LZSS), lossless data compression algorithm, to a parallelized version suitable for general purpose graphic processor units (GPGPU), specifically for NVIDIA's CUDA Framework. The two main stages of the algorithm, substring matching and encoding, are studied in detail to fit into the GPU architecture. We conducted detailed analysis of our performance results and compared them to serial and parallel CPU implementations of LZSS algorithm. We also benchmarked our algorithm in comparison with well known, widely used programs, GZIP and ZLIB. We achieved up to 34x better throughput than the serial CPU implementation of LZSS algorithm and up to 2.21x better than the parallelized version.

Creating consensus group using online learning based reputation in blockchain networks
Towards large-scale molecular dynamics simulations on graphics processors
A taxonomy for Blockchain based distributed storage technologies
Optimizing LZSS compression on GPGPUs
Securing blockchain shards by using learning based reputation and verifiable random functions
In order to meet the increasing demand of the blockchain, it needs to find a solution to the scalability problem. It has been focused on sharding recently to address the scalability problem. In the sharding method, the blockchain is divided into pieces. Instead of a more extensive network, networks with fewer nodes are created. As a result, it becomes more important that each node in the network is reliable. In this study, studies using sharding method have been investigated, and methods for the assigning nodes to shards are proposed. The use of learning-based adaptive methods for this process will contribute to the safe and reliable use of shards. The probability of the shards to deteriorate and influence the whole blockchain will be reduced.

Achieving TeraCUPS on Longest Common Subsequence Problem using GPGPUs
In this paper, we describe a novel technique to optimize longest common subsequence (LCS) algorithm for one-to-many matching problem on GPUs by transforming the computation into bit-wise operations and a post-processing step. The former can be highly optimized and achieves more than a trillion operations (cell updates) per second (CUPS)-a first for LCS algorithms. The latter is more efficiently done on CPUs, in a fraction of the bit-wise computation time. The bit-wise step promises to be a foundational step and a fundamentally new approach to developing algorithms for increasingly popular heterogeneous environments that could dramatically increase the applicability of hybrid CPU-GPU environments.

Data cleaning for process mining with smart contract
Process Mining (PM) is a special data mining technique that allows extracting information from data of critical transactions (i.e. event logs) carried out in Information Systems and monitors the patterns in these transactions. When we start to process event logs with process mining tools, we face with data quality problems such as incorrect and insufficient logging and timing. Thus, data cleaning operations must be applied to event logs before applying process mining on these logs. Being an innovative medium of distributed data processing and storage with the features of enhanced security, traceability, automated transaction verification and integration, Blockchain Technology and Smart Contracts might be a good option to process and store event logs for process mining. In this paper, we focused on the cleaning of the event logs by smart contract as data is flowing from the information systems into the blockchain, and used Hyperledger Composer by IBM to develop our solution. We tested our proposal on an open process data of 1555 records, and compared the cleaning performance of our proposal with that of DataWrangler by Stanford University. Our proposal not only cleaned all 1313 records identified and cleaned by DataWrangler, it also saved 12 additional records with a different date format that was caught and corrected by our smart contract implementation.

A Conceptual Model for Blockchain-Based Software Project Information Sharing
Accurate estimations play a significant role in the success of software projects, and companies should have sufficient number of past project data to make these estimations accurate and reliable. Some institutions gather project metrics from companies to create cross-company datasets and open these datasets to companies for paid or free of charge. On the other hand, many companies do not want to make public all or part of their project information so it prevents the growth of such datasets. Blockchain technology and smart contracts, as a medium to store private information and share it with predefined constraints, might be a solution to this problem. In this study, we propose a conceptual model as a reference for blockchain-based software project information sharing, and discuss issues related to its feasibility.

Culzss-bit: A bit-vector algorithm for lossless data compression on gpgpus
In this paper, we describe an algorithm to improve dictionary based lossless data compression on GPGPUs. The presented algorithm uses bit-wise computations and leverages bit parallelism for the core part of the algorithm which is the longest prefix match calculations. Using bit parallelism, also known as bit-vector approach, is a fundamentally new approach for data compression and promising in performance for hybrid CPU-GPU environments.The implementation of the new compression algorithm on GPUs improves the performance of the compression process compared to the previous attempts. Moreover, the bit-vector approach opens new opportunities for improvement and increases the applicability of popular heterogeneous environments.

Fast longest common subsequence with general integer scoring support on gpus
Graphic Processing Units (GPUs) have been gaining popularity among high-performance users. Certain classes of algorithms benefit greatly from the massive parallelism of GPUs. One such class of algorithms is longest common subsequence (LCS). Combined with bit parallelism, recent studies have been able to achieve terascale performance for LCS on GPUs. However, the reported results for the one-to-many matching problem lack correlation with weighted scoring algorithms. In this paper, we describe a novel technique to improve the score significance of the length of LCS algorithm for multiple matching. We extend the bit-vector algorithms for LCS to include integer scoring and parallelize them for hybrid CPU-GPU platforms. We benchmark our algorithm against the well-known sequence alignment algorithm on GPUs, CUDASW++, for accuracy and report performance on three different systems.

Personal data protection in blockchain with zero-knowledge proof
An optimized GPU-accelerated route planning of multi-UAV systems using simulated annealing
Grafik processor accelerated real time software defined radio applications
This paper presents optimizations for spectrogram matrix and signal detection processes which are used for signal sensing in software defined radio (SDR) applications. In industrial and literature applications, the software defined radio or software based receivers include field programmable gate array (FPGA) and digital signal processors (DSP) units for real time signal processing steps. In this study, it is performed real time signal sensing due to using GPU, providing four times speedup compared to multicore CPU which works at same platform. The other important result of our study is making SDR applications at real-time with mobile a computer instead of complex receiver architectures with high cost

Efficient heterogeneous parallel programming for compressed sensing based direction of arrival estimation
In the direction of arrival (DoA) estimation, typically sensor arrays are used where the number of required sensors can be large depending on the application. With the help of compressed sensing (CS), hardware complexity of the sensor array system can be reduced since reliable estimations are possible by using the compressed measurements where the compression is done by measurement matrices. After the compression, DoAs are reconstructed by using sparsity promoting algorithms such as alternating direction method of multipliers (ADMM). For the given procedure, both the measurement matrix design and the reconstruction algorithm may include computationally intensive operations, which are addressed in this study. The presented simulation results imply the feasibility of the system in real‐time processing with energy efficient implementations. We propose employing parallel programming to satisfy the real‐time processing requirements. While the measurement matrix design has been accelerated 16 × with CPU based parallel version with respect to the fastest serial implementation, ADMM based DoA estimation has been improved 1.1 × with GPU based parallel version compared to the fastest CPU parallel implementation. In addition, we achieved, to the best of our knowledge, the first energy‐efficient real‐time DoA estimation on embedded Jetson GPGPUs in 15 W power consumption without affecting the DoA accuracy performance.

Mobil Cihazlar Üzerinde Enerji Verimli Sanal Sabit Numara Sistemi
— The change in user habits towards mobile devices, the increase in mobile device prevalence, and the widespread use of the mobile devices have gained a new dimension for the existing fixed telephone operators by gaining a different dimension with the mobile use of SMS service as well as fixed telephone services (telephone and fax). Although many companies such as Skype and Whatsapp are products of a similar nature

Towards Tera-scale Performance for Longest Common Subsequence using Graphics Processors
1. EXTENDED ABSTRACT GPUs tradeoff complex hardware-based support for instruction level parallelism for a large number of simpler processing cores. This has a far reaching impact on application programs. Data-parallel programs with regular control flow and memory-access patterns are able to utilize the GPU hardware effectively, while programs that have thread-dependent control flow or irregular memory access patterns are unable to exploit the performance potential of GPUs. This latter category is often referred to as irregular applications, as against the former category of regular applications.

A role-based access control management model on blockchain for restricted facilities: An airport example
Since numerous firms and governments are in dire need of safeguarding their premises, the confidentiality of data is of utmost importance in achieving this goal. The security of facility entry is crucial, but it's also crucial for future demands that the entrance logs are auditable. Due to these reasons, restricted locations require access control and round-the-clock management. A centralized approach for administering grants and monitoring access is a simple choice as it is among the most frequently employed ones. In this respect, airports are deemed as being one of the safest places since every airport provides certain public and security services and accession to these restricted facilities must be controlled and authorized by the airport management organization. Unchangeable, auditable and always-available access records and open management in accordance with its confidentiality necessities are all answers to the issues experienced in the sector with regard to access control. In this work, a role-based access control strategy for regulating airport entrances that are scattered and require government oversight is presented and implemented using the Ethereum blockchain. Our obtained results demonstrate the adaptability and also utility of our proposal for other restricted facilities that have similar requirements.

GPU-Based Parallel Genetic Algorithm for Increasing the Coverage of WSNs
Advances in wireless communication, digital systems and micro-electronic-mechanical system technologies led to the development of wireless sensor networks (WSNs) which are used in various critical real-world applications. The fact that WSNs are low cost and eliminate the need for infrastructure led to their replacing traditional networks in area/event monitoring and tracking applications. WSNs consist of small and resource-limited sensor nodes, due to which several problems arise in the WSN development process. One of these problems is coverage. Providing the best coverage with a minimum number of sensor nodes is an NP-hard problem known as the maximum coverage sensor deployment problem (MCSDP). Genetic Algorithms (GAs) have been proved effective in solving optimization problems in many different disciplines (increasing coverage in WSNs, image processing, route planning, etc.). In this study, a GPU-based parallel GA solution for increasing the coverage of a given homogeneous WSN topology in a 2-D Euclidean area is proposed which is the first time this technique is used and parallelized on GPUs to the best of our knowledge. Finally, performance results of the proposed algorithm are compared to the previous work with the emphasis on the achieved performance improvement.

DONE