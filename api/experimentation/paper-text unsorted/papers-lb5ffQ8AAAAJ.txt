How to emulate shared memory
I/O-complexity of graph algorithms
We show lower bounds of Q( $ Sort(V)) for the I/Ocomplexity of graph theoretic problems like connected components, biconnected components, and minimum spanning trees, where E and V are the number of edges and vertices in the input graph, respectively. We also present a deterministic O($ Sort(V) ’ max(l,loglog 9)) algorithm for the problem of graph connectivity, where B and D denote respectively the block size and number of disks. Our algorithm includes a breadth first search; this maybe of independent

Randomized routing and sorting on fixed-connection networks
This paper presents a general paradigm for the design of packet routing algorithms for fixed-connection networks. Its basis is a randomized on-line algorithm for scheduling any set of N packets whose paths have congestion c on any bounded-degree leveled network with depth L in O(c + L + log N) steps, using constant-size queues. In this paradigm, the design of a routing algorithm is broken into three parts: (1) showing that the underlying network can emulate a leveled network, (2) designing a path selection strategy for the leveled network, and (3) applying the scheduling algorithm

Improved multithreading techniques for hiding communication latency in multiprocessors
Shared memory multiprocessors are considered among the easiest parallel computers to program. However building shared memory machines with thousands of processors has proved difficult because of the inevitably long memory latencies. Much previous research has focused on cache coherency techniques, but it remains unclear if caches can obtain sufficiently high hit rates. In this paper we present improved multithreading techniques that can easily tolerate latencies of hundreds of cycles, and yet only require a small number of threads per processor. High performance is achieved by introducing an explicit context switch instruction that can be used by a simple optimizing compiler to group together several shared accesses. This grouping of shared accesses dramatically reduces the frequency of context switches compared to simpler multithreading models. The combination of our techniques achieves efficiencies of 80% or higher on a broad set of applications.

The Fluent abstract machine
Abstract : The fluent abstract machine supports a very powerful programming model. In addition to arbitrary access patterns, the instruction repertoire of the fluent machine also includes the multiprefix operation and high-level set operations. The fluent machine consists of over one hundred thousand processors interconnected by a butterfly network. The efficiency of the fluent machine derives from a very simple router, which effectively eliminates the possibility of congestion. The routing hardware is extremely simple inexpensive, and probably efficient.

Dynamic tree embeddings in butterflies and hypercubes
This paper presents simple randomized algorithms for dynamically embedding M-node binary trees in either a butterfly or a hypercube network of N processors. These algorithms are dynamic in the sense that the tree to be embedded may start as one node and grow by dynamically spawning children. The nodes are incrementally embedded as they are spawned. Thus, the algorithm is especially suited for maintaining dynamic tree structures like those in divide-and-conquer and branch-and-bound algorithms.In the embeddings, the paper seeks to optimize the load on the processors of the network, the dilation of the tree edges, and the congestion on the network edges, in order to satisfy the demands of load balancing, process locality, and communication efficiency. The paper begins by presenting a simple level-by-level scheme for dynamically embedding trees in a butterfly network, and by successive modifications. The following results are obtained: 1. An embedding algorithm for the hypercube that achieves dilation 1 and, ...

Multiprocessor/memory interconnection network wherein messages sent through the network to the same memory are combined
Optimal speedup for backtrack search on a butterfly network
A variation on SVD based image compression
Fluent parallel computation
The communication efficiency of meshes, boolean cubes and cube connected cycles for wafer scale integration
In this paper we analyze the emulation of two-dimensional meshes, butterfly networks, and spanning trees on meshes, Boolean cubes, and Cube Connected Cycles (CCC) networks. We consider three timing models for signal propagation dong a wire: constant delay, capacitive delay, and resistive delay. We ais0 present novel layouts for hypercubes and CCCs that offer better performance for some problems, while essentially maintainingthe performance for other problems. The mesh interconnection performs better on all emulations for all delay models,if the communication throughput determines the performance. With resistive delay model, meshes also offer the best latency for all emulations. The hypercube and CCC layouts yield lower latency for emulating butterlly networks and spanning trees for the constant delay and capacitive delay models.

A new formulation and a column generation-based heuristic for the multiple depot vehicle scheduling problem
Randomized load balancing for tree-structured computation
Studies the performance of a randomized algorithm for balancing load across a multiprocessor executing a dynamic irregular task tree. Specifically, we show that the time taken to explore a task tree is likely to be within a small constant factor of an inherent lower bound for the tree instance. Our model permits arbitrary task times and overlap between computation and load balance, and thus extends earlier work (R.M. Karp and Y. Zhang, 1988) which assumed fixed cost tasks and used a bulk synchronous style in which the system alternated between distinct computing and load balancing steps. Our analysis is supported by experiments with application codes, demonstrating that the efficiency is high enough to make this method practical.<<ETX>>

Virtual processor techniques in a SIMD multiprocessor array
Parallel Numerics.- Teraflops Computing: A Challenge to Parallel Numerics?.- Non-standard Parallel Solution Strategies for Distributed Sparse Linear Systems.- Parallel Numerics.- Optimal Tridiagonal Solvers on Mesh Interconnection Networks.- Parallel Pivots LU Algorithm on the Cray T3E.- Experiments with Parallel One - Sided and Two - Sided Algorithms for SVD.- Combined Systolic Array for Matrix Portrait Computation.- Parallel Numerics.- A Class of Explicit Two-Step Runge-Kutta Methods with Enlarged Stability Regions for Parallel Computers.- A Parallel Strongly Implicit Algorithm for Solving of Diffusion Equations.- A Parallel Algorithm for Lagrange Interpolation on k-ary n-Cubes.- Parallel Numerics.- Parallel Quasi-Monte Carlo Integration Using (t,s)-Sequences.- Parallel Random Number Generation: Long-Range Correlations Among Multiple Processors.- A Monte-Carlo Method with Inherent Parallelism for Numerical Solving Partial Differential Equations with Boundary Conditions.- Parallel Numerics.- Blocking Techniques in Numerical Software.- HPF and Numerical Libraries.- paradeis: An Object Library for Parallel Sparse Array Computation.- Parallel Numerics.- Performance Analysis and Derived Parallelization Strategy for a SCF Program at the Hartree Fock Level.- Computational Issues in Optimizing Ophthalmic Lens.- Parallel Finite Element Modeling of Solidification Processes.- Parallel Computing in Image Processing, Video Processing, and Multimedia.- Architectural Approaches for Multimedia Processing.- On Parallel Reconfigurable Architectures for Image Processing.- Parallel Computing in Image Processing, Video Processing, and Multimedia.- Parallel Multiresolution Image Segmentation with Watershed Transformation.- Solving Irregular Inter-processor Data Dependency in Image Understanding Tasks.- A New Parallelism Management Scheme for Multiprocessor Systems.- Parallel Computing in Image Processing, Video Processing, and Multimedia.- A Flexible VLSI Parallel Processing System for Block-Matching Motion Estimation in Low Bit-Rate Video Coding Applications.- Hierarchical Block Matching Motion Estimation on a Hypercube Multiprocessor.- Classification Based Speed-Up Methods for Fractal Image Compression on Multicomputers.- Accurate Motion Estimation in Image Sequences: Massive vs. Distributed Parallelism.- Parallel Computing in Image Processing, Video Processing, and Multimedia.- A Real-Time Distributed Video Image Processing System on PC-Cluster.- Modeling and Scheduling for MPEG-4 Based Video Encoder Using a Cluster of Workstations.- Fractal Video Compression on Shared Memory Systems.- The Split-Proxy Approach: A New Architecture for Parallel Video Servers .- Parallel Computing in Image Processing, Video Processing, and Multimedia.- A Wavelet Toolbox for Large Scale Image Processing.- Hardware and Software Aspects for 3-D Wavelet Decomposition on Shared Memory MIMD Computers.- On the Parallel Implementation of the Fast Wavelet Packet Transform on MIMD Distributed Memory Environments.- Algorithms and Programming Paradigms for 2-D Wavelet Packet Decomposition on Multicomputers and Multiprocessors.- Real-Time Layered Video Compression Using SIMD Computation.- Parallel Computing in Image Processing, Video Processing, and Multimedia.- Parallelisation of a Satellite Signal Processing Code - Strategies and Tools.- MMIPPS - A Software Package for Multitemporal and Multispectral Image Processing on Parallel Systems.- Parallel Matching of Synthetic Aperture Radar Images.- General Aspects of Parallel Computation.- Parallel Decomposition of Distance-Hereditary Graphs.- Asynchronous Parallel Construction of Recursive Tree Hierarchies.- The Locality Property in Topological Irregular Graph Hierarchies.- General Aspects of Parallel Computation.- Geometry-Aided Rectilinear Partitioning of Unstructured Meshes.- Reducing Cache Conflicts by a Parametrized Memory Mapping.- Optimizing I/O for Irregular Applications on Distributed-Memory Machines.- General Aspects of Parallel Computation.- Cellular Multiprocessor Arrays with Adaptive Resource Utilization.- NOPE: A Nondeterministic Program Evaluator.- Visual-MCM: Visualising Execution Histories on Multiple Memory Consistency Models.- General Aspects of Parallel Computation.- High Performance Implementation of MPI for Myrinet.- Parallel Cluster Computing with IEEE1394-1995.- Simulating Load Balancing on Heterogeneous Workstation Clusters.- General Aspects of Parallel Computation.- Global Virtual Time Approximation for Split Queue Time Warp.- MPI-parallelized Radiance on SGI CoW and SMP.- Parallel Sub-collection Join Query Algorithms for a High Performance Object-Oriented Database Architecture.- General Aspects of Parallel Computation.- An Evaluation of Parallel Computing in PC Clusters with Fast Ethernet.- Parallel MPEG-2 Encoder on ATM and Ethernet-Connected Workstations.- Block and Partitioned Neville Elimination.- An Object-Oriented DataBase for Movies-on-Demand: Two Approaches .- Parallel Tree Algorithms for N-body Simulations.- Parallel Numerical Algorithms for Distributed Memory Machines.- Dynamic Scheduling on a Network Heterogeneous Computer System.- Interaction between PVM Parameters and Communication Performances on ATM Networks.- How To Share a Divisible Load in a Hypercube.- Overlapped Four-Step FFT Computation.- Design of Parallel Processing System for Facial Image Retrieval.- Inter-procedural Analysis for Parallelization of Java Programs.- Fast Recursive Computation of Local Axial Moments by Using Primitive Kernel Functions.- Speed Up Estimation for a Parallel Method for Systems of Linear Ordinary Differential Equations.- Efficient Parallel Algorithms for Dense Cholesky Factorization.

Scattering and gathering messages in networks of processors
The operations of scattering and gathering in a network of processors involve one processor of the network (P/sub 0/) communicating with all other processors. In scattering, P/sub 0/ sends distinct messages to P/sub 0/. The authors consider networks that are trees of processors. Algorithms for scattering messages from and gathering messages to the processor that resides at the root of the tree are presented. The algorithms are quite general, in that the messages transmitted can differ arbitrarily in length; quite strong, in that they send messages along noncolliding paths, and hence do not require any buffering or queueing mechanisms in the processors; and quite efficient in that algorithms for scattering in general trees are optimal, the algorithm for gathering in a path is optimal and the algorithms for gathering in general trees are nearly optimal. The algorithms can easily be converted using spanning trees to efficient algorithms for scattering and gathering in networks of arbitrary topologies. >

Parallelism and locality in priority queues
We explore two ways of incorporating parallelism into priority queues. The first is to speed up the execution of individual priority operations so that they can be performed one operation per time step, unlike sequential implementations which require O(log N) time steps per operation for an N element heap. We give an optimal parallel implementation that uses a linear array of O(log N) processors. Second, we consider parallel operations on the priority queue. We show that using a d-dimensional array (constant d) of P processors we can insert or delete the smallest P elements from a heap in time O(P/sup 1/d/log/sup 1-1/d/ P), where the number of elements in the heap is assumed to be polynomial in P. We also show a matching lower bound, based on communication complexity arguments, for a range of deterministic implementations. Finally, using randomization, we show that the time can be reduced to the optimal O(P/sup 1/d/) time with high probability.<<ETX>>

Commuting with delay prone buses
What is the fas tes t way to go from point X to point Y within a city using its public bus service, with bus changes as necessary? If buses ran according to a schedule this is an easy query it can be solved by using classical shor tes t pa th a lgor i thms. Unfortunately, as everyone knows, buses in most cities hardly confirm to a fixed schedule. Once you get into a bus it usually takes a predictable a m o u n t of t ime to reach its destinb.tion, bu t the amoun t of t ime you spend waiting for it is qui te r a ndom and can usually be es t imated only statistically. We present an a lgor i thm to generate travel p lans for such conditions. Our plans allow actions of the form "wai t for buses with route n u m b e r R1,R2 ... if you get into R1 get off at ... and wait for ..., else if you get into R2 ..." and are modeled as a tree. We gua ran tee tha t the expected t ime taken by the plan we genera te will be min imum over all possible plans for travel between the given s tar t ing point and dest inat ion. The algori thm has been implemented for the bus sys tem of Mumbai , and it genera tes the (optimal) travel plan essentially instantaneously. A t ranscr ip t of a session with the p rog ram is

On bufferless routing of variable length messages in leveled networks
We study the most general communication paradigm on a multiprocessor, wherein each processor has a distinct message (of possibly distinct lengths) for each other processor. We study this paradigm, which we call chatting, on multiprocessors that do not allow messages once dispatched ever to be delayed on their routes. By insisting on oblivious routes for messages, we convert the communication problem to a pure scheduling problem. We introduce the notion of a virtual chatting schedule, and we show how efficient chatting schedules can often be produced from efficient virtual chatting schedules. We present a number of strategies for producing efficient virtual chatting schedules on a variety of network topologies.

A simpler analysis of the Karp-Zhang parallel branch-and-bound method
Karp and Zhang presented a general method for deriving randomized parallel branch-and-boxed algorithms from sequential ones. They showed that with high probability the resulting algorithms attained optimal speedup to within a constant: factor, for large enough problems. We present an alternate analysis of their method. Our analysis is considerably simpler, and gives good bounds even for small problem sizes.

Precedence constrained scheduling in (2− 73p+ 1)⋅ optimal
DONE