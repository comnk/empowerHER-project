Managing dynamic enterprise and urgent workloads on clouds using layered queuing and historical performance models
Predictive and dynamic resource allocation for enterprise applications
Dynamic resource allocation has the potential to provide significant increases in total revenue in enterprise systems through the reallocation of available resources as the demands on hosted applications change over time. This paper investigates the combination of workload prediction algorithms and switching policies: the former aim to forecast the workload associated with Internet services, the latter switch resources between applications according to certain system criteria. An evaluation of two well known switching policies – the proportional switching policy (PSP) and the bottleneck aware switching policy (BSP) – is conducted in the context of seven workload prediction algorithms. This study uses real-world workload traces consisting of approximately 3.5M requests, and models a multi-tiered, cluster-based, multi-server solution. The results show that a combination of the bottleneck aware switching policy and workload predictions based on an autoregressive, integrated, moving-average model can improve system revenue by as much as 43%.

A system for dynamic server allocation in application server clusters
Application server clusters are often used to service high-throughput web applications. In order to host more than a single application, an organisation will usually procure a separate cluster for each application. Over time the utilisation of the clusters will vary, leading to variation in the response times experienced by users of the applications. Techniques that statically assign servers to each application prevent the system from adapting to changes in the workload, and are thus susceptible to providing unacceptable levels of service. This paper investigates a system for allocating server resources to applications dynamically, thus allowing applications to automatically adapt to variable workloads. Such a scheme requires meticulous system monitoring, a method for switching application servers between \text it {server pools} and a means of calculating when a server switch should be made (balancing switching cost against perceived benefits). Experimentation is performed using such a switching system on a Web application testbed hosting two applications across eight application servers. The testbed is used to compare several theoretically derived switching policies under a variety of workloads. Recommendations are made as to the suitability of different policies under different workload conditions.

Dynamic resource allocation in enterprise systems
It is common that Internet service hosting centres use several logical pools to assign server resources to different applications, and that they try to achieve the highest total revenue by making efficient use of these resources. In this paper, multi-tiered enterprise systems are modelled as multi-class closed queueing networks, with each network station corresponding to each application tier. In such queueing networks, bottlenecks can limit overall system performance, and thus should be avoided. We propose a bottleneck-aware server switching policy, which responds to system bottlenecks and switches servers to alleviate these problems as necessary. The switching engine compares the benefits and penalties of a potential switch, and makes a decision as to whether it is likely to be worthwhile switching. We also propose a simple admission control scheme, in addition to the switching policy, to deal with system overloading and optimise the total revenue of multiple applications in the hosting centre. Performance evaluation has been done via simulation and results are compared with those from a proportional switching policy and also a system that implements no switching policy. The experimental results show that the combination of the bottleneck-aware switching policy and the admission control scheme consistently outperforms the other two policies in terms of revenue contribution.

A modular failure-aware resource allocation architecture for cloud computing
The prevalence of cloud computing environments and the ever increasing reliance of large organisations on computational resources has meant that service providers must operate at unprecedented scales and levels of efficiency. Dynamic resource allocation (DRA) policies have been shown to allow service providers to improve resource utilisation and operational efficiency in presence of unpredictable demands, hence maximising profitability. However, practical considerations, such as power and space, have led service providers to adopt rack based approaches to application servicing. This co-location of computation resources, and the associated common provision of utilities it encourages, has immediate implications for system dependability. Specifically, in the presence of rack crash failures which can lead to all the servers within a rack becoming unavailable, resource allocation policies need to be cognisant of failures. In this paper, we address this issue and make the following specific contributions: (i) we present a modular architecture for failure-aware resource allocation, where a performance- oriented DRA policy is composed with a failure-aware resource allocator, (ii) we propose a metric, called Capacity Loss, to capture the exposure of an application to a rack failure, (iii) we develop an algorithm for reducing the proposed metric across all applications in a system operating under a DRA policy, and (iv) we evaluate the effectiveness of the proposed architecture on a large-scale DRA policy in context of rack failures, ultimately concluding that our approach reduces the number of failed requests as compared to a single random allocation. The main benefit of our approach is that we have developed a failure-aware resource allocation framework that can work in tandem with any DRA policy.

Model-driven server allocation in distributed enterprise systems
Internet service providers (ISPs) usually use several server pools to host different web applications, to ensure smooth system management and minimum interference between applications. The workload demand in each of the pools can vary dramatically due to a number of factors, including timing and the types of the hosted applications. Therefore, it is desirable that servers should be able to switch between pools to optimise resource usage and maximise company revenue. Internet applications can be modelled as multi-tier queueing networks, with each network station corresponding to each application tier. The advantage of using an analytical model is that performance metrics can be easily computed, and potential system bottlenecks can be identified without running the actual system. In this paper, an analytical model is used to assist dynamic resource allocation in server pools. In addition, an admission control scheme is also used to deal with system overloading. Performance evaluation is conducted via simulation and the experimental results show the benefits of our approach for various workload scenarios.

Scheduling dependent tasks in edge networks
In this paper, we focus on the problem of offloading of jobs composed of dependent tasks in a mobile edge network (MEN). We formalise the problem and develop a heuristic lower the completion time of a job executing in MEN than when executing on a mobile device. We show the viability of the heuristic through our simulation results.

Dynamic resource allocation and active predictive models for enterprise applications
This work is concerned with dynamic resource allocation for multi-tiered, cluster-based web hosting environ- ments. Dynamic resource allocation is reactive, that is, when overloading occurs in one resource pool, servers are moved from another (quieter) pool to meet this demand. Switching servers comes with some overhead, so it is important to weigh up the costs of the switch against possible system gains. In this paper we combine the reactive behaviour of two well known switching policies – the Proportional Switching Policy (PSP) and the Bottleneck Aware Switching Policy (BSP) – with the proactive properties of several workload forecasting models. Seven forecasting models are used, including Last Observation, Simple Algorithm, Sample Moving Average, Exponential Moving Algorithm, Low Pass Filter and Autoregressive Moving Average. As each of the forecasting schemes has its own bias, we also develop three meta-forecasting algorithms (the Active Window Model, the Voting Model and the Selective Model) to ensure consistent and improved results. We show that request servicing capability can be improved by as much as 40% when the right combination of dynamic server switching and workload forecasting are used. As important is that we can generate consistently improved results, even when we apply this scheme to real-world, highly-variable workload traces from several sources.

A framework for data center scale dynamic resource allocation algorithms
The scale and complexity of online applications and e-business infrastructures has led service providers to rely on the capabilities of large-scale hosting platforms, i.e., data centers. Dynamic resource allocation (DRA) algorithms have been shown to allow server resource allocation to be matched with application workload, which can improve server resource utilisation and drive down costs for service providers. However, research on DRA algorithms has almost exclusively focused on their performance characteristics at small-scale, precluding their useful application in commercial hosting environments, such as those dedicated to supporting cloud computing. In this paper, we show, and subsequently propose a framework to address, the scalability problems of current DRA algorithms. We then build on the proposed framework to develop a highly-scalable algorithm for dynamic resource allocation.

The effect of server reallocation time in dynamic resource allocation
Since web workloads are known to vary dynamically with time, in this paper, we argue that dynamic resource allocation techniques are necessary to provide guarantees to web applications running on shared data centers. To address this issue, we use a system architecture that combines online measurements with prediction and resource allocation techniques. To capture the transient behavior of the application workloads, we model a server resource using a time-domain description of a generalized processor sharing (GPS) server. This model relates application resource requirements to their dynamically changing workload characteristics. The parameters of this model are continuously updated using an online monitoring and prediction framework. This framework uses time series analysis techniques to predict expected workload parameters from measured system metrics. We then employ a constrained non-linear optimization technique to dynamically allocate the server resources based on the estimated application requirements. The main advantage of our techniques is that they capture the transient behavior of applications while incorporating nonlinearity in the system model. We evaluate our techniques using simulations with synthetic as well as real-world web workloads. Our results show that these techniques can judiciously allocate system resources, especially under transient overload conditions.

Dynamic resource allocation for multi-tiered, cluster-based web hosting environments
Dynamic active window management: a method for improving revenue generation in dynamic enterprise systems
In dynamic resource allocation systems, servers are moved between pools when overloading is detected. In this work, we investigate the impact to such systems of combining three adaptive monitoring techniques. First we employ two well known switching policies -- the Proportional Switching Policy (PSP) and the Bottleneck Aware Switching Policy (BSP) -- to move servers between server pools as appropriate. Second we use a meta-forecasting technique to predict the movement in future system workload. Third, we use a Dynamic Active Window Model (DAWM), which defines the period over which workload data is analysed. We have previously shown that request servicing capability can be improved by as much as 40\% when the right combination of dynamic server switching and workload forecasting are used. This extended model shows that a further 51.5\% improvement can be achieved when the switching server policy, meta-forecasting and dynamic active window management are employed together over a real-world workload based on Internet traces.

Towards Understanding Checkpointing in Transiently Powered IoT Networks
The finite energy supply available for computation and communication is one of the major shortcomings in IoT/sensor networks. To circumvent this issue, energy harvesting has been proposed to enable embedded devices to mitigate their dependency on traditional battery-driven power sources. However, energy supply due to energy harvesting often varies, leading to nodes crashing due to energy exhaustion, with the application(s) losing their state. Efficient state checkpointing in non-volatile memory (NVM) has been proposed to enable forward progress, albeit at the expense of significant overheads (viz., energy and time). This work is based on the observation that, as the network evolves, IoT applications adapt/refresh to ensure they have up-to-date state of information, thereby reducing the need for checkpointing. However, little is known about (i) whether checkpointing is actually needed for any application, (ii) if needed, what to checkpoint for a given application and (iii) when checkpointing is needed in an application. In this paper, we address the first two problems and make the following novel contributions: (i) we formalise a variant of the checkpointing problem for IoT networks and show that it is NP-complete, (ii) we introduce a theory of state checkpointing and develop two types of checkpointing: (a) local checkpointing and (b) network checkpointing, (iii) we show that there is no general checkpointing strategy for the network-based application that will improve efficiency and (iv) we briefly survey related works and show that most if not all, works on checkpointing of IoT-based applications that lead to increased efficiency have focused on node-centric (i.e., local) programs. We run experiments on an actual testbed to confirm our expectations, and our results show that checkpointing can actually have a negative impact on applications when wrongly used. Thus, compared to existing works, we make major gains: No checkpointing is needed for network-based applications.

Towards effective dynamic resource allocation for enterprise applications
The growing use of online services requires substantial supporting infrastructure. 
The efficient deployment of applications relies on the cost effectiveness of 
commercial hosting providers who deliver an agreed quality of service as governed 
by a service level agreement for a fee. The priorities of the commercial 
hosting provider are to maximise revenue, by delivering agreed service levels, 
and minimise costs, through high resource utilisation. 
In order to deliver high service levels and resource utilisation, it may be 
necessary to reorganise resources during periods of high demand. This reorganisation 
process may be manual or alternatively controlled by an autonomous 
process governed by a dynamic resource allocation algorithm. Dynamic resource 
allocation has been shown to improve service levels and utilisation and 
hence, profitability. 
In this thesis several facets of dynamic resource allocation are examined 
to asses its suitability for the modern data centre. Firstly, three theoretically 
derived policies are implemented as a middleware for a modern multi-tier Web 
application and their performance is examined under a range of workloads in a 
real world test bed. 
The scalability of state-of-the art resource allocation policies are explored in 
two dimensions, namely the number of applications and the quantity of servers 
under control of the resources allocation policy. The results demonstrate that current policies presented in the literature demonstrate poor scalability in one 
or both of these dimensions. A new policy is proposed which has significantly 
improved scalability characteristics and the new policy is demonstrated at scale 
through simulation. 
The placement of applications in across a datacenter makes them susceptible 
to failures in shared infrastructure. To address this issue an application 
placement mechanism is developed to augment any dynamic resource allocation 
policy. The results of this placement mechanism demonstrate a significant improvement 
in the worst case when compared to a random allocation mechanism. 
A model for the reallocation of resources in a dynamic resource allocation 
system is also devised. The model demonstrates that the assumption of a constant 
resource reallocation cost is invalid under both physical reallocation and 
migration of virtualised resources.

A system for dynamic resource allocation in application server clusters
DONE