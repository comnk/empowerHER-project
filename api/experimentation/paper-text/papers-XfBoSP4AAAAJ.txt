Tissue classification based on 3D local intensity structures for volume rendering
This paper describes a novel approach to tissue classification using three-dimensional (3D) derivative features in the volume rendering pipeline. In conventional tissue classification for a scalar volume, tissues of interest are characterized by an opacity transfer function defined as a one-dimensional (1D) function of the original volume intensity. To overcome the limitations inherent in conventional 1D opacity functions, we propose a tissue classification method that employs a multidimensional opacity function, which is a function of the 3D derivative features calculated from a scalar volume as well as the volume intensity. Tissues of interest are characterized by explicitly defined classification rules based on 3D filter responses highlighting local structures, such as edge, sheet, line, and blob, which typically correspond to tissue boundaries, cortices, vessels, and nodules, respectively, in medical volume data. The 3D local structure filters are formulated using the gradient vector and Hessian matrix of the volume intensity function combined with isotropic Gaussian blurring. These filter responses and the original intensity define a multidimensional feature space in which multichannel tissue classification strategies are designed. The usefulness of the proposed method is demonstrated by comparisons with conventional single-channel classification using both synthesized data and clinical data acquired with CT (computed tomography) and MRI (magnetic resonance imaging) scanners. The improvement in image quality obtained using multichannel classification is confirmed by evaluating the contrast and contrast-to-noise ratio in the resultant volume-rendered images with variable opacity values.

Her 2 challenge contest: a detailed assessment of automated her 2 scoring algorithms in whole slide images of breast cancer tissues
Analysis of retinal vasculature using a multiresolution hermite model
This paper presents a vascular representation and segmentation algorithm based on a multiresolution Hermite model (MHM). A two-dimensional Hermite function intensity model is developed which models blood vessel profiles in a quad-tree structure over a range of spatial resolutions. The use of a multiresolution representation simplifies the image modeling and allows for a robust analysis by combining information across scales. Estimation over scale also reduces the overall computational complexity. As well as using MHM for vessel labelling, the local image modeling can accurately represent vessel directions, widths, amplitudes, and branch points which readily enable the global topology to be inferred. An expectation-maximization (EM) type of optimization scheme is used to estimate local model parameters and an information theoretic test is then applied to select the most appropriate scale/feature model for each region of the image. In the final stage, Bayesian stochastic inference is employed for linking the local features to obtain a description of the global vascular structure. After a detailed description and analysis of MHM, experimental results on two standard retinal databases are given that demonstrate its comparative performance. These show MHM to perform comparably with other retinal vessel labelling methods

Computer-assisted surgical planning for cerebrovascular neurosurgery
OBJECTIVE
We used three-dimensional reconstructed magnetic resonance images for planning the operations of 16 patients with various cerebrovascular diseases. We studied the cases of these patients to determine the advantages and current limitations of our computer-assisted surgical planning system as it applies to the treatment of vascular lesions.


METHODS
Magnetic resonance angiograms or thin slice gradient echo magnetic resonance images were processed for three-dimensional reconstruction. The segmentation, based on the signal intensities and voxel connectivity, separated each anatomic structure of interest, such as the brain, vessels, and skin. A three-dimensional model was then reconstructed by surface rendering. This three-dimensional model could be colored, made translucent, and interactively rotated by a mouse-controlled cursor on a workstation display. In addition, a three-dimensional blood flow analysis was performed, if necessary. The three-dimensional model was used to assist in three stages of surgical planning, as follows: 1) to choose the best method of intervention, 2) to evaluate surgical risk, 3) to select a surgical approach, and 4) to localize lesions.


RESULTS
The generation of three-dimensional models allows visualization of pathological anatomy and its relationship to adjacent normal structures, accurate lesion volume determination, and preoperative computer-assisted visualization of alternative surgical approaches.


CONCLUSION
Computer-assisted surgical planning is useful for patients with cerebrovascular disease at various stages of treatment. Lesion identification, therapeutic and surgical option planning, and intraoperative localization are all enhanced with these techniques.

The Bhattacharyya space for feature selection and its application to texture segmentation
Towards electronically assisted peer assessment: a case study
One of the primary goals of formative assessment is to give informative feedback to the learner on their progress and attainment of the learning objectives. However, when the student/tutor ratios are large, effective and timely feedback is hard to achieve. Many testing systems have been developed that use multiple choice questions (MCQ), which are easy to mark automatically. MCQ tests are simple to develop and administer through Web-based technologies (browsers, Internet and Web servers). One of the principal drawbacks of current systems is that the testing format is limited to MCQ and general questions requiring free responses are not included because marking cannot be easily automated. Consequently, many learning tasks, such as the correctness and style of solutions to programming problems, cannot be assessed automatically. Our approach is a hybrid system combining MCQ testing with free response questions. Our system, OASYS, marks MCQs automatically and then controls the anonymous distribution of completed scripts amongst learners for peer assessment of free response answers. We briefly describe the design and implementation of OASYS, which is built on freely available technologies. We present and discuss findings from a case study which used OASYS for 240 students taking a programming class involving four assessed programming laboratories in groups of approximately forty students

Kernel designs for efficient multiresolution edge detection and orientation estimation
Deals with the design of filter kernels having specified radial and angular frequency responses based on combined optimization and frequency sampling. This is used to generate small-radius, low-pass, and edge-detection kernels for multiresolution pyramids. The performance of the new kernels in estimating orientation is shown to be significantly better than that of other commonly used pyramid kernels. >

Using local 3D structure for segmentation of bone from computer tomography images
In this paper we focus on using local 3D structure for segmentation. A tensor descriptor is estimated for each neighbourhood, i.e. for each voxel in the data set. The tensors are created from a combination of the outputs form a set of 3D quadrature filters. The shape of the tensors describe locally the structure of the neighbourhood in terms of how much it is like a plane, a line, and a sphere. We apply this to segmentation of bone from Computer Tomography data (CT). Traditional methods are based purely on gray-level value discrimination and have difficulties in recovering thin bone structures due to so called partial voluming, a problem which is present in all such sampled data. We illuminate the partial voluming problem by showing that thresholding creates complicated artifacts even if the signal is densely enough sampled and can be perfectly reconstructed. The unwanted effects of thresholding can be reduced by a change of the signal basis. We show that by using additional local structure information can significantly reduce the degree of sampling artifacts. Evaluation of the method on a clinical case is presented, the segmentation of a human skull from a CT volume. The method shows that many of the thin bone structures which disappear in a pure thresholding can be recovered.

Tensor controlled local structure enhancement of CT images for bone segmentation
Volumetric texture segmentation by discriminant feature selection and multiresolution classification
In this paper, a multiresolution volumetric texture segmentation (M-VTS) algorithm is presented. The method extracts textural measurements from the Fourier domain of the data via subband filtering using an orientation pyramid (Wilson and Spann, 1988). A novel Bhattacharyya space, based on the Bhattacharyya distance, is proposed for selecting the most discriminant measurements and producing a compact feature space. An oct tree is built of the multivariate features space and a chosen level at a lower spatial resolution is first classified. The classified voxel labels are then projected to lower levels of the tree where a boundary refinement procedure is performed with a three-dimensional (3-D) equivalent of butterfly filters. The algorithm was tested with 3-D artificial data and three magnetic resonance imaging sets of human knees with encouraging results. The regions segmented from the knees correspond to anatomical structures that can be used as a starting point for other measurements such as cartilage extraction

Cross-modal prototype driven network for radiology report generation
Radiology report generation (RRG) aims to describe automatically a radiology image with human-like language and could potentially support the work of radiologists, reducing the burden of manual reporting. Previous approaches often adopt an encoder-decoder architecture and focus on single-modal feature learning, while few studies explore cross-modal feature interaction. Here we propose a Cross-modal PROtotype driven NETwork (XPRONET) to promote cross-modal pattern learning and exploit it to improve the task of radiology report generation. This is achieved by three well-designed, fully differentiable and complementary modules: a shared cross-modal prototype matrix to record the cross-modal prototypes; a cross-modal prototype network to learn the cross-modal prototypes and embed the cross-modal information into the visual and textual features; and an improved multi-label contrastive loss to enable and enhance multi-label prototype learning. XPRONET obtains substantial improvements on the IU-Xray and MIMIC-CXR benchmarks, where its performance exceeds recent state-of-the-art approaches by a large margin on IU-Xray and comparable performance on MIMIC-CXR.

Robust detection of microaneurysms for sight threatening retinopathy screening
Diabetic retinopathy is one of the major causes of blindness. However, diabetic retinopathy does not usually causea loss of sight until it has reached an advanced stage. The earliest sign of the disease are microaneurysms (MA) which appear as small red dots on retinal fundus images. Various screening programmes have been established in the UK and other countries to collect and assess images on a regular basis, especially in the diabetic population. A considerable amount of time and money is spent in manually grading these images, a large percentage of which are normal. By automatically identifying the normal images, the manual workload and costs could be reduced greatly while increasing the effectiveness of the screening programmes. A novel method of microaneurysm detection from digital retinal screening images is proposed. It is based on filtering using complex-valued circular-symmetric filters, and an eigen-image, morphological analysis of the candidate regions to reduce the false-positve rate. We detail the image processing algorithms and present results on a typical set of 89 image from a published database. Our method is shown to have a best operating sensitivity of 82.6% at a specificity of 80.2% which makes it viable for screening. We discuss the results in the context of a model of visual search and the ROC curves that it can predict.

Volumetric texture description and discriminant feature selection for MRI
Semantic annotation for computational pathology: multidisciplinary experience and best practice recommendations
Recent advances in whole‐slide imaging (WSI) technology have led to the development of a myriad of computer vision and artificial intelligence‐based diagnostic, prognostic, and predictive algorithms. Computational Pathology (CPath) offers an integrated solution to utilise information embedded in pathology WSIs beyond what can be obtained through visual assessment. For automated analysis of WSIs and validation of machine learning (ML) models, annotations at the slide, tissue, and cellular levels are required. The annotation of important visual constructs in pathology images is an important component of CPath projects. Improper annotations can result in algorithms that are hard to interpret and can potentially produce inaccurate and inconsistent results. Despite the crucial role of annotations in CPath projects, there are no well‐defined guidelines or best practices on how annotations should be carried out. In this paper, we address this shortcoming by presenting the experience and best practices acquired during the execution of a large‐scale annotation exercise involving a multidisciplinary team of pathologists, ML experts, and researchers as part of the Pathology image data Lake for Analytics, Knowledge and Education (PathLAKE) consortium. We present a real‐world case study along with examples of different types of annotations, diagnostic algorithm, annotation data dictionary, and annotation constructs. The analyses reported in this work highlight best practice recommendations that can be used as annotation guidelines over the lifecycle of a CPath project.

Parallel file system analysis through application I/O tracing
Input/Output (I/O) operations can represent a significant proportion of the run-time of parallel scientific computing applications. Although there have been several advances in file format libraries, file system design and I/O hardware, a growing divergence exists between the performance of parallel file systems and the compute clusters that they support. In this paper, we document the design and application of the RIOT I/O toolkit (RIOT) being developed at the University of Warwick with our industrial partners at the Atomic Weapons Establishment and Sandia National Laboratories. We use the toolkit to assess the performance of three industry-standard I/O benchmarks on three contrasting supercomputers, ranging from a mid-sized commodity cluster to a large-scale proprietary IBM BlueGene/P system. RIOT provides a powerful framework in which to analyse I/O and parallel file system behaviour—we demonstrate, for example, the large file locking overhead of IBM's General Parallel File System, which can consume nearly 30% of the total write time in the FLASH-IO benchmark. Through I/O trace analysis, we also assess the performance of HDF-5 in its default configuration, identifying a bottleneck created by the use of suboptimal Message Passing Interface hints. Furthermore, we investigate the performance gains attributed to the Parallel Log-structured File System (PLFS) being developed by EMC Corporation and the Los Alamos National Laboratory. Our evaluation of PLFS involves two high-performance computing systems with contrasting I/O backplanes and illustrates the varied improvements to I/O that result from the deployment of PLFS (ranging from up to 25× speed-up in I/O performance on a large I/O installation to 2× speed-up on the much smaller installation at the University of Warwick).

Reducing the run-time of MCMC programs by multithreading on SMP architectures
The increasing availability of multi-core and multiprocessor architectures provides new opportunities for improving the performance of many computer simulations. Markov chain Monte Carlo (MCMC) simulations are widely used for approximate counting problems, Bayesian inference and as a means for estimating very high-dimensional integrals. As such MCMC has found a wide variety of applications infields including computational biology and physics, financial econometrics, machine learning and image processing. This paper presents a new method for reducing the run-time of Markov chain Monte Carlo simulations by using SMP machines to speculatively perform iterations in parallel, reducing the runtime of MCMC programs whilst producing statistically identical results to conventional sequential implementations. We calculate the theoretical reduction in runtime that may be achieved using our technique under perfect conditions, and test and compare the method on a selection of multi-core and multi-processor architectures. Experiments are presented that show reductions in runtime of 35% using two cores and 55% using four cores.

Fingerprint matching method and apparatus
Unsupervised image segmentation combining region and boundary estimation
Tensor splats: Visualising tensor fields by texture mapped volume rendering
Inferring vascular structure from 2d and 3d imagery
DONE