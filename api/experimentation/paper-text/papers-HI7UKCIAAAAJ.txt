Automatic Segmentation of Pulmonary Lobes Using a Progressive Dense V-Network
Semi-supervised Multi-task Learning with Chest X-Ray Images
Fast and automatic segmentation of pulmonary lobes from chest CT using a progressive dense V-network
ABSTRACT Automatic, reliable lobe segmentation is crucial to the diagnosis, assessment, and quantification of pulmonary diseases. Existing pulmonary lobe segmentation techniques are prohibitively slow, undesirably rely on prior (airway/vessel) segmentation, and/or require user interactions for optimal results. We introduce a reliable, fast, and fully automated lung lobe segmentation method based on a Progressive Dense V-Network (PDV-Net). The proposed method can segment lung lobes in one forward pass of the network, with an average runtime of 2 seconds using a single Nvidia Titan XP GPU. An extensive robustness analysis of our method demonstrates reliable lobe segmentation of both healthy and pathological lungs in CT images acquired by scanners from different vendors, across various CT scan protocols and acquisition parameters.

Partly Supervised Multi-Task Learning
Semi-supervised learning has recently been attracting attention as an alternative to fully supervised models that require large pools of labeled data. Moreover, optimizing a model for multiple tasks can provide better generalizability than single-task learning. Leveraging self-supervision and adversarial training, we propose a novel, general purpose semi-supervised, multiple-task model—namely, self-supervised, semi-supervised, multi-task learning (S4MTL)—for accomplishing two important medical image analysis tasks: segmentation and diagnostic classification. Experimental results on chest and spine X-ray datasets confirm that our S4MTL model significantly outperforms semi-supervised single-task, semi/fully-supervised multi-task, and fully-supervised single-task models, even with a 50% reduction in class and segmentation labels.

Fully-Automated Analysis of Scoliosis from Spinal X-Ray Images
Scoliosis is a congenital disease in which the spine is deformed from its normal shape. Radiography is the most cost-effective and accessible modality for imaging the spine. Conventional spinal assessment, diagnosis of scoliosis, and treatment planning relies on tedious and time-consuming manual analysis of spine radiographs that is susceptible to observer variation. A reliable, fully-automated method that can accurately identify vertebrae, a crucial step in image-guided scoliosis assessment, is presently unavailable in the literature. Leveraging a novel, deep-learning-based image segmentation model, we develop an end-to-end spine radiograph analysis pipeline that automatically provides an accurate segmentation and identification of the vertebrae, culminating in the reliable estimation of the Cobb angle, the most widely used measurement to quantify the magnitude of scoliosis. Our experimental results with anterior-posterior spine X-ray images indicate that our system is effective in the identification and labeling of vertebrae, and can potentially provide assistance to medical practitioners in the assessment of scoliosis.

Multi-adversarial variational autoencoder networks
The unsupervised training of GANs and VAEs has enabled them to generate realistic images mimicking real-world distributions and perform unsupervised clustering or semi-supervised classification of images. Combining the power of these two generative models, we introduce a novel network architecture, Multi-Adversarial Variational autoEncoder Networks (MAVENs), which incorporate an ensemble of discriminators in a combined VAE-GAN network, with simultaneous adversarial learning and variational inference. We apply MAVENs to the generation of synthetic images and propose a new distribution measure to evaluate the quality of the generated images. Our experimental results using the computer vision datasets SVHN and CIFAR-10 demonstrate competitive performance against state-of-the-art semi-supervised models both in image generation and classification tasks.

SSIQA: Multi-Task Learning for Non-Reference CT Image Quality Assessment With Self-Supervised Noise Level Prediction
Reduction of CT radiation dose is important due to the potential effects on patients. But lowering dose incurs degradation in the reconstructed image quality, furthering compromise in the diagnostic and image-based analyses performance. Considering the patient health risks, high quality reference images cannot be easily obtained, making the assessment challenging. Therefore, automatic no-reference image quality assessment is desirable. Leveraging an innovative self-supervised regularization in a convolutional neural network, we propose a novel, fully automated, no-reference CT image quantification method namely self-supervised image quality assessment (SSIQA). Extensive experimentation via in-domain (abdomen CT) and cross-domain (chest CT) evaluations demonstrates SSIQA is accurate in quantifying CT image quality, generalized across the scan types, and consistent with the established metrics and different relative dose levels.

Upstream Machine Learning in Radiology
Multi-Adversarial Variational Autoencoder Nets for Simultaneous Image Generation and Classification
Estimation of adipose compartment volumes in CT images of a mastectomy specimen
Anthropomorphic software breast phantoms have been utilized for preclinical quantitative validation of breast imaging systems. Efficacy of the simulation-based validation depends on the realism of phantom images. Anatomical measurements of the breast tissue, such as the size and distribution of adipose compartments or the thickness of Cooper’s ligaments, are essential for the realistic simulation of breast anatomy. Such measurements are, however, not readily available in the literature. In this study, we assessed the statistics of adipose compartments as visualized in CT images of a total mastectomy specimen. The specimen was preserved in formalin, and imaged using a standard body CT protocol and high X-ray dose. A human operator manually segmented adipose compartments in reconstructed CT images using ITK-SNAP software, and calculated the volume of each compartment. In addition, the time needed for the manual segmentation and the operator’s confidence were recorded. The average volume, standard deviation, and the probability distribution of compartment volumes were estimated from 205 segmented adipose compartments. We also estimated the potential correlation between the segmentation time, operator’s confidence, and compartment volume. The statistical tests indicated that the estimated compartment volumes do not follow the normal distribution. The compartment volumes are found to be correlated with the segmentation time; no significant correlation between the volume and the operator confidence. The performed study is limited by the mastectomy specimen position. The analysis of compartment volumes will better inform development of more realistic breast anatomy simulation.

Spatial distribution of adipose compartments size, shape and orientation in a CT breast image of a mastectomy specimen
Development of efficient clinical breast imaging system has become a major concern in medical imaging today. Recent advancement in 3D imaging has allowed detailed analysis of breast anatomic structures. To assist with preclinical validation of breast imaging systems, high resolution anthropomorphic software phantoms have been introduced [1]. The phantoms developed at the University of Pennsylvania and Delaware State University simulate the breast outline covered by a skin layer, and the breast interior matrix of Cooper's ligaments which define compartments filled with adipose or fibro-glandular (dense) tissue. The simulation algorithms require input parameters such as number of compartments, distribution, and size and shape of adipose compartments. To obtain more realistic phantoms, these simulation parameters should be inferred from clinical images. In this study, we have investigated the distribution and spatial placement of adipose compartments in reconstructed CT images of a mastectomy specimen.

MultiMix: Sparingly Supervised, Extreme Multitask Learning From Medical Images
Semi-supervised learning from limited quantities of labeled data, an alternative to fully-supervised schemes, benefits by maximizing knowledge gains from copious unlabeled data. Furthermore, learning multiple tasks within the same model improves model generalizability. We propose MultiMix, a novel multitask learning model that jointly learns disease classification and anatomical segmentation in a sparingly supervised manner, while preserving explainability through bridge saliency between the two tasks. Extensive experimentation with varied quantities of labeled data in the training sets affirms the effectiveness of our multitasking model in classifying pneumonia and segmenting lungs from chest X-ray images. Moreover, both in-domain and cross-domain evaluations across the tasks further showcase the potential of our model to adapt to challenging generalization scenarios.

Personalized CT organ dose estimation from scout images
Generalized Multi-Task Learning from Substantially Unlabeled Multi-Source Medical Image Data
Deep learning-based models, when trained in a fully-supervised manner, can be effective in performing complex image analysis tasks, although contingent upon the availability of large labeled datasets. Especially in the medical imaging domain, however, expert image annotation is expensive, time-consuming, and prone to variability. Semi-supervised learning from limited quantities of labeled data has shown promise as an alternative. Maximizing knowledge gains from copious unlabeled data benefits semi-supervised learning models. Moreover, learning multiple tasks within the same model further improves its generalizability. We propose MultiMix, a new multi-task learning model that jointly learns disease classification and anatomical segmentation in a semi-supervised manner while preserving explainability through a novel saliency bridge between the two tasks. Our experiments with varying quantities of multi-source labeled data in the training sets confirm the effectiveness of MultiMix in the simultaneous classification of pneumonia and segmentation of the lungs in chest X-ray images. Moreover, both in-domain and cross-domain evaluations across these tasks further showcase the potential of our model to adapt to challenging generalization scenarios. Our code is available at https://github.com/ayaanzhaque/MultiMix

Self-Supervised, Semi-Supervised, Multi-Context Learning for the Combined Classification and Segmentation of Medical Images
To tackle the problem of limited annotated data, semi-supervised learning is attracting attention as an alternative to fully supervised models. Moreover, optimizing a multiple-task model to learn “multiple contexts” can provide better generalizability compared to single-task models. We propose a novel semi-supervised multiple-task model leveraging self-supervision and adversarial training—namely, self-supervised, semi-supervised, multi-context learning (S4MCL)—and apply it to two crucial medical imaging tasks, classification and segmentation. Our experiments on spine X-rays reveal that the S4MCL model significantly outperforms semi-supervised single-task, semi-supervised multi-context, and fully-supervised single-task models, even with a 50% reduction of classification and segmentation labels.

Progressive Adversarial Semantic Segmentation
Medical image computing has advanced rapidly with the advent of deep learning techniques. Deep convolutional neural networks can perform well given full supervision. However, the success of such fully-supervised models in various image analysis tasks (e.g., anatomy or lesion segmentation from medical images) depends on the availability of massive quantities of labeled data. Given small sample sizes, such models are prohibitively data biased with large domain shifts. To tackle this problem, we propose a novel end - to-end medical image segmentation model, namely Progressive Adversarial Semantic Segmentation (PASS), which can make improved and consistent pixel-wise segmentation predictions without requiring any domain-specific data during training. Our extensive experimentation with 8 public diabetic retinopathy and chest X-ray datasets confirms the effectiveness of PASS in accurate vascular and pulmonary segmentation, both for in-domain and cross-domain evaluations.

Optimization of the simulation parameters for improving realism in anthropomorphic breast phantoms
Virtual clinical trials (VCTs) were introduced as a preclinical alternative to clinical imaging trials, and for the evaluation of breast imaging systems. Realism in computer models of breast anatomy (software phantoms), critical for VCT performance, can be improved by optimizing simulation parameters based on the analysis of clinical images. We optimized the simulation to improve the realism of simulated tissue compartments, defined by the breast Cooper’s ligaments. We utilized the anonymized, previously acquired CT images of a mastectomy specimen to manually segment 205 adipose compartments. We generated 1,440 anthropomorphic breast phantoms based on octree recursive partitioning. These phantoms included variations of simulation parameters—voxel size, number of compartments, percentage of dense tissue, and shape and orientation of the compartments. We compared distributions of the compartment volumes in segmented CT images and phantoms using Kolmogrov-Smirnov (KS) distance, Kullback-Leibler (KL) divergence and a novel distance metric (based on weighted sum of distribution descriptors differences). We identified phantoms with the size distributions closest to CT images. For example, KS resulted in the phantom with 1000 compartments, ligament thickness of 0.4 mm and skin thickness of 12 mm. We applied multilevel analysis of variance (ANOVAN) to these distance measures to identify parameters that most significantly influence the simulated compartment size distribution. We have demonstrated an efficient method for the optimization of phantom parameters to achieve realistic distribution of adipose compartment size. The proposed methodology could be extended to other phantom parameters (e.g., ligaments and skin thicknesses), to further improve realism of the simulation and VCTs.

Joint Learning with Local and Global Consistency for Improved Medical Image Segmentation
From fully-supervised, single-task to scarcely-supervised, multi-task deep learning for medical image analysis
Author(s): Imran, Abdullah-Al-Zubaer | Advisor(s): Terzopoulos, Demetri | Abstract: Image analysis based on machine learning has gained prominence with the advent of deep learning, particularly in medical imaging. To be effective in addressing challenging image analysis tasks, however, conventional deep neural networks require large corpora of annotated training data, which are unfortunately scarce in the medical domain, thus often rendering fully-supervised learning strategies ineffective.This thesis devises for use in a variety of medical image analysis applications a series of novel deep learning methods, ranging from fully-supervised, single-task learning to scarcely-supervised, multi-task learning that makes efficient use of annotated training data. Specifically, its main contributions include (1) fully-supervised, single-task learning for the segmentation of pulmonary lobes from chest CT scans and the analysis of scoliosis from spine X-ray images; (2) supervised, single-task, domain-generalized pulmonary segmentation in chest X-ray images and retinal vasculature segmentation in fundoscopic images; (3) largely-unsupervised, multiple-task learning via deep generative modeling for the joint synthesis and classification of medical image data; and (4) partly-supervised, multiple-task learning for the combined segmentation and classification of chest and spine X-ray images.

Creation of Realistic Structured Backgrounds using Adipose Compartment Models in a Test Object for Breast Imaging Performance Analysis
DONE