Methods of combining multiple classifiers and their applications to handwriting recognition
Possible solutions to the problem of combining classifiers can be divided into three categories according to the levels of information available from the various classifiers. Four approaches based on different methodologies are proposed for solving this problem. One is suitable for combining individual classifiers such as Bayesian, k-nearest-neighbor, and various distance classifiers. The other three could be used for combining any kind of individual classifiers. On applying these methods to combine several classifiers for recognizing totally unconstrained handwritten numerals, the experimental results show that the performance of individual classifiers can be improved significantly. For example, on the US zipcode database, 98.9% recognition with 0.90% substitution and 0.2% rejection can be obtained, as well as high reliability with 95% recognition, 0% substitution, and 5% rejection. >

A distribution-free theory of nonparametric regression
Rival penalized competitive learning for clustering analysis, RBF net, and curve detection
It is shown that frequency sensitive competitive learning (FSCL), one version of the recently improved competitive learning (CL) algorithms, significantly deteriorates in performance when the number of units is inappropriately selected. An algorithm called rival penalized competitive learning (RPCL) is proposed. In this algorithm, not only is the winner unit modified to adapt to the input for each input, but its rival (the 2nd winner) is delearned by a smaller learning rate. RPCL can be regarded as an unsupervised extension of Kohonen's supervised LVQ2. RPCL has the ability to automatically allocate an appropriate number of units for an input data set. The experimental results show that RPCL outperforms FSCL when used for unsupervised classification, for training a radial basis function (RBF) network, and for curve detection in digital images.

Learning and design of principal curves
Principal curves have been defined as "self-consistent" smooth curves which pass through the "middle" of a d-dimensional probability distribution or data cloud. They give a summary of the data and also serve as an efficient feature extraction tool. We take a new approach by defining principal curves as continuous curves of a given length which minimize the expected squared distance between the curve and points of the space randomly chosen according to a given distribution. The new definition makes it possible to theoretically analyze principal curve learning from training data and it also leads to a new practical construction. Our theoretical learning scheme chooses a curve from a class of polygonal lines with k segments and with a given total length to minimize the average squared distance over n training points drawn independently. Convergence properties of this learning scheme are analyzed and a practical version of this theoretical algorithm is implemented. In each iteration of the algorithm, a new vertex is added to the polygonal line and the positions of the vertices are updated so that they minimize a penalized squared distance criterion. Simulation results demonstrate that the new algorithm compares favorably with previous methods, both in terms of performance and computational complexity, and is more robust to varying data models.

On the strong universal consistency of nearest neighbor regression function estimates
Two results are presented concerning the consistency of the k-nearest neighbor regression estimate. We show that all modes of convergence in L 1 (in probability, almost sure, complete) are equivalent if the regression variable is bounded. Under the additional condition k/log n → ∞ we also obtain the strong universal consistency of the estimate

Fast SVM training algorithm with decomposition on very large data sets
Training a support vector machine on a data set of huge size with thousands of classes is a challenging problem. This paper proposes an efficient algorithm to solve this problem. The key idea is to introduce a parallel optimization step to quickly remove most of the nonsupport vectors, where block diagonal matrices are used to approximate the original kernel matrix so that the original problem can be split into hundreds of subproblems which can be solved more efficiently. In addition, some effective strategies such as kernel caching and efficient computation of kernel matrix are integrated to speed up the training process. Our analysis of the proposed algorithm shows that its time complexity grows linearly with the number of classes and size of the data set. In the experiments, many appealing properties of the proposed algorithm have been investigated and the results show that the proposed algorithm has a much better scaling capability than Libsvm, SVM/sup light/, and SVMTorch. Moreover, the good generalization performances on several large databases have also been achieved.

Piecewise linear skeletonization using principal curves
Conventional image skeletonization techniques implicitly assume the pixel level connectivity. However, noise inside the object regions destroys the connectivity and exhibits sparseness in the image. We present a skeletonization algorithm designed for these kinds of sparse shapes. The skeletons are produced quickly by using three operations. First, initial skeleton nodes are selected by farthest point sampling with circles containing the maximum effective information. A skeleton graph of these nodes is imposed via inheriting the neighborhood of their associated pixels, followed by an edge collapse operation. Then a skeleton tting process based on feature-preserving Laplacian smoothing is applied. Finally, a re nement step is proposed to further improve the quality of the skeleton and deal with noise or different local shape scales. Numerous experiments demonstrate that our algorithm can effectively handle several disconnected shapes in an image simultaneously, and generate more faithful skeletons for shapes with intersections or different local scales than classic methods.

Image denoising using neighbouring wavelet coefficients
The denoising of a natural image corrupted by Gaussian noise is a classical problem in signal or image processing. Donoho and his coworkers at Stanford pioneered a wavelet denoising scheme by thresholding the wavelet coefficients arising from the standard discrete wavelet transform. This work has been widely used in science and engineering applications. However, this denoising scheme tends to kill too many wavelet coefficients that might contain useful image information. In this paper, we propose one wavelet image thresholding scheme by incorporating neighbouring coefficients, namely NeighShrink. This approach is valid because a large wavelet coefficient will probably have large wavelet coefficients as its neighbours. Experimental results show that NeighShrink is better than the Wiener filter and the conventional wavelet denoising approaches: VisuShrink and SUREShrink. We also investigate different neighbourhood sizes and find that a size of 3/spl times/3 is the best among all window sizes.

Computer-aided breast cancer diagnosis based on the analysis of cytological images of fine needle biopsies
The effectiveness of the treatment of breast cancer depends on its timely detection. An early step in the diagnosis is the cytological examination of breast material obtained directly from the tumor. This work reports on advances in computer-aided breast cancer diagnosis based on the analysis of cytological images of fine needle biopsies to characterize these biopsies as either benign or malignant. Instead of relying on the accurate segmentation of cell nuclei, the nuclei are estimated by circles using the circular Hough transform. The resulting circles are then filtered to keep only high-quality estimations for further analysis by a support vector machine which classifies detected circles as correct or incorrect on the basis of texture features and the percentage of nuclei pixels according to a nuclei mask obtained using Otsu's thresholding method. A set of 25 features of the nuclei is used in the classification of the biopsies by four different classifiers. The complete diagnostic procedure was tested on 737 microscopic images of fine needle biopsies obtained from patients and achieved 98.51% effectiveness. The results presented in this paper demonstrate that a computerized medical diagnosis system based on our method would be effective, providing valuable, accurate diagnostic information.

On radial basis function nets and kernel regression: Statistical consistency, convergence rates, and receptive field size
Robust estimation for range image segmentation and reconstruction
This correspondence presents a segmentation and fitting method using a new robust estimation technique. We present a robust estimation method with high breakdown point which can tolerate more than 80% of outliers. The method randomly samples appropriate range image points in the current processing region and solves equations determined by these points for parameters of selected primitive type. From K samples, we choose one set of sample points that determines a best-fit equation for the largest homogeneous surface patch in the region. This choice is made by measuring a residual consensus (RESC), using a compressed histogram method which is effective at various noise levels. After we get the best-fit surface parameters, the surface patch can be segmented from the region and the process is repeated until no pixel left. The method segments the range image into planar and quadratic surfaces. The RESC method is a substantial improvement over the least median squares method by using histogram approach to inferring residual consensus. A genetic algorithm is also incorporated to accelerate the random search. >

Distribution-free pointwise consistency of kernel regression estimate
Nonparametric estimation and classification using radial basis function nets and empirical risk minimization
Studies convergence properties of radial basis function (RBF) networks for a large class of basis functions, and reviews the methods and results related to this topic. The authors obtain the network parameters through empirical risk minimization. The authors show the optimal nets to be consistent in the problem of nonlinear function approximation and in nonparametric classification. For the classification problem the authors consider two approaches: the selection of the RBF classifier via nonlinear function estimation and the direct method of minimizing the empirical error probability. The tools used in the analysis include distribution-free nonasymptotic probability inequalities and covering numbers for classes of functions.

Classification of breast cancer malignancy using cytological images of fine needle aspiration biopsies
Classification of Breast Cancer Malignancy Using Cytological Images of Fine Needle Aspiration Biopsies According to the World Health Organization (WHO), breast cancer (BC) is one of the most deadly cancers diagnosed among middle-aged women. Precise diagnosis and prognosis are crucial to reduce the high death rate. In this paper we present a framework for automatic malignancy grading of fine needle aspiration biopsy tissue. The malignancy grade is one of the most important factors taken into consideration during the prediction of cancer behavior after the treatment. Our framework is based on a classification using Support Vector Machines (SVM). The SVMs presented here are able to assign a malignancy grade based on preextracted features with the accuracy up to 94.24%. We also show that SVMs performed best out of four tested classifiers.

Pricing of High‐Dimensional American Options by Neural Networks
Pricing of American options in discrete time is considered, where the option is allowed to be based on several underlyings. It is assumed that the price processes of the underlyings are given Markov processes. We use the Monte Carlo approach to generate artificial sample paths of these price processes, and then we use the least squares neural networks regression estimates to estimate from this data the so‐called continuation values, which are defined as mean values of the American options for given values of the underlyings at time t subject to the constraint that the options are not exercised at time t. Results concerning consistency and rate of convergence of the estimates are presented, and the pricing of American options is illustrated by simulated data.

Radial basis function networks and complexity regularization in function learning
In this paper we apply the method of complexity regularization to derive estimation bounds for nonlinear function estimation using a single hidden layer radial basis function network. Our approach differs from previous complexity regularization neural-network function learning schemes in that we operate with random covering numbers and l(1) metric entropy, making it possible to consider much broader families of activation functions, namely functions of bounded variation. Some constraints previously imposed on the network parameters are also eliminated this way. The network is trained by means of complexity regularization involving empirical risk minimization. Bounds on the expected risk in terms of the sample size are obtained for a large class of loss functions. Rates of convergence to the optimal loss are also derived.

Segmentation of handwritten digits using contour features
A new method of separating touching unconstrained handwritten digits is proposed. A binary image containing a string of touching digits is scanned to give contour chains. The chains are analyzed and subdivided into four kinds of regions: valleys, mountains, holes, and open regions. Individual points of interest in the outer contour are then identified, e.g., points of high curvature. The separating path is assumed to pass between some pair of these significant contour points (SCPs). Nine features of the SCPs are measured and are used to sort the list of all possible pairings of SCPs. Preliminary results show that the correct cut is sorted within the first three choices in 89% of tests.<<ETX>>

White blood cell differential counts using convolutional neural networks for low resolution images
Nonparametric regression based on hierarchical interaction models
In this paper, we introduce the so-called hierarchical interaction models, where we assume that the computation of the value of a function <inline-formula> <tex-math notation="LaTeX">$m: { \mathbb {R}^{d}}\rightarrow \mathbb {R}$ </tex-math></inline-formula> is done in several layers, where in each layer a function of at most <inline-formula> <tex-math notation="LaTeX">$d^{*}$ </tex-math></inline-formula> inputs computed by the previous layer is evaluated. We investigate two different regression estimates based on polynomial splines and on neural networks, and show that if the regression function satisfies a hierarchical interaction model and all occurring functions in the model are smooth, the rate of convergence of these estimates depends on <inline-formula> <tex-math notation="LaTeX">$d^{*}$ </tex-math></inline-formula> (and not on <inline-formula> <tex-math notation="LaTeX">$d$ </tex-math></inline-formula>). Hence, in this case, the estimates can achieve good rate of convergence even for large <inline-formula> <tex-math notation="LaTeX">$d$ </tex-math></inline-formula>, and are in this sense able to circumvent the so-called curse of dimensionality.

Contour-based handwritten numeral recognition using multiwavelets and neural networks
DONE