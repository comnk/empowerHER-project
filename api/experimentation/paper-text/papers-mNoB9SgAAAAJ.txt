Improved embeddings with easy positive triplet mining
Deep metric learning seeks to define an embedding where semantically similar images are embedded to nearby locations, and semantically dissimilar images are embedded to distant locations. Substantial work has focused on loss functions and strategies to learn these embeddings by pushing images from the same class as close together in the embedding space as possible. In this paper, we propose an alternative, loosened embedding strategy that requires the embedding function only map each training image to the most similar examples from the same class, an approach we call "Easy Positive" mining. We provide a collection of experiments and visualizations that highlight that this Easy Positive mining leads to embed-dings that are more flexible and generalize better to new unseen data. This simple mining strategy yields recall performance that exceeds state of the art approaches (including those with complicated loss functions and ensemble methods) on image retrieval datasets including CUB, Stanford Online Products, In-Shop Clothes and Hotels-50K. Code is available at: https://github.com/littleredxh/EasyPositiveHardNegative

Hard negative examples are hard, but useful
Visualizing deep similarity networks
For convolutional neural network models that optimize an image embedding, we propose a method to highlight the regions of images that contribute most to pairwise similarity. This work is a corollary to the visualization tools developed for classification networks, but applicable to the problem domains better suited to similarity learning. The visualization shows how similarity networks that are fine-tuned learn to focus on different features. We also generalize our approach to embedding networks that use different pooling strategies and provide a simple mechanism to support image similarity searches on objects or sub-regions in the query image.

Hotels-50k: A global hotel recognition dataset
Recognizing a hotel from an image of a hotel room is important for human trafficking investigations. Images directly link victims to places and can help verify where victims have been trafficked, and where their traffickers might move them or others in the future. Recognizing the hotel from images is challenging because of low image quality, uncommon camera perspectives, large occlusions (often the victim), and the similarity of objects (e.g., furniture, art, bedding) across different hotel rooms. To support efforts towards this hotel recognition task, we have curated a dataset of over 1 million annotated hotel room images from 50,000 hotels. These images include professionally captured photographs from travel websites and crowd-sourced images from a mobile application, which are more similar to the types of images analyzed in real-world investigations. We present a baseline approach based on a standard network architecture and a collection of data-augmentation approaches tuned to this problem domain.

Data-driven artificial intelligence for calibration of hyperspectral big data
Near-earth hyperspectral big data present both huge opportunities and challenges for spurring developments in agriculture and high-throughput plant phenotyping and breeding. In this article, we present data-driven approaches to address the calibration challenges for utilizing near-earth hyperspectral data for agriculture. A data-driven, fully automated calibration workflow that includes a suite of robust algorithms for radiometric calibration, bidirectional reflectance distribution function (BRDF) correction and reflectance normalization, soil and shadow masking, and image quality assessments was developed. An empirical method that utilizes predetermined models between camera photon counts (digital numbers) and downwelling irradiance measurements for each spectral band was established to perform radiometric calibration. A kernel-driven semiempirical BRDF correction method based on the Ross Thick-Li Sparse (RTLS) model was used to normalize the data for both changes in solar elevation and sensor view angle differences attributed to pixel location within the field of view. Following rigorous radiometric and BRDF corrections, novel rule-based methods were developed to conduct automatic soil removal; and a newly proposed approach was used for image quality assessment; additionally, shadow masking and plot-level feature extraction were carried out. Our results show that the automated calibration, processing, storage, and analysis pipeline developed in this work can effectively handle massive amounts of hyperspectral data and address the urgent challenges related to the production of sustainable bioenergy and food crops, targeting methods to accelerate plant breeding for improving yield and biomass traits.

Traffickcam: Crowdsourced and computer vision based approaches to fighting sex trafficking
According to a 2016 study by researchers at the University of New Hampshire, over sixty percent of child sex trafficking survivors were at one point advertised online [13]. These advertisements often include photos of the victim posed provocatively in a hotel room. It is imperative that law enforcement be able to quickly identify where these photos were taken to determine where a trafficker moves their victims. In previous work, we proposed a system to crowdsource the collection of hotel room photos that could be searched using different local feature and image descriptors. In this work, we present the fully realized crowd-sourcing platform, called TraffickCam, report on its usage by the public, and present a production system for fast national search by image, based on features extracted from a neural network trained explicitly for this purpose.

Terra-ref, an open reference data set from high resolution genomics, phenomics, and imaging sensors
The Transportation Energy Resources from Renewable Agriculture Phenotyping Reference Platform (TERRA-REF) provides a data and computation pipeline responsible for collecting, transferring, processing and distributing large volumes of crop sensing and genomic data from genetically informative germplasm sets. The primary source of these data is a field scanner system built over an experimental field at the University of Arizona Maricopa Agricultural Center. The scanner uses several different sensors to observe the field at a dense collection frequency with high resolution. These sensors include RGB stereo, thermal, pulse-amplitude modulated chlorophyll fluorescence, imaging spectrometer cameras, a 3D laser scanner, and environmental monitors. In addition, data from sensors mounted on tractors, UAVs, an indoor controlled-environment facility, and manually collected measurements are integrated into the pipeline. Up to two TB of data per day are collected and transferred to the National Center for Supercomputing Applications at the University of Illinois (NCSA) where they are processed. In this paper we describe the technical architecture for the TERRA-REF data and computing pipeline. This modular and scalable pipeline provides a suite of components to convert raw imagery to standard formats, geospatially subset data, and identify biophysical and physiological plant features related to crop productivity, resource use, and stress tolerance. Derived data products are uploaded to the Clowder content management system and the BETYdb traits and yields database for querying, supporting research at an experimental plot level. All software is open source2 under a BSD 3-clause or similar license and the data products are open access (currently for evaluation with a full release in fall 2019). In addition, we provide computing environments in which users can explore data and develop new tools. The goal of this system is to enable scientists to evaluate and use data, create new algorithms, and advance the science of digital agriculture and crop improvement.

Characterizing feature matching performance over long time periods
Many computer vision applications rely on matching features of a query image to reference data sets, but little work has explored how quickly data sets become out of date. In this paper we measure feature matching performance across 5 years of time-lapse data from 20 static cameras to empirically study how feature matching is affected by changing sunlight direction, seasons, weather, and the structural changes over time in outdoor settings. We identify several trends that may be relevant in real world applications: (1) features are much more likely to match within a few days of the reference data, (2) weather and sun-direction have a large effect on feature matching, and (3) there is a slow decay over time due to physical changes in a scene, but this decay is much smaller than effects of lighting direction and weather. These trends are consistent across standard choices for feature detection (DoG, MSER) and feature description (SIFT, SURF, and DAISY). Across all choices, analysis of the feature detection and matching pipeline highlights that performance decay is mostly due to failures in key point detection rather than feature description.

Learning from outdoor webcams: surveillance of physical activity across environments
Webcams, crowdsourcing, and enhanced crosswalks: developing a novel method to analyze active transportation
Introduction Active transportation opportunities and infrastructure are an important component of a community’s design, livability, and health. Features of the built environment influence active transportation, but objective study of the natural experiment effects of built environment improvements on active transportation is challenging. The purpose of this study was to develop and present a novel method of active transportation research using webcams and crowdsourcing, and to determine if crosswalk enhancement was associated with changes in active transportation rates, including across a variety of weather conditions. Methods The 20,529 publicly available webcam images from two street intersections in Washington, DC, USA were used to examine the impact of an improved crosswalk on active transportation. A crowdsource, Amazon Mechanical Turk, annotated image data. Temperature data were collected from the National Oceanic and Atmospheric Administration, and precipitation data were annotated from images by trained research assistants. Results Summary analyses demonstrated slight, bi-directional differences in the percent of images with pedestrians and bicyclists captured before and after the enhancement of the crosswalks. Chi-square analyses revealed these changes were not significant. In general, pedestrian presence increased in images captured during moderate temperatures compared to images captured during hot or cold temperatures. Chi-square analyses indicated the crosswalk improvement may have encouraged walking and biking in uncomfortable outdoor conditions (P < 0.5). Conclusion The methods employed provide an objective, cost-effective alternative to traditional means of examining the effects of built environment changes on active transportation. The use of webcams to collect active transportation data has applications for community policymakers, planners, and health professionals. Future research will work to validate this method in a variety of settings as well as across different built environment and community policy initiatives.

Multi-resolution outlier pooling for sorghum classification
Automated high throughput plant phenotyping involves leveraging sensors, such as RGB, thermal and hyperspectral cameras (among others), to make large scale and rapid measurements of the physical properties of plants for the purpose of better understanding the difference between crops and facilitating rapid plant breeding programs. One of the most basic phenotyping tasks is to determine the cultivar, or species, in a particular sensor product. This simple phenotype can be used to detect errors in planting and to learn the most differentiating features between cultivars. It is also a challenging visual recognition task, as a large number of highly related crops are grown simultaneously, leading to a classification problem with low inter-class variance. In this paper, we introduce the Sorghum-100 dataset, a large dataset of RGB imagery of sorghum captured by a state-of-the-art gantry system, a multi-resolution network architecture that learns both global and fine-grained features on the crops, and a new global pooling strategy called Dynamic Outlier Pooling which outperforms standard global pooling strategies on this task.

Indexing Open Imagery to Create Tools to Fight Sex Trafficking
Images are important to fighting sex trafficking because they are: (a) used to advertise for sex services, (b) shared among criminal networks, and (c) connect a person in an image to the place where the image was taken. This work explores the ability to link images to indoor places in order to support the investigation and prosecution of criminal activity. We propose a framework which includes a database of open-source information available on the Internet, a crowd-sourcing approach to gathering additional images, and two baseline matching approaches. We concentrate on spatio-temporal indexing of hotel rooms, and to date have an index of more than 1.5 million geo-coded images. Our smart-phone app collects contextual information and metadata along-side images. On a test that included a database of 1800 images from 200 different hotels in St. Loouis, the correct hotel that matched a query images was found in the top 10 responses two-thirds of the time. We conclude with an analysis ocitehe successes and limitations of our data set, our matching process, and suggestions for future research.

Comparing deep learning approaches for understanding genotype× phenotype interactions in biomass sorghum
We explore the use of deep convolutional neural networks (CNNs) trained on overhead imagery of biomass sorghum to ascertain the relationship between single nucleotide polymorphisms (SNPs), or groups of related SNPs, and the phenotypes they control. We consider both CNNs trained explicitly on the classification task of predicting whether an image shows a plant with a reference or alternate version of various SNPs as well as CNNs trained to create data-driven features based on learning features so that images from the same plot are more similar than images from different plots, and then using the features this network learns for genetic marker classification. We characterize how efficient both approaches are at predicting the presence or absence of a genetic markers, and visualize what parts of the images are most important for those predictions. We find that the data-driven approaches give somewhat higher prediction performance, but have visualizations that are harder to interpret; and we give suggestions of potential future machine learning research and discuss the possibilities of using this approach to uncover unknown genotype × phenotype relationships.

Collaborative rephotography
Rephotography is the process of capturing the same scene at a different time, in order to capture changes. Previous work at SIGGRAPH [BAE2010] demonstrated the ability for smart-phone apps to guide a user to the correct viewpoint, here we promote the use of such tools distributed widely over space and time, by enabling collaborative projects that allow multiple users to re-photograph multiple sites over time. These sites may be architectural, social, urban scenes or ecological. Current projects utilizing our mobile tools range from nation-scale rephotography of scenic overlooks, to monitoring of urban street trees in NYC by local conservancy group volunteers. Rephotography directly connects pictures at one time to pictures at another time. It also connects a photographer at one time to a photographer at another time, by providing a mechanism to collaboratively record the story of how our world changes.

Collaborative imaging of urban forest dynamics: augmenting re-photography to visualize changes over time
The ecological sciences face the challenge of making measurements to detect subtle changes sometimes over large areas across varied temporal scales. The challenge is thus to measure patterns of slow, subtle change occurring along multiple spatial and temporal scales, and then to visualize those changes in a way that makes important variations visceral to the observer. Imaging plays an important role in ecological measurement but existing techniques often rely on approaches that are limited with respect to their spatial resolution, view angle, and/or temporal resolution. Furthermore, integrating imaging acquired through different modalities is often difficult, if not impossible. This research envisions a community-based and participatory approach based around augmented rephotography of ecosystems. We show a case study for the purpose of monitoring the urban tree canopy. The goal is to explore, for a set of urban locations, the integration of ground level rephotography with available LiDAR data, and to create a dynamic view of the urban forest, and its changes across various spatial and temporal scales. This case study gives the opportunity to explore various augments to improve the ground level image capture process, protocols to support 3D inference from the contributed photography, and both in-situ and web based visualizations of the temporal change over time.

Democratizing the visualization of 500 million webcam images
Five years ago we reported at AIPR on a nascent project to archive images from every webcam in the world and to develop algorithms to geo-locate, calibrate, and annotate this data. This archive of many outdoor scenes (AMOS) has now grown to include 28000 live outdoor cameras and over 630 million images. This is actively being used in projects ranging from large scale environmental monitoring to characterizing how built environment changes (such as adding bike lanes in DC) affects physical activity patterns over time. But the biggest value in a very long term, widely distributed image dataset is the rich set of before data that can be analyzed to evaluate changes from unexpected or sudden events. To facilitate the analysis of these natural experiments, we build and share a collection of web-tools that support large scale, data driven exploration. In this work we discuss and motivate a visualization tool that uses PCA to find the subspace that characterizes the variations in this scene, This anomaly detection captures both imaging failures such as lens flare and also unusual situations such as street fairs, and we give initial algorithm to clusters anomalies so that they can be quickly evaluated for whether they are of interest.

Visualizing paired image similarity in transformer networks
Transformer architectures have shown promise for a wide range of computer vision tasks, including image embedding. As was the case with convolutional neural networks and other models, explainability of the predictions is a key concern, but visualization approaches tend to be architecture-specific. In this paper, we introduce a new method for producing interpretable visualizations that, given a pair of images encoded with a Transformer, show which regions contributed to their similarity. Additionally, for the task of image retrieval, we compare the performance of Transformer and ResNet models of similar capacity and show that while they have similar performance in aggregate, the retrieved results and the visual explanations for those results are quite different. Code is available at https://github.com/vidarlab/xformer-paired-viz.

Cameras and crowds in transportation tracking
Active transportation is an important contributor to physical activity. Understanding active transportation trends and transportation mode share is important to public health research and city planners. Objective measurement of active transportation can be costly and time-consuming, and existing camera-based algorithms, while developing, are functionally limited to specific settings and distances. In this study, 28,992 publicly available webcam images from two intersections in Washington, D.C., were used to establish trends in active transportation. Amazon Mechanical Turk workers were found to be reliable identifiers of pedestrian and vehicular activity, data validated against trained research assistant image annotation. Webcam and crowdsource annotation provides a cost-effective alternative to traditional objective measures of active transportation and mode share through the use of publicly available wireless webcams. Additional research is needed to expand the utility and external validity of publicly available imaged-based active transportation methodology and image annotation.

Sparklegeometry: glitter imaging for 3D point tracking
We consider a geometric inference problem for an imaging system consisting of a camera that views the world through a planar, rectangular sheet of glitter. We describe a procedure to calibrate this imaging geometry as a generalized camera which characterizes the subset of the light field viewed through each piece of glitter. We propose an easy to construct physical prototype and characterize its performance for estimating the 3D position of a moving point light source just by viewing the changing sparkle patterns visible on the glitter sheet.

What Does TERRA-REF's High Resolution, Multi Sensor Plant Sensing Public Domain Data Offer the Computer Vision Community?
A core objective of the TERRA-REF project was to generate an open-access reference dataset for the evaluation of sensing technologies to study plants under field conditions. The TERRA-REF program deployed a suite of high-resolution, cutting edge technology sensors on a gantry system with the aim of scanning 1 hectare (104m) at around 1 mm2 spatial resolution multiple times per week. The system contains co-located sensors including a stereo-pair RGB camera, a thermal imager, a laser scanner to capture 3D structure, and two hyperspectral cameras covering wavelengths of 300-2500nm. This sensor data is provided alongside over sixty types of traditional plant phenotype measurements that can be used to train new machine learning models. Associated weather and environmental measurements, information about agronomic management and experimental design, and the genomic sequences of hundreds of plant varieties have been collected and are available alongside the sensor and plant phenotype data.Over the course of four years and ten growing seasons, the TERRA-REF system generated over 1 PB of sensor data and almost 45 million files. The subset that has been released to the public domain accounts for two seasons and about half of the total data volume. This provides an unprecedented opportunity for investigations far beyond the core biological scope of the project.The focus of this paper is to provide the Computer Vision and Machine Learning communities an overview of the available data and some potential applications of this one of a kind data.

DONE