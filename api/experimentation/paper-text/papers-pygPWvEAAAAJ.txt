Modeling and reasoning with Bayesian networks
A knowledge compilation map
We propose a perspective on knowledge compilation which calls for analyzing different compilation approaches according to two key dimensions: the succinctness of the target compilation language, and the class of queries and transformations that the language supports in polytime. We then provide a knowledge compilation map, which analyzes a large number of existing target compilation languages according to their succinctness and their polytime transformations and queries. We argue that such analysis is necessary for placing new compilation approaches within the context of existing ones. We also go beyond classical, flat target compilation languages based on CNF and DNF, and consider a richer, nested class based on directed acyclic graphs (such as OBDDs), which we show to include a relatively large number of target compilation languages.

On the logic of iterated belief revision
Inference in belief networks: A procedural guide
A differential approach to inference in Bayesian networks
We present a new approach to inference in Bayesian networks, which is based on representing the network using a polynomial and then retrieving answers to probabilistic queries by evaluating and differentiating the polynomial. The network polynomial itself is exponential in size, but we show how it can be computed efficiently using an arithmetic circuit that can be evaluated and differentiated in time and space linear in the circuit size. The proposed framework for inference subsumes one of the most influential methods for inference in Bayesian networks, known as the tree-clustering or jointree method, which provides a deeper understanding of this classical method and lifts its desirable characteristics to a much more general setting. We discuss some theoretical and practical implications of this subsumption.

Decomposable negation normal form
This paper integrates weak decomposable negation normal form (wDNNF) circuits, introduced by Akshay et al. in 2018, into the knowledge compilation map. This circuit type generalises decomposable negation normal form (DNNF) circuits in such a way that they allow a restricted form of sharing variables among the inputs of a conjunction node. We show that wDNNF circuits have the same properties as DNNF circuits regarding the queries and transformations presented in the knowledge compilation map, whilst being strictly more succinct than DNNF circuits (that is, they can represent Boolean functions compactly). We also present and evaluate a knowledge compiler, called Bella, for converting CNF formulae into wDNNF circuits. Our experiments demonstrate that wDNNF circuits are suitable for configuration instances.

On probabilistic inference by weighted model counting
SDD: A new canonical representation of propositional knowledge bases
A lightweight component caching scheme for satisfiability solvers
Recursive conditioning
This paper presents a novel smart-grid power-conditioning system with intelligent control for solving a critical issue affecting the performance and life of three-phase asynchronous machines that are highly sensitive to voltage and phase drifts power disturbances of the electric grid. The present work of controlling the power supply of an electric-grid is based on an original power conditioning control algorithm; named recursive stochastic optimization (RSO) algorithm. An asynchronous machine steady-state model is derived based on symmetrical components analysis, permitting the evaluation of the effect of both voltage and phase unbalance using voltage and current unbalance factors. The three-phase power supply is conveyed through a power conditioning and intelligent control system, driven by a digital signal processor (DSP) controller, which measures the input currents, estimates the unbalance factors, and searches for voltage levels that minimize the electric-grid unbalance using the RSO, in order to act on a feedback power-electronics stage. The RSO algorithm shows excellent results along with very fast response time of the order of 20 ms allowing real-time control. Results obtained are validated with a Simulink dynamic model. The intelligent control system architecture based on TMS320C6748 DSP board with signal acquisition and control microcontroller is proposed.

New advances in compiling CNF to decomposable negation normal form
We describe a new algorithm for compiling conjunctive normal form (CNF) into Deterministic Decomposable Negation Normal (d-DNNF), which is a tractable logical form that permits model counting in polynomial time. The new implementation is based on latest techniques from both the SAT and OBDD literatures, and appears to be orders of magnitude more efficient than previous algorithms for this purpose. We compare our compiler experimentally to state of the art model counters, OBDD compilers, and previous CNF2dDNNF compilers.

On the tractable counting of theory models and its application to truth maintenance and belief revision
We address in this paper the problem of counting the models of a propositional theory under incremental changes to its literals. Specifcally, we show that if a propositional theory Δ is in a special form that we call smooth, deterministic, decomposable negation normal form (sd-DNNF), then for any consistent set of literals S, we can simultaneously count (in time linear in the size of Δ) the models of Δ ∪ S and the models of every theory Δ ∪ T where T results from adding, removing or flipping a literal in S. We present two results relating to the time and space complexity of compiling propositional theories into sd-DNNF. First, we show that if a conjunctive normal form (CNF) has a bounded treewidth, then it can be compiled into an sd-DNNF in time and space which are linear in its size. Second, we show that sd-DNNF is a strictly more space efficient representation than Free Binary Decision Diagrams (FBDDs). Finally, we discuss some applications of the counting results to truth maintenance systems, belief revision, and model-based diagnosis.

A symbolic approach to explaining bayesian network classifiers
We propose an approach for explaining Bayesian network classifiers, which is based on compiling such classifiers into decision functions that have a tractable and symbolic form. We introduce two types of explanations for why a classifier may have classified an instance positively or negatively and suggest algorithms for computing these explanations. The first type of explanation identifies a minimal set of the currently active features that is responsible for the current classification, while the second type of explanation identifies a minimal set of features whose current state (active or not) is sufficient for the classification. We consider in particular the compilation of Naive and Latent-Tree Bayesian network classifiers into Ordered Decision Diagrams (ODDs), providing a context for evaluating our proposal using case studies and experiments based on classifiers from the literature.

Complexity results and approximation strategies for MAP explanations
MAP is the problem of finding a most probable instantiation of a set of variables given evidence. MAP has always been perceived to be significantly harder than the related problems of computing the probability of a variable instantiation (Pr), or the problem of computing the most probable explanation (MPE). This paper investigates the complexity of MAP in Bayesian networks. Specifically, we show that MAP is complete for NPPP and provide further negative complexity results for algorithms based on variable elimination. We also show that MAP remains hard even when MPE and Pr become easy. For example, we show that MAP is NP-complete when the networks are restricted to polytrees, and even then can not be effectively approximated. 
 
Given the difficulty of computing MAP exactly, and the difficulty of approximating MAP while providing useful guarantees on the resulting approximation, we investigate best effort approximations. We introduce a generic MAP approximation framework. We provide two instantiations of the framework; one for networks which are amenable to exact inference (Pr), and one for networks for which even exact inference is too hard. This allows MAP approximation on networks that are too complex to even exactly solve the easier problems, Pr and MPE. Experimental results indicate that using these approximation algorithms provides much better solutions than standard techniques, and provide accurate MAP estimates in many cases.

OARPLAN: Generating project plans by reasoning about objects, actions and resources
This paper describes OARPLAN, a prototype planning system that generates construction project plans from a description of the objects that comprise the completed facility. OARPLAN is based upon the notion that activities in a project plan can be viewed as intersections of their constituents: objects, actions and resources. Planning knowledge in OARPLAN is represented as constraints based on activity constituents and their interrelationships; the planner functions as a constraint satisfaction engine that attempts to satisfy these constraints. The goal of the OARPLAN project is to develop a planning shell for construction projects that (i) provides a natural and powerful constraint language for expressing knowledge about construction planning, and (ii) generates a facility construction plan by satisfying constraints expressed in this language. To generate its construction plan, OARPLAN must be supplied with extensive knowledge about construction objects, actions and resources, and about spatial, topological, temporal and other relations that may exist between them. We suggest that much of the knowledge required to plan the construction of a given facility can be drawn directly from a three-dimensional CAD model of the facility, and from a variety of databases currently used in design and project management software. In the prototype OARPLAN system, facility data must be input directly as frames. However, we are collaborating with database researchers to develop intelligent interfaces to such sources of planning data, so that OARPLAN will eventually be able to send high level queries to an intelligent database access system without regard for the particular CAD system in which the project was designed. We begin by explaining why classical AI planners and domain specific expert system approaches are both inadequate for the task of generating construction project plans. We describe the activity representation developed in OARPLAN and demonstrate its use in producing a plan of about 50 activities for a steel-frame building, based on spatial and topological constraints that express structural support, weather protection and safety concerns in construction planning. We conclude with a discussion of the research issues raised by our experiments with OARPLAN to date.

Compiling Bayesian networks with local structure
Recent work on compiling Bayesian networks has reduced the problem to that of factoring CNF encodings of these networks, providing an expressive framework for exploiting local structure. For networks that have local structure, large CPTs, yet no excessive determinism, the quality of the CNF encodings and the amount of local structure they capture can have a significant effect on both the offline compile time and online inference time. We examine the encoding of such Bayesian networks in this paper and report on new findings that allow us to significantly scale this compilation approach. In particular, we obtain order-of-magnitude improvements in compile time, compile some networks successfully for the first time, and obtain ordersof-magnitude improvements in online inference for some networks with local structure, as compared to baseline jointree inference, which does not exploit local structure.

Bayesian networks
Model-based diagnosis using structured system descriptions
This paper presents a comprehensive approach for model-based diagnosis which includes proposals for characterizing and computing preferred diagnoses, assuming that the system description is augmented with a system structure (a directed graph explicating the interconnections between system components). Specifically, we first introduce the notion of a consequence, which is a syntactically unconstrained propositional sentence that characterizes all consistency-based diagnoses and show that standard characterizations of diagnoses, such as minimal conflicts, correspond to syntactic variations on a consequence. Second, we propose a new syntactic variation on the consequence known as negation normal form (NNF) and discuss its merits compared to standard variations. Third, we introduce a basic algorithm for computing consequences in NNF given a structured system description. We show that if the system structure does not contain cycles, then there is always a linear-size consequence in NNF which can be computed in linear time. For arbitrary system structures, we show a precise connection between the complexity of computing consequences and the topology of the underlying system structure. Finally, we present an algorithm that enumerates the preferred diagnoses characterized by a consequence. The algorithm is shown to take linear time in the size of the consequence if the preference criterion satisfies some general conditions.

Probabilistic sentential decision diagrams
We propose the Probabilistic Sentential Decision Diagram (PSDD): A complete and canonical representation of probability distributions defined over the models of a given propositional theory. Each parameter of a PSDD can be viewed as the (conditional) probability of making a decision in a corresponding Sentential Decision Diagram (SDD). The SDD itself is a recently proposed complete and canonical representation of propositional theories. We explore a number of interesting properties of PSDDs, including the independencies that underlie them. We show that the PSDD is a tractable representation. We further show how the parameters of a PSDD can be efficiently estimated, in closed form, from complete data. We empirically evaluate the quality of PS-DDs learned from data, when we have knowledge, a priori, of the domain logical constraints.

On the power of clause-learning SAT solvers as resolution engines
DONE