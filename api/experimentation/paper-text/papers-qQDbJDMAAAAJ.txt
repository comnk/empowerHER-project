Boxcars: 3d boxes as cnn input for improved fine-grained vehicle recognition
We are dealing with the problem of fine-grained vehicle make&model recognition and verification. Our contribution is showing that extracting additional data from the video stream - besides the vehicle image itself - and feeding it into the deep convolutional neural network boosts the recognition performance considerably. This additional information includes: 3D vehicle bounding box used for "unpacking" the vehicle image, its rasterized low-resolution shape, and information about the 3D vehicle orientation. Experiments show that adding such information decreases classification error by 26% (the accuracy is improved from 0.772 to 0.832) and boosts verification average precision by 208% (0.378 to 0.785) compared to baseline pure CNN without any input modifications. Also, the pure baseline CNN outperforms the recent state of the art solution by 0.081. We provide an annotated set "BoxCars" of surveillance vehicle images augmented by various automatically extracted auxiliary information. Our approach and the dataset can considerably improve the performance of traffic surveillance systems.

Vehicle re-identification for automatic video traffic surveillance
This paper proposes an approach to the vehicle reidentification problem in a multiple camera system. We focused on the re-identification itself assuming that the vehicle detection problem is already solved including extraction of a full-fledged 3D bounding box. The re-identification problem is solved by using color histograms and histograms of oriented gradients by a linear regressor. The features are used in separate models in order to get the best results in the shortest CPU computation time. The proposed method works with a high accuracy (60% true positives retrieved with 10% false positive rate on a challenging subset of the test data) in 85 milliseconds of the CPU (Core i7) computation time per one vehicle re-identification assuming the fullHD resolution video input. The applications of this work include finding important parameters such as travel time, traffic flow, or traffic information in a distributed traffic surveillance and monitoring system.

Boxcars: Improving fine-grained recognition of vehicles using 3-d bounding boxes in traffic surveillance
In this paper, we focus on fine-grained recognition of vehicles mainly in traffic surveillance applications. We propose an approach that is orthogonal to recent advancements in fine-grained recognition (automatic part discovery and bilinear pooling). In addition, in contrast to other methods focused on fine-grained recognition of vehicles, we do not limit ourselves to a frontal/rear viewpoint, but allow the vehicles to be seen from any viewpoint. Our approach is based on 3-D bounding boxes built around the vehicles. The bounding box can be automatically constructed from traffic surveillance data. For scenarios where it is not possible to use precise construction, we propose a method for an estimation of the 3-D bounding box. The 3-D bounding box is used to normalize the image viewpoint by “unpacking” the image into a plane. We also propose to randomly alter the color of the image and add a rectangle with random noise to a random position in the image during the training of convolutional neural networks (CNNs). We have collected a large fine-grained vehicle data set BoxCars116k, with 116k images of vehicles from various viewpoints taken by numerous surveillance cameras. We performed a number of experiments, which show that our proposed method significantly improves CNN classification accuracy (the accuracy is increased by up to 12% points and the error is reduced by up to 50% compared with CNNs without the proposed modifications). We also show that our method outperforms the state-of-the-art methods for fine-grained recognition.

Calibration of rgb camera with velodyne lidar
Calibration of the LiDAR sensor with RGB camera finds its usage in many application fields from enhancing 
image classification to the environment perception and mapping. This paper presents a pipeline for mutual pose 
and orientation estimation of the mentioned sensors using a coarse to fine approach. Previously published methods 
use multiple views of a known chessboard marker for computing the calibration parameters, or they are limited to 
the calibration of the sensors with a small mutual displacement only. Our approach presents a novel 3D marker for 
coarse calibration which can be robustly detected in both the camera image and the LiDAR scan. It also requires 
only a single pair of camera-LiDAR frames for estimating large sensors displacement. Consequent refinement step 
searches for more accurate calibration in small subspace of calibration parameters. The paper also presents a novel 
way for evaluation of the calibration precision using projection error.

Automatic camera calibration for traffic understanding.
We propose a method for fully automatic calibration of traffic surveillance cameras. This method allows for calibration of the camera – including scale – without any user input, only from several minutes of input surveillance video. The targeted applications include speed measurement, measurement of vehicle dimensions, vehicle classification, etc. The first step of our approach is camera calibration by determining three vanishing points defining the stream of vehicles. The second step is construction of 3D bounding boxes of individual vehicles and their measurement up to scale. We propose to first construct the projection of the bounding boxes and then, by using the camera calibration obtained earlier, create their 3D representation. In the third step, we use the dimensions of the 3D bounding boxes for calibration of the scene scale. We collected a dataset with ground truth speed and distance measurements and evaluate our approach on it. The achieved mean accuracy of speed and distance measurement is below 2%. Our efficient C++ implementation runs in real time on a low-end processor (Core i3) with a safe margin even for full-HD videos.

Fully automatic roadside camera calibration for traffic surveillance
This paper deals with automatic calibration of roadside surveillance cameras. We focus on parameters necessary for measurements in traffic-surveillance applications. Contrary to the existing solutions, our approach requires no a priori knowledge, and it works with a very wide variety of road settings (number of lanes, occlusion, quality of ground marking), as well as with practically unlimited viewing angles. The main contribution is that our solution works fully automatically-without any percamera or per-video manual settings or input whatsoever-and it is computationally inexpensive. Our approach uses tracking of local feature points and analyzes the trajectories in a manner based on cascaded Hough transform and parallel coordinates. An important assumption for the vehicle movement is that at least a part of the vehicle motion is approximately straight-we discuss the impact of this assumption on the applicability of our approach and show experimentally that this assumption does not limit the usability of our approach severely. We efficiently and robustly detect vanishing points, which define the ground plane and vehicle movement, except for the scene scale. Our algorithm also computes parameters for radial distortion compensation. Experiments show that the obtained camera parameters allow for measurements of relative lengths (and potentially speed) with ~2% mean accuracy. The processing is performed easily in real time, and typically, a 2-min-long video is sufficient for stable calibration.

Holistic recognition of low quality license plates by CNN using track annotated data
This work is focused on recognition of license plates in low resolution and low quality images. We present a methodology for collection of real world (non-synthetic) dataset of low quality license plate images with ground truth transcriptions. Our approach to the license plate recognition is based on a Convolutional Neural Network which holistically processes the whole image, avoiding segmentation of the license plate characters. Evaluation results on multiple datasets show that our method significantly outperforms other free and commercial solutions to license plate recognition on the low quality data. To enable further research of low quality license plate recognition, we make the datasets publicly available.

Traffic surveillance camera calibration by 3d model bounding box alignment for accurate vehicle speed measurement
Fast detection and recognition of QR codes in high-resolution images
This paper deals with detection and recognition of matrix codes, such as the QR codes, in high-resolution images of real-world scenes. The goal is to provide a detector capable of operation in real time even on high-resolution images (several megapixels). We present an efficient algorithm for detection of possible occurrences of the codes. This algorithm is characterized by a very low false negative rate and a reasonable false alarm rate. The results of our algorithm are to be followed by an accurate detection/recognition algorithm. We propose to use a recent matrix code detection and recognition algorithm based on Hough transform, because it can reuse some information computed by our new pre-detection algorithm and thus a further reduce of computational demands can be achieved. Since there are no publicly available annotated datasets for evaluation of this kind of algorithm, we collected a number of images and annotated them; these images will be made publicly available to allow for a proper comparison. Our algorithm was evaluated on this dataset and the results are reported in the paper.

CNN for IMU assisted odometry estimation using velodyne LiDAR
We introduce a novel method for odometry estimation using convolutional neural networks from 3D LiDAR scans. The original sparse data are encoded into 2D matrices for the training of proposed networks and for the prediction. Our networks show significantly better precision in the estimation of translational motion parameters comparing with state of the art method LOAM, while achieving real-time performance. Together with IMU support, high quality odometry estimation and LiDAR data registration is realized. Moreover, we propose alternative CNNs trained for the prediction of rotational motion parameters while achieving results also comparable with state of the art. The proposed method can replace wheel encoders in odometry estimation or supplement missing GPS data, when the GNSS signal absents (e.g. during the indoor mapping). Our solution brings real-time performance and precision which are useful to provide online preview of the mapping results and verification of the map completeness in real time.

Collar line segments for fast odometry estimation from velodyne point clouds
We present a novel way of odometry estimation from Velodyne LiDAR point cloud scans. The aim of our work is to overcome the most painful issues of Velodyne data - the sparsity and the quantity of data points - in an efficient way, enabling more precise registration. Alignment of the point clouds which yields the final odometry is based on random sampling of the clouds using Collar Line Segments (CLS). The closest line segment pairs are identified in two sets of line segments obtained from two consequent Velodyne scans. From each pair of correspondences, a transformation aligning the matched line segments into a 3D plane is estimated. By this, significant planes (ground, walls, ...) are preserved among aligned point clouds. Evaluation using the KITTI dataset shows that our method outperforms publicly available and commonly used state-of-the-art method GICP for point cloud registration in both accuracy and speed, especially in cases where the scene lacks significant landmarks or in typical urban elements. For such environments, the registration error of our method is reduced by 75% compared to the original GICP error.

Review of Hough transform for line detection
Cnn for very fast ground segmentation in velodyne lidar data
This paper presents a novel method for ground segmentation in Velodyne point clouds. We propose an encoding of sparse 3D data from the Velodyne sensor suitable for training a convolutional neural network (CNN). This general purpose approach is used for segmentation of the sparse point cloud into ground and non-ground points. The LiDAR data are represented as a multi-channel 2D signal where the horizontal axis corresponds to the rotation angle and the vertical axis represents channels — laser beams. Multiple topologies of relatively shallow CNNs (i.e. 3–5 convolutional layers) are trained and evaluated, using a manually annotated dataset we prepared. The results show significant improvement of performance over the state-of-the-art method by Zhang et al. in terms of speed and also minor improvements in terms of accuracy.

Comprehensive data set for automatic single camera visual speed measurement
In this paper, we focus on traffic camera calibration and a visual speed measurement from a single monocular camera, which is an important task of visual traffic surveillance. Existing methods addressing this problem are difficult to compare due to a lack of a common data set with reliable ground truth. Therefore, it is not clear how the methods compare in various aspects and what factors are affecting their performance. We captured a new data set of 18 full-HD videos, each around 1 hr long, captured at six different locations. Vehicles in the videos (20 865 instances in total) are annotated with the precise speed measurements from optical gates using LiDAR and verified with several reference GPS tracks. We made the data set available for download and it contains the videos and metadata (calibration, lengths of features in image, annotations, and so on) for future comparison and evaluation. Camera calibration is the most crucial part of the speed measurement; therefore, we provide a brief overview of the methods and analyze a recently published method for fully automatic camera calibration and vehicle speed measurement and report the results on this data set in detail.

Yet faster ray-triangle intersection (using SSE4)
Ray-triangle intersection is an important algorithm, not only in the field of realistic rendering (based on ray tracing) but also in physics simulation, collision detection, modeling, etc. Obviously, the speed of this well-defined algorithm's implementations is important because calls to such a routine are numerous in rendering and simulation applications. Contemporary fast intersection algorithms, which use SIMD instructions, focus on the intersection of ray packets against triangles. For intersection between single rays and triangles, operations such as horizontal addition or dot product are required. The SSE4 instruction set adds the dot product instruction which can be used for this purpose. This paper presents a new modification of the fast ray-triangle intersection algorithms commonly used, which-when implemented on SSE4-outperforms the current state-of-the-art algorithms. It also allows both a single ray and ray packet intersection calculation with the same precomputed data. The speed gain measurements are described and discussed in the paper.

PClines—Line detection using parallel coordinates
Detection of lines in raster images is often performed using Hough transform. This paper presents a new parameterization of lines and a modification of the Hough transform–PClines. PClines are based on parallel coordinates, a coordinate system used mostly or solely for high-dimensional data visualization. The PClines algorithm is described in the paper; its accuracy is evaluated numerically and compared to the commonly used line detectors based on the Hough transform. The results show that PClines outperform the existing approaches in terms of accuracy. Besides, PClines are computationally extremely efficient, require no floating-point operations, and can be easily accelerated by different hardware architectures.

Real-time object detection on CUDA
Large-scale physics-based terrain editing using adaptive tiles on the GPU
Physics-based approaches could simplify terrain modeling by increasing its realism. However, most simulations provide only a low level of user control because they fail on large-scale phenomena or focus only on the modeling of limited effects. A new physics-based system for digital terrain editing is suitable for digital-content authors such as game designers, artists, and 3D modelers. It doesn't assume in-depth knowledge about physics-based simulations. Users can load large terrains from external sources, generate them procedurally, or create them manually, and they can edit them at interactive frame rates on a GPU. To allow large-scale editing, the system divides terrain into tiles of different resolutions according to the terrain's complexity, and it stores each tile as a mip-map texture. In addition, the physics-based simulation uses different levels of detail, depending on the terrain-change dynamics. Compared to nonadaptive computation, this approach can achieve 50 percent speedup and use 25 percent less memory. The Web extra is a video that shows how physics-based approaches to modeling can process terrain sizes larger than what was previously possible.

Real Projective Plane Mapping for Detection of Orthogonal Vanishing Points.
This paper deals with the detection of orthogonal vanishing points. The aim is to efficiently cope with the clutter edges in real-life images and to determine the camera orientation in the Manhattan world reliably. We are using a modified scheme of the Cascaded Hough Transform where only one Hough space is accumulated – the space of the vanishing points. The parameterization of the vanishing points – the “diamond space” – is based on the PClines line parameterization and it is defined as a mapping of the whole real projective plane to a finite space. Our algorithm for detection of vanishing points operates directly on edgelets detected by an edge detector, skipping the common step of grouping edges into straight lines or line segments. This decreases the number of configuration parameters and reduces the complexity of the algorithm. Evaluated on the York Urban DB, our algorithm yields 98.04 % success rate at 10◦ angular error tolerance, which outperforms comparable existing solutions. Our parameterization of vanishing points is in all aspects linear; it involves no goniometric or other non-linear operations and thus it is suitable for implementation in embedded chips and circuitry. The iterative search scheme allows for finding orthogonal triplets of vanishing points with high accuracy and low computational costs. At the same time, our approach can be used without the orthogonality constraint.

Real-time precise detection of regular grids and matrix codes
DONE