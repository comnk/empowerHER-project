On clusterings: Good, bad and spectral
We propose a new measure for assessing the quality of a clustering. A simple heuristic is shown to give worst-case guarantees under the new measure. Then we present two results regarding the quality of the clustering found by a popular spectral algorithm. One proffers worst case guarantees whilst the other shows that if there exists a "good" clustering then the spectral algorithm will find one close to it.

Finding odd cycle transversals
Nash equilibria in competitive societies, with applications to facility location, traffic routing and auctions
We consider the following class of problems. The value of an outcome to a society is measured via a submodular utility function (submodularity has a natural economic interpretation: decreasing marginal utility). Decisions, however, are controlled by non-cooperative agents who seek to maximise their own private utility. We present, under basic assumptions, guarantees on the social performance of Nash equilibria. For submodular utility functions, any Nash equilibrium gives an expected social utility within a factor 2 of optimal, subject to a function-dependent additive term. For non-decreasing, submodular utility functions, any Nash equilibrium gives an expected social utility within a factor 1+/spl delta/ of optimal, where 0/spl les//spl delta//spl les/1 is a number based upon discrete curvature of the function. A condition under which all sets of social and private utility functions induce pure strategy Nash equilibria is presented. The case in which agents themselves make use of approximation algorithms in decision making is discussed and performance guarantees given. Finally we present specific problems that fall into our framework. These include competitive versions of the facility location problem and k-median problem, a maximisation version of the traffic routing problem studied by Roughgarden and Tardos (2000), and multiple-item auctions.

Sink equilibria and convergence
We introduce the concept of a sink equilibrium. A sink equilibrium is a strongly connected component with no outgoing arcs in the strategy profile graph associated with a game. The strategy profile graph has a vertex set induced by the set of pure strategy profiles; its arc set corresponds to transitions between strategy profiles that occur with nonzero probability. (Here our focus will just be on the special case in which the strategy profile graph is actually a best response graph; that is, its arc set corresponds exactly to best response moves that result from myopic or greedy behaviour). We argue that there is a natural convergence process to sink equilibria in games where agents use pure strategies. This leads to an alternative measure of the social cost of a lack of coordination, the price of sinking, which measures the worst case ratio between the value of a sink equilibrium and the value of the socially optimal solution. We define the value of a sink equilibrium to be the expected social value of the steady state distribution induced by a random walk on that sink. We illustrate the value of this measure in three ways. Firstly, we show that it may more accurately reflects the inefficiency of uncoordinated solutions in competitive games when the use of pure strategies is the norm. In particular, we give an example (a valid-utility game) in which the game converges to solutions which are a factor n worse than socially optimal. The price of sinking is indeed n, but the price of anarchy is close to 1. Secondly, sink equilibria always exist. Thus, even in games in which pure strategy Nash equilibria (PSNE) do not exist, we can still calculate the price of sinking. Thirdly, we show that bounding the price of sinking can have important implications for the speed of convergence to socially good solutions in games where the agents make best response moves in a random order. We present two examples to illustrate our ideas. (i) Unsplittable selfish routing (and weighted congestion games):we prove that the price of sinking for the weighted unsplittable flow version of the selfish routing problem (for bounded-degree polynomial latency functions) is at most O(2/sup 2d/ d/sup 2d + 3/). In comparison, we give instances of these games without any PSNE. Moreover, our proof technique implies fast convergence to socially good (approximate) solutions. This is in contrast to the negative result of Fabrikant, Papadimitriou, and Talwar (2004) showing the existence of exponentially long best-response paths. (ii) Valid-utility games: we show that for valid-utility games the price of sinking is at most n+1; thus the worst case price of sinking in a valid-utility game is between it and n+1. We use our proof to show fast convergence to constant factor approximate solutions in basic-utility games. In addition, we present a hardness result which shows that, in general, there might be states that are exponentially far from any sink equilibrium in valid-utility games. We prove this by showing that the problem of finding a sink equilibrium (or a PSNE) in valid-utility games is PLS-complete.

Approximation algorithms for minimum-cost k-vertex connected subgraphs
(MATH) We present two new algorithms for the problem of finding a minimum-cost k-vertex connected spanning subgraph. The first algorithm works on undirected graphs with at least 6k2 vertices and achieves an approximation factor of 6 times the kth harmonic number, which is $O(\log k)$. The second algorithm works on directed and undirected graphs. It gives an $O(\sqrt{ n /\keps})$-approximation algorithm for any $\keps > 0$ and $k \le (1-\keps)n$. The latter algorithm also extends to other problems in network design with vertex connectivity requirements. Our main tools are setpair relaxations, a theorem of Mader's (in the undirected case) and iterative rounding (general case).

Convergence issues in competitive games
Nash equilibria in random games
We consider Nash equilibria in 2-player random games and analyze a simple Las Vegas algorithm for finding an equilibrium. The algorithm is combinatorial and always finds a Nash equilibrium; on m /spl times/ n payoff matrices, it runs in time O(m/sup 2/n log log n + n/sup 2/m log log m) with high probability. Our main tool is a polytope formulation of equilibria.

Factor 4/3 approximations for minimum 2-connected subgraphs
On the odd-minor variant of Hadwiger's conjecture
Network design via iterative rounding of setpair relaxations
Approximating the minimum strongly connected subgraph via a matching lower bound
We present a 3/2-approximation algorithm for the problem of finding a minimum strongly connected spanning subgraph in a given directed graph. As a corollary we obtain a 3/2-approximation algorithm for the more general minimum equivalent digraph problem. The performance of our algorithm is measured against a lower bound obtained from a simple matching problem. The performance guarantee is optimal with respect to the lower bound.

Simultaneous clustering of multiple gene expression and physical interaction datasets
Many genome-wide datasets are routinely generated to study different aspects of biological systems, but integrating them to obtain a coherent view of the underlying biology remains a challenge. We propose simultaneous clustering of multiple networks as a framework to integrate large-scale datasets on the interactions among and activities of cellular components. Specifically, we develop an algorithm JointCluster that finds sets of genes that cluster well in multiple networks of interest, such as coexpression networks summarizing correlations among the expression profiles of genes and physical networks describing protein-protein and protein-DNA interactions among genes or gene-products. Our algorithm provides an efficient solution to a well-defined problem of jointly clustering networks, using techniques that permit certain theoretical guarantees on the quality of the detected clustering relative to the optimal clustering. These guarantees coupled with an effective scaling heuristic and the flexibility to handle multiple heterogeneous networks make our method JointCluster an advance over earlier approaches. Simulation results showed JointCluster to be more robust than alternate methods in recovering clusters implanted in networks with high false positive rates. In systematic evaluation of JointCluster and some earlier approaches for combined analysis of the yeast physical network and two gene expression datasets under glucose and ethanol growth conditions, JointCluster discovers clusters that are more consistently enriched for various reference classes capturing different aspects of yeast biology or yield better coverage of the analysed genes. These robust clusters, which are supported across multiple genomic datasets and diverse reference classes, agree with known biology of yeast under these growth conditions, elucidate the genetic control of coordinated transcription, and enable functional predictions for a number of uncharacterized genes.

A priority-based model of routing
We consider a priority-based selfish routing model, where agents may have different priorities on a link. An agent with a higher priority on a link can traverse it with a smaller delay or cost than an agent with lower priority. This general framework can be used to model a number of different problems. The structural properties that lead to inefficiencies in routing choices appear different in this priority-based model compared to the classical model. In particular, in parallel link networks with nonatomic agents, the price of anarchy is exactly one in the priority-based model; that is, selfish behaviour leads to optimal routings. In contrast, in the standard model the worst possible price of anarchy can be achieved in a simple two-link network. For multi-commodity networks, selfish routing does lead to inefficiencies in the priority-based model. We present tight bounds on the price of anarchy for such networks. Specifically, in the nonatomic case the worst-case price of anarchy is exactly (d + 1) for polynomial latency functions of degree d (hence 4 for linear cost functions). For atomic games, the worst-case price of anarchy is exactly 3+ 2 √ 2 in the weighted case, and exactly 17/3 in the unweighted case. An upper bound of O(2d) is also shown for polynomial cost functions in the atomic case, although this is not tight. Our framework (and results) also generalise to include models similar to congestion games. ACM Classification: F.2.0, F.2.2 AMS Classification: 68Q25, 68M10, 90B18

Approximation algorithms for network design with metric costs
We study undirected networks with edge costs that satisfy the triangle inequality. Let <i>n</i> denote the number of nodes. We present an <i>O</i>(1)-approximation algorithm for a generalization of the metric-cost subset <i>k</i>-node-connectivity problem. Our approximation guarantee is proved via lower bounds that apply to the simple edge-connectivity version of the problem, where the requirements are for edge-disjoint paths rather than for openly node-disjoint paths. A corollary is that, for metric costs and for each <i>k</i>=1,2,…,<i>n</i>-1, there exists a <i>k</i>-node connected graph whose cost is within a factor of 24 of the cost of any simple <i>k</i>-edge connected graph. This resolves an open question in the area. Based on our <i>O</i>(1)-approximation algorithm, we present an <i>O</i>(log <i>r</i><inf>max</inf>)-approximation algorithm for the node-connectivity survivable network design problem where <i>r</i><inf>max</inf> denotes the maximum requirement over all pairs of nodes. Our results contrast with the case of edge costs of zero or one, where Kortsarz et al. [20]recently proved, assuming NP⊈, quasi-P, a hardness-of-approximation lower bound of 2<sup>log 1-ε<i>n</i></sup> for the subset <i>k</i>-node-connectivity problem, where ε denotes a small positive number.

An approximation algorithm for the maximum leaf spanning arborescence problem
We present an O(&sqrt;opt)-approximation algorithm for the maximum leaf spanning arborescence problem, where opt is the number of leaves in an optimal spanning arborescence. The result is based upon an O(1)-approximation algorithm for a special class of directed graphs called willows. Incorporating the method for willow graphs as a subroutine in a local improvement algorithm gives the bound for general directed graphs.

An upper bound for the chromatic number of line graphs
The demand-matching problem
Planar graph bipartization in linear time
Randomized experimental design for causal graph discovery
We examine the number of controlled experiments required to discover a causal graph. Hauser and Buhlmann [1] showed that the number of experiments required is logarithmic in the cardinality of maximum undirected clique in the essential graph. Their lower bounds, however, assume that the experiment designer cannot use randomization in selecting the experiments. We show that significant improvements are possible with the aid of randomization - in an adversarial (worst-case) setting, the designer can then recover the causal graph using at most O(log log n) experiments in expectation. This bound cannot be improved; we show it is tight for some causal graphs. 
 
We then show that in a non-adversarial (average-case) setting, even larger improvements are possible: if the causal graph is chosen uniformly at random under a Erdos-Renyi model then the expected number of experiments to discover the causal graph is constant. Finally, we present computer simulations to complement our theoretic results. 
 
Our work exploits a structural characterization of essential graphs by Andersson et al. [2]. Their characterization is based upon a set of orientation forcing operations. Our results show a distinction between which forcing operations are most important in worst-case and average-case settings.

A near-optimal mechanism for impartial selection
DONE