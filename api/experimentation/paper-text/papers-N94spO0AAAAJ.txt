Faster fully dynamic matchings with small approximation ratios
Maximum cardinality matching is a fundamental algorithmic problem with many algorithms and applications. The fully dynamic version, in which edges are inserted and deleted over time has also been the subject of much attention. Existing algorithms for dynamic matching (in general n-vertex m-edge graphs) fall into two groups: there are fast (mostly randomized) algorithms that achieve a 2-approximation or worse, and there are slow algorithms with Ω([EQUATION]) update time that achieve a better-than-2 approximation. Thus the obvious question is whether we can design an algorithm that achieves a tradeoff between these two: a o([EQUATION]) update time and a better-than-2 approximation simultaneously. We answer this question in the affirmative. Previously, such bounds were only known for the special case of bipartite graphs. Our main result is a fully dynamic deterministic algorithm that maintains a (3/2 + e)-approximation in amortized update time O(m1/4e--2.5). In addition to achieving the trade-off described above, our algorithm manages to be polynomially faster than all existing deterministic algorithms (excluding an existing log n-approximation of Onak and Rubinfeld), while still maintaining a better-than-2 approximation. We also give stronger results for graphs whose arboricity is at most α. We show how to maintain a (1 + e)-approximate fractional matching or a (3/2 + e)-approximate integral matching in worst-case time O(α(α + log n)) for constant e. When the arboricity is constant, this bound is O(log n) and when the arboricity is polylogarithmic the update time is also polylogarithmic. Previous results for small arboricity non-bipartite graphs could only maintain a maximal matching (2-approximation). We maintain the approximate matching without explicitly using augmenting paths. We define an intermediate graph, called an EDCS and show that the EDCS H contains a large matching, and show how to maintain an EDCS in G. The EDCS was used in previous works on bipartite graphs, however the details and proofs are completely different in general graphs. The algorithm for bipartite graphs relies on ideas from flows and cuts to non-constructively prove the existence of a good matching in H, but these ideas do not seem to extend to non-bipartite graphs. In this paper we instead explicitly construct a large fractional matching in H. In some cases we can guarantee that this fractional matching is γ-restricted, which means that it only uses values either in the range [0, γ] or 1. We then combine this matching with a new structural property of maximum matchings in non-bipartite graphs, which is analogous to the cut induced by maximum matchings in bipartite graphs.

Coresets meet EDCS: algorithms for matching and vertex cover on massive graphs
Randomized composable coresets were introduced recently as an effective technique for solving matching and vertex cover problems in various models of computation. In this technique, one partitions the edges of an input graph randomly into multiple pieces, compresses each piece into a smaller subgraph, namely a coreset, and solves the problem on the union of these coresets to find the solution. By designing small size randomized composable coresets, one can obtain efficient algorithms, in a black-box way, in multiple computational models including streaming, distributed communication, and the massively parallel computation (MPC) model. 
We develop randomized composable coresets of size $\widetilde{O}(n)$ that for any constant $\varepsilon > 0$, give a $(3/2+\varepsilon)$-approximation to matching and a $(3+\varepsilon)$-approximation to vertex cover. Our coresets improve upon the previously best approximation ratio of $O(1)$ for matching and $O(\log{n})$ for vertex cover. Most notably, our result for matching goes beyond a 2-approximation, which is a natural barrier for maximum matching in many models of computation. 
Furthermore, inspired by the recent work of Czumaj et.al. (arXiv 2017), we study algorithms for matching and vertex cover in the MPC model with only $\widetilde{O}(n)$ memory per machine. Building on our coreset constructions, we develop parallel algorithms that give a $(1+\varepsilon)$-approximation to matching and $O(1)$-approximation to vertex cover in only $O_{\varepsilon}(\log\log{n})$ MPC rounds and $\widetilde{O}(n)$ memory per machine. 
A key technical ingredient of our paper is a novel application of edge degree constrained subgraphs (EDCS). At the heart of our proofs are new structural properties of EDCS that identify these subgraphs as sparse certificates for large matchings and small vertex covers which are quite robust to sampling and composition.

A nearly optimal oracle for avoiding failed vertices and edges
We present an improved oracle for the distance sensitivity problem. The goal is to preprocess a directed graph G = (V,E) with non-negative edge weights to answer queries of the form: what is the length of the shortest path from x to y that does not go through some failed vertex or edge f. The previous best algorithm produces an oracle of size ~O(n2) that has an O(1) query time, and an ~O(n2√m) construction time. It was a randomized Monte Carlo algorithm that worked with high probability. Our oracle also has a constant query time and an ~O(n2) space requirement, but it has an improved construction time of ~O(mn), and it is deterministic. Note that O(1) query, O(n2) space, and O(mn) construction time is also the best known bound (up to logarithmic factors) for the simpler problem of finding all pairs shortest paths in a weighted, directed graph. Thus, barring improved solutions to the all pairs shortest path problem, our oracle is optimal up to logarithmic factors.

Fully dynamic matching in bipartite graphs
Maintaining shortest paths under deletions in weighted directed graphs
We present an improved algorithm for maintaining all-pairs $(1 + \epsilon)$ approximate shortest paths under deletions and weight-increases. The previous state of the art for this problem is total update time $\widetilde{O}(n^2\sqrt{m}/\epsilon)$ over all updates for directed unweighted graphs [S. Baswana, R. Hariharan, and S. Sen, J. Algorithms, 62 (2007), pp. 74--92], and $\widetilde{O}(mn/\epsilon)$ for undirected unweighted graphs [L. Roditty and U. Zwick, in Proceedings of the 45th FOCS, Rome, Italy, 2004, pp. 499--508]. Both algorithms are randomized and have constant query time. Very recently, Henzinger, Krinninger, and Nanongkai presented a deterministic version of the latter algorithm [M. Henzinger, S. Krinninger, and D. Nanongkai, in IEEE FOCS, 2013, pp. 538--547]. Note that $\widetilde{O}(mn)$ is a natural barrier because even with a $(1 + \epsilon)$ approximation, there is no $o(mn)$ combinatorial algorithm for the static all-pairs shortest path problem. Our algorithm works on directed weighte...

Fully dynamic (2+ ε) approximate all-pairs shortest paths with fast query and close to linear update time
For any fixed $1 ≫ \eps ≫ 0$ we present a fully dynamic algorithm for maintaining $(2 + \eps)$-approximate all-pairs shortest paths in undirected graphs with positive edge weights. We use a randomized (Las Vegas) update algorithm (but a deterministic query procedure), so the time given is the \emph{expected} amortized update time. \\\indent Our query time $O(\log \log \log n)$. The update time is $\widetilde{O}(mn^{O(1/\sqrt{\log n})}\log(nR))$, where $R$ is the ratio between the heaviest and the lightest edge weight in the graph (so $R = 1$ in unweighted graphs). Unfortunately, the update time does have the drawback of a super-polynomial dependence on $\eps$: it grows as $(3 / \eps)^{\sqrt{\log n / \log(3/\eps)}} = n^{\sqrt{\log(3/\eps) / \log n}}$.\\\indent Our algorithm has a significantly faster update time than any other algorithm with sub-polynomial query time. For exact distances, the state of the art algorithm has an update time of $\widetilde{O}(n^2)$. For approximate distances, the best previous algorithm has a $O(kmn^{1/k})$ update time and returns $(2k - 1)$ stretch paths. Thus, it needs an update time of $O(m\sqrt{n})$ to get close to our approximation, and it has to return $O(\sqrt{\log n})$ approximate distances to match our update time.

A deamortization approach for dynamic spanner and dynamic maximal matching
Many dynamic graph algorithms have an amortized update time, rather than a stronger worst-case guarantee. But amortized data structures are not suitable for real-time systems, where each individual operation has to be executed quickly. For this reason, there exist many recent randomized results that aim to provide a guarantee stronger than amortized expected. The strongest possible guarantee for a randomized algorithm is that it is always correct (Las Vegas) and has high-probability worst-case update time, which gives a bound on the time for each individual operation that holds with high probability. In this article, we present the first polylogarithmic high-probability worst-case time bounds for the dynamic spanner and the dynamic maximal matching problem. (1) For dynamic spanner, the only known o(n) worst-case bounds were O(n3/4) high-probability worst-case update time for maintaining a 3-spanner and O(n5/9) for maintaining a 5-spanner. We give a O(1)k log3 (n) high-probability worst-case time bound for maintaining a (2k-1)-spanner, which yields the first worst-case polylog update time for all constant k. (All the results above maintain the optimal tradeoff of stretch 2k-1 and Õ(n1+1/k) edges.) (2) For dynamic maximal matching, or dynamic 2-approximate maximum matching, no algorithm with o(n) worst-case time bound was known and we present an algorithm with O(log 5 (n)) high-probability worst-case time; similar worst-case bounds existed only for maintaining a matching that was (2+ϵ)-approximate, and hence not maximal. Our results are achieved using a new approach for converting amortized guarantees to worst-case ones for randomized data structures by going through a third type of guarantee, which is a middle ground between the two above: An algorithm is said to have worst-case expected update time ɑ if for every update σ, the expected time to process σ is at most ɑ. Although stronger than amortized expected, the worst-case expected guarantee does not resolve the fundamental problem of amortization: A worst-case expected update time of O(1) still allows for the possibility that every 1/f(n) updates requires ϴ (f(n)) time to process, for arbitrarily high f(n). In this article, we present a black-box reduction that converts any data structure with worst-case expected update time into one with a high-probability worst-case update time: The query time remains the same, while the update time increases by a factor of O(log 2(n)). Thus, we achieve our results in two steps: (1) First, we show how to convert existing dynamic graph algorithms with amortized expected polylogarithmic running times into algorithms with worst-case expected polylogarithmic running times. (2) Then, we use our black-box reduction to achieve the polylogarithmic high-probability worst-case time bound. All our algorithms are Las-Vegas-type algorithms.

Improved dynamic algorithms for maintaining approximate shortest paths under deletions
We present the first dynamic shortest paths algorithms that make any progress beyond a long-standing <i>O</i>(<i>n</i>) update time barrier (while maintaining a reasonable query time), although it is only progress for not-too-sparse graphs. In particular, we obtain new decremental algorithms for two approximate shortest-path problems in unweighted, undirected graphs. Both algorithms are randomized (Las Vegas).
 • Given a source <i>s</i>, we present an algorithm that maintains (1 + ε)-approximate shortest paths from <i>s</i> with an expected <i>total</i> update time of <i>Õ</i>(<i>n</i><sup>2+<i>O</i>(1/√log <i>n</i>)</sup>) over all deletions (so the amortized time is about <i>Õ</i>(<i>n</i><sup>2</sup>/<i>m</i>)). The worst-case query time is constant. The best previous result goes back <i>three</i> decades to Even and Shiloach [16] and Dinitz [12]. They show how to decrementally maintain an <i>exact</i> shortest path tree with a total update time of <i>O</i>(<i>mn</i>) (amortized update time <i>O</i>(<i>n</i>)). Roditty and Zwick [22] have shown that <i>O</i>(<i>mn</i>) is actually optimal for <i>exact</i> paths (barring a better combinatorial algorithm for boolean matrix multiplication), unless we are willing to settle for a Ω(<i>n</i>) query time. In fact, until now, even <i>approximate</i> dynamic algorithms were not able to go beyond <i>O</i>(<i>mn</i>).
 • For any fixed integer <i>k</i> ≥ 2, we present an algorithm that decrementally maintains a distance oracle (for <i>all pairs</i> shortest distances) with a total expected update time of <i>Õ</i>(<i>n</i><sup>2+1/<i>k</i>+<i>O</i>(1/√log <i>n</i>)</sup>) (amortized update time about <i>Õ</i>(<i>n</i><sup>2+1/<i>k</i></sup>/<i>m</i>). The space requirement is only <i>O</i>(<i>m</i> + <i>n</i><sup>1+1/<i>k</i></sup>), the stretch of the returned distances is at most 2<i>k</i> − 1 + ε, and the worst-case query time is <i>O</i>(1). The best previous result of Roditty and Zwick [21] had a total update time of <i>Õ</i>(<i>mn</i>) and a stretch of 2<i>k</i> − 1. Note that our algorithm implicitly solves the decremental all-pairs shortest path problem with the same bounds; the best previous approximation algorithm of Roditty and Zwick [21] returned (1 + ε) approximate distances, but used <i>O</i>(<i>n</i><sup>2</sup>) space, and required <i>Õ</i>(<i>mn</i>) total update time. As with the previous problem, our algorithm is the first to make progress beyond the <i>O</i>(<i>mn</i>) total update time barrier while maintaining a small query time.
 We present a general framework for accelerating decremental algorithms. In particular, our main idea is to run existing decremental algorithms on a sparse subgraph (such as a spanner or emulator) of the graph rather than on the original graph <i>G</i>. Although this is a common approach for <i>static</i> approximate shortest-path problems, it has never been used in a decremental setting because maintaining the subgraph <i>H</i> as edges are being deleted from <i>G</i> might require <i>inserting</i> edges into <i>H</i>, thus ruining the "decrementality" of the setting. We overcome this by presenting an emulator whose maintenance only requires a limited number of well-behaved insertions.
 In other words, we present a general technique for running decremental algorithms on a sparse subgraph of the graph. Once our framework is in place, applying it to any particular decremental algorithm only requires trivial modifications; most of the work consists of showing that these algorithms <i>as they are</i> still work in our restricted fully dynamic setting, where we encounter not just arbitrary deletions (as in the original setting), but also restricted insertions.

Deterministic decremental reachability, scc, and shortest paths via directed expanders and congestion balancing
Let $G=(V, E, w)$ be a weighted, directed graph subject to a sequence of adversarial edge deletions. In the decremental single-source reachability problem (SSR), we are given a fixed source $s$ and the goal is to maintain a data structure that can answer path-queries $s\rightarrowtail v$ for any $v\in V$. In the more general single-source shortest paths (SSSP) problem the goal is to return an approximate shortest path to $v$, and in the SCC problem the goal is to maintain strongly connected components of $G$ and to answer path queries within each component. All of these problems have been very actively studied over the past two decades, but all the fast algorithms are randomized and, more significantly, they can only answer path queries if they assume a weaker model: they assume an oblivious adversary which is not adaptive and must fix the update sequence in advance. This assumption significantly limits the use of these data structures, most notably preventing them from being used as subroutines in static algorithms. All the above problems are notoriously difficult in the adaptive setting. In fact, the state-of-the-art is still the Even and Shiloach tree, which dates back all the way to 1981 [1] and achieves total update time $O(mn)$. We present the first algorithms to break through this barrier. •deterministic decremental SSR/SSC with total update time $mn^{2/3+o(1)}$ •deterministic decremental SSSP with total update time $n^{2+2/3+o(1)}$ To achieve these results, we develop two general techniques for working with dynamic graphs. The first generalizes expander-based tools to dynamic directed graphs. While these tools have already proven very successful in undirected graphs, the underlying expander decomposition they rely on does not exist in directed graphs. We thus need to develop an efficient framework for using expanders in directed graphs, as well as overcome several technical challenges in processing directed expanders. We establish several powerful primitives that we hope will pave the way for other expander-based algorithms in directed graphs. The second technique, which we call congestion balancing, provides a new method for maintaining flow under adversarial deletions. The results above use this technique to maintain an embedding of an expander. The technique is quite general, and to highlight its power, we use it to achieve the following additional result: •The first near-optimal algorithm for decremental bipartite matching

A nearly optimal algorithm for approximating replacement paths and k shortest simple paths in general graphs
Let <i>G = (V, E)</i> be a directed graph with positive edge weights, let <i>s, t</i> be two specified vertices in this graph, and let π(<i>s, t</i>) be the shortest path between them. In the <i>replacement paths</i> problem we want to compute, for every edge <i>e</i> on π(<i>s, t</i>), the shortest path from <i>s</i> to <i>t</i> that avoids <i>e</i>. The naive solution to this problem would be to remove each edge <i>e</i>, one at a time, and compute the shortest <i>s - t</i> path each time; this yields a running time of <i>O(mn + n<sup>2</sup></i> log <i>n</i>). Gotthilf and Lewenstein [8] recently improved this to <i>O(mn+n<sup>2</sup></i> log log <i>n</i>), but no <i>o(mn)</i> algorithms are known.
 We present the first <i>approximation</i> algorithm for replacement paths in directed graphs with positive edge weights. Given any ε ε [0, 1), our algorithm returns (1 + ε)-approximate replacement paths in <i>O</i>(ε<sup>-1</sup> log<sup>2</sup> <i>n</i> log(<i>nC/c)(m+n</i> log <i>n)) = Õ(m</i> log(<i>nC/c</i>)/ε) time, where <i>C</i> is the largest edge weight in the graph and <i>c</i> is the smallest weight.
 We also present an even faster (1 + ε) approximate algorithm for the simpler problem of approximating the <i>k</i> shortest simple <i>s - t</i> paths in a directed graph with positive edge weights. That is, our algorithm outputs <i>k</i> different simple <i>s - t</i> paths, where the kth path we output is a (1 + ε) approximation to the actual kth shortest simple <i>s - t</i> path. The running time of our algorithm is <i>O(k</i>ε<sup>-1</sup> log<sup>2</sup> <i>n(m + n</i> log <i>n)) = Õ(km/ε)</i>. The fastest <i>exact</i> algorithm for this problem has a running time of <i>O(k(mn+n</i><sup>2</sup> log log <i>n)) = Õ(kmn)</i> [8]. The previous best approximation algorithm was developed by Roditty [15]; it has a stretch of 3/2 and a running time of Õ(km√n) (it does not work for replacement paths).
 Note that all of our running times are nearly optimal except for the <i>O</i> (log(<i>nC/c</i>)) factor in the replacements paths algorithm. Also, our algorithm can solve the variant of approximate replacement paths where we avoid vertices instead of edges.

Deterministic decremental single source shortest paths: beyond the o (mn) bound
In this paper we consider the decremental single-source shortest paths (SSSP) problem, where given a graph G and a source node s the goal is to maintain shortest paths between s and all other nodes in G under a sequence of online adversarial edge deletions. In their seminal work, Even and Shiloach [JACM 1981] presented an exact solution to the problem with only O(mn) total update time over all edge deletions. Their classic algorithm was the best known result for the decremental SSSP problem for three decades, even when approximate shortest paths are allowed. The first improvement over the Even-Shiloach algorithm was given by Bernstein and Roditty [SODA 2011], who for the case of an unweighted and undirected graph presented an approximate (1+) algorithm with constant query time and a total update time of O(n2+O(1/√logn)). This work triggered a series of new results, culminating in a recent breakthrough of Henzinger, Krinninger and Nanongkai [FOCS 14], who presented a -approximate algorithm whose total update time is near linear O(m1+ O(1/√logn)). In this paper they posed as a major open problem the question of derandomizing their result. In fact, all known improvements over the Even-Shiloach algorithm are randomized. All these algorithms maintain some truncated shortest path trees from a small subset of nodes. While in the randomized setting it is possible to “hide” these nodes from the adversary, in the deterministic setting this is impossible: the adversary can delete all edges touching these nodes, thus forcing the algorithm to choose a new set of nodes and incur a new computation of shortest paths. In this paper we present the first deterministic decremental SSSP algorithm that breaks the Even-Shiloach bound of O(mn) total update time, for unweighted and undirected graphs. Our algorithm is (1 + є) approximate and achieves a total update time of Õ(n2). Our algorithm can also achieve the same bounds in the incremental setting. It is worth mentioning that for dense instances where m = Ω(n2 − 1/√log(n)), our algorithm is also faster than all existing randomized algorithms.

Online Bipartite Matching with Amortized O(log 2 n) Replacements
In the online bipartite matching problem with replacements, all the vertices on one side of the bipartition are given, and the vertices on the other side arrive one-by-one with all their incident edges. The goal is to maintain a maximum matching while minimizing the number of changes (replacements) to the matching. We show that the greedy algorithm that always takes the shortest augmenting path from the newly inserted vertex (denoted the SAP protocol) uses at most amortized O(log 2 n) replacements per insertion, where n is the total number of vertices inserted. This is the first analysis to achieve a polylogarithmic number of replacements for any replacement strategy, almost matching the Ω (log n) lower bound. The previous best strategy known achieved amortized O(√ n) replacements [Bosek, Leniowski, Sankowski, Zych, FOCS 2014]. For the SAP protocol in particular, nothing better than the trivial O(n) bound was known except in special cases. Our analysis immediately implies the same upper bound of O(log 2 n) reassignments for the capacitated assignment problem, where each vertex on the static side of the bipartition is initialized with the capacity to serve a number of vertices. We also analyze the problem of minimizing the maximum server load. We show that if the final graph has maximum server load L, then the SAP protocol makes amortized O(min { L log2 n , √ nlog n}) reassignments. We also show that this is close to tight, because Ω (min { L, √ n}) reassignments can be necessary.

Deterministic decremental sssp and approximate min-cost flow in almost-linear time
In the decremental single-source shortest paths problem, the goal is to maintain distances from a fixed source $s$ to every vertex $v$ in an m-edge graph undergoing edge deletions. In this paper, we conclude a long line of research on this problem by showing a near-optimal deterministic data structure that maintains (1 + E) -approximate distance estimates and runs in m1+o(1)total update time. Our result, in particular, removes the oblivious adversary assumption required by the previous breakthrough result by Henzinger et al. [FOCS'14], which leads to our second result: the first almost-linear time algorithm for (1 - E) -approximate min-cost flow in undirected graphs where capacities and costs can be taken over edges and vertices. Previously, algorithms for max flow with vertex capacities, or min-cost flow with any capacities required super-linear time. Our result essentially completes the picture for approximate flow in undirected graphs. The key technique of the first result is a novel framework that allows us to treat low-diameter graphs like expanders. This allows us to harness expander properties while bypassing shortcomings of expander decomposition, which almost all previous expander-based algorithms needed to deal with. For the second result, we break the notorious flow-decomposition barrier from the multiplicative-weight-update framework using randomization.

Fully-dynamic graph sparsifiers against an adaptive adversary
Designing dynamic graph algorithms against an adaptive adversary is a major goal in the field of dynamic graph algorithms. While a few such algorithms are known for spanning trees, matchings, and single-source shortest paths, very little was known for an important primitive like graph sparsifiers. The challenge is how to approximately preserve so much information about the graph (e.g., all-pairs distances and all cuts) without revealing the algorithms' underlying randomness to the adaptive adversary. 
In this paper we present the first non-trivial efficient adaptive algorithms for maintaining spanners and cut sparisifers. These algorithms in turn imply improvements over existing algorithms for other problems. Our first algorithm maintains a polylog$(n)$-spanner of size $\tilde O(n)$ in polylog$(n)$ amortized update time. The second algorithm maintains an $O(k)$-approximate cut sparsifier of size $\tilde O(n)$ in $\tilde O(n^{1/k})$ amortized update time, for any $k\ge1$, which is polylog$(n)$ time when $k=\log(n)$. The third algorithm maintains a polylog$(n)$-approximate spectral sparsifier in polylog$(n)$ amortized update time. The amortized update time of both algorithms can be made worst-case by paying some sub-polynomial factors. Prior to our result, there were near-optimal algorithms against oblivious adversaries (e.g. Baswana et al. [TALG'12] and Abraham et al. [FOCS'16]), but the only non-trivial adaptive dynamic algorithm requires $O(n)$ amortized update time to maintain $3$- and $5$-spanner of size $O(n^{1+1/2})$ and $O(n^{1+1/3})$, respectively [Ausiello et al. ESA'05]. 
Our results are based on two novel techniques. The first technique, is a generic black-box reduction that allows us to assume that the graph undergoes only edge deletions and, more importantly, remains an expander with almost-uniform degree. The second technique we call proactive resampling. [...]

Distributed exact weighted all-pairs shortest paths in near-linear time
In the distributed all-pairs shortest paths problem (APSP), every node in the weighted undirected distributed network (the CONGEST model) needs to know the distance from every other node using least number of communication rounds (typically called time complexity). The problem admits (1+o(1))-approximation Θ(n)-time algorithm and a nearly-tight Ω(n) lower bound [Nanongkai, STOC’14; Lenzen and Patt-Shamir PODC’15]. For the exact case, Elkin [STOC’17] presented an O(n5/3 log2/3 n) time bound, which was later improved to Õ(n5/4) in [Huang, Nanongkai, Saranurak FOCS’17].It was shown that any super-linear lower bound (in n) requires a new technique [Censor-Hillel, Khoury, Paz, DISC’17], but otherwise it remained widely open whether there exists a Õ(n)-time algorithm for the exact case, which would match the best possible approximation algorithm. This paper resolves this question positively: we present a randomized (Las Vegas) Õ(n)-time algorithm, matching the lower bound up to polylogarithmic factors. Like the previous Õ(n5/4) bound, our result works for directed graphs with zero (and even negative) edge weights. In addition to the improved running time, our algorithm works in a more general setting than that required by the previous Õ(n5/4) bound; in our setting (i) the communication is only along edge directions (as opposed to bidirectional), and (ii) edge weights are arbitrary (as opposed to integers in {1, 2, ... poly(n)}). The previously best algorithm for this more difficult setting required Õ(n3/2) time [Agarwal and Ramachandran, ArXiv’18] (this can be improved to Õ(n4/3) if one allows bidirectional communication). Our algorithm is extremely simple and relies on a new technique called Random Filtered Broadcast. Given any sets of nodes A,B⊆ V and assuming that every b ∈ B knows all distances from nodes in A, and every node v ∈ V knows all distances from nodes in B, we want every v∈ V to know DistThroughB(a,v) = minb∈ B dist(a,b) + dist(b,v) for every a∈ A. Previous works typically solve this problem by broadcasting all knowledge of every b∈ B, causing super-linear edge congestion and time. We show a randomized algorithm that can reduce edge congestions and thus solve this problem in Õ(n) expected time.

Towards a unified theory of sparsification for matching problems
In this paper, we present a construction of a `matching sparsifier', that is, a sparse subgraph of the given graph that preserves large matchings approximately and is robust to modifications of the graph. We use this matching sparsifier to obtain several new algorithmic results for the maximum matching problem: 
* An almost $(3/2)$-approximation one-way communication protocol for the maximum matching problem, significantly simplifying the $(3/2)$-approximation protocol of Goel, Kapralov, and Khanna (SODA 2012) and extending it from bipartite graphs to general graphs. 
* An almost $(3/2)$-approximation algorithm for the stochastic matching problem, improving upon and significantly simplifying the previous $1.999$-approximation algorithm of Assadi, Khanna, and Li (EC 2017). 
* An almost $(3/2)$-approximation algorithm for the fault-tolerant matching problem, which, to our knowledge, is the first non-trivial algorithm for this problem. 
Our matching sparsifier is obtained by proving new properties of the edge-degree constrained subgraph (EDCS) of Bernstein and Stein (ICALP 2015; SODA 2016)---designed in the context of maintaining matchings in dynamic graphs---that identifies EDCS as an excellent choice for a matching sparsifier. This leads to surprisingly simple and non-technical proofs of the above results in a unified way. Along the way, we also provide a much simpler proof of the fact that an EDCS is guaranteed to contain a large matching, which may be of independent interest.

A generative theory of similarity
A Generative Theory of Similarity Charles Kemp, Aaron Bernstein & Joshua B. Tenenbaum {ckemp, aaronber, jbt}@mit.edu Department of Brain and Cognitive Sciences Massachusetts Institute of Technology Abstract We propose that similarity judgments are inferences about generative processes, and that two objects appear similar when they are likely to have been generated by the same process. We present a formal model based on this idea, and suggest that it may be particularly useful for explaining high-level judgments of similarity. We compare our model to featural and transformational accounts, and describe an experiment where it outper- forms a transformational model. Keywords: similarity; generative processes; computational theory Every object is the outcome of a generative process. An animal grows from a fertilized egg into an adult, a city develops from a settlement into a metropolis, and an artifact is assembled from a pile of raw materials ac- cording to the plan of its designer. Observations like these motivate the generative approach, which proposes that an object may be understood by thinking about the process that generated it. The promise of the approach is that apparently complex objects may be produced by simple processes, an insight that has proved productive across disciplines including biology (Thompson, 1961), physics (Wolfram, 2002), and architecture (Alexander, 1979). To give two celebrated examples from biology, the shape of a pinecone and the markings on a cheetah’s tail can be generated by remarkably simple processes of growth. These patterns can be characterized much more compactly by describing their causal history than by at- tempting to describe them directly. Leyton has argued that the generative approach pro- vides a general framework for understanding cognition. Applications of the approach can be found in generative theories of memory (Leyton, 1992), categorization (An- derson, 1991; Feldman, 1997; Rehder, 2003), visual perception (Leyton, 1992), speech perception (Liber- man et al., 1967), syntax (Chomsky, 1965) 1 , and mu- sic (Lehrdahl and Jackendoff, 1996). This paper offers a generative theory of similarity, a notion often invoked by models of high-level cognition. We argue that two objects are similar to the extent that they seem to have been generated by the same underlying process. The literature on similarity covers settings that ex- tend from the comparison of simple stimuli like tones and colored patches to the comparison of highly-structured objects like narratives. The generative approach is rele- vant to the entire spectrum of applications, but we are particularly interested in high-level similarity. In par- ticular, we are interested in how similarity judgments draw on intuitive theories, or systems of rich conceptual knowledge (Murphy and Medin, 1985). Intuitive theories and generative processes are intimately linked: Murphy (1993), for example, defines a theory as “a set of causal relations that collectively generate or explain the phe- nomena in a domain.” Our generative framework should therefore help to explain how similarity judgments are guided by intuitive theories. Others have recognized the importance of this issue: Murphy and Medin (1985) sug- gest, for example, that “the notion of similarity must be extended to include theoretical knowledge.” We develop a formal theory of similarity and compare it to two existing theories. The featural account (Tver- sky, 1977) suggests that the similarity of two objects is a function of their common and distinctive features, and the transformation account suggests that similarity de- pends on the number of operations required to transform one object into the other (Hahn et al., 2003). We show that versions of both approaches emerge as special cases of our model, and present an experiment that directly compares our model with the transformation account. Generative processes and similarity Before introducing our formal model, we describe several cases where the assessment of similarity relies on infer- ences about generative processes. Suppose we are shown The approach we have described should be distinguished from two usages of “generative” that are found in the linguis- tics literature. Generativity sometimes refers to the infinite use of finite means: for us, a generative process need not meet this criterion, although many interesting processes will. The second (and more central) usage refers to a grammar’s abil- ity to generate the set of grammatical sentences: Chomsky (1965) defines a generative grammar as “a system of rules that in some explicit and well-defined way assigns structural descriptions to sentences.” A system of this sort need not be generative in our sense — in particular, it need not as- sume that structural descriptions have derivational histories. Generative grammars, however, are typically expressed us- ing formalisms that do assign derivational histories to struc- tural descriptions, and theories that assume the psychological reality of these histories are instances of the generative ap- proach. Chomsky (1995) has rejected theories of this sort: “the ordering of operations [in grammatical theory] is ab- stract, expressing postulated properties of the language fac- ulty of the brain, with no temporal interpretation implied.” Others, however, argue for linguistic theories that are gener- ative in our sense (Marantz, To appear)

Improved bounds for matching in random-order streams
We study low-rank approximation in the streaming model in which the rows of an n x d matrix A are presented one at a time in an arbitrary order. At the end of the stream, the streaming algorithm should output a k x d matrix R so that ‖A - AR† R‖2F ≤ (1 + ∊)‖A - Ak‖2F where Ak is the best rank-k approximation to A. A deterministic streaming algorithm of Liberty (KDD, 2013), with an improved analysis of Ghashami and Phillips (SODA, 2014), provides such a streaming algorithm using O(dk/∊) words of space. A natural question is if smaller space is possible. We give an almost matching lower bound of Ω(dk/∊) bits of space, even for randomized algorithms which succeed only with constant probability. Our lower bound matches the upper bound of Ghashami and Phillips up to the word size, improving on a simple Ω(dk) space lower bound.

Decremental strongly-connected components and single-source reachability in near-linear time
Computing the Strongly-Connected Components (SCCs) in a graph G=(V,E) is known to take only O(m+n) time using an algorithm by Tarjan from 1972[SICOMP 72] where m = |E|, n=|V|. For fully-dynamic graphs, conditional lower bounds provide evidence that the update time cannot be improved by polynomial factors over recomputing the SCCs from scratch after every update. Nevertheless, substantial progress has been made to find algorithms with fast update time for decremental graphs, i.e. graphs that undergo edge deletions. In this paper, we present the first algorithm for general decremental graphs that maintains the SCCs in total update time Õ(m), thus only a polylogarithmic factor from the optimal running time. Previously such a result was only known for the special case of planar graphs [Italiano et al, STOC 17]. Our result should be compared to the formerly best algorithm for general graphs achieving Õ(m√n) total update time by Chechik et.al. [FOCS 16] which improved upon a breakthrough result leading to O(mn0.9 + o(1)) total update time by Henzinger, Krinninger and Nanongkai [STOC 14, ICALP 15]; these results in turn improved upon the longstanding bound of O(mn) by Roditty and Zwick [STOC 04]. All of the above results also apply to the decremental Single-Source Reachability (SSR) problem, which can be reduced to decrementally maintaining SCCs. A bound of O(mn) total update time for decremental SSR was established already in 1981 by Even and Shiloach [JACM 81].

Improved distance sensitivity oracles via random sampling
We present improved oracles for the distance sensitivity problem. The goal is to preprocess a graph G = (V,E) with non-negative edge weights to answer queries of the form: what is the length of the shortest path from x to y that does not go through some <i>failed</i> vertex or edge f. There are two state of the art algorithms for this problem. The first produces an oracle of size <i>Õ</i>(n<sup>2</sup>) that has an O(1) query time, and an <i>Õ</i>(mn<sup>2</sup>) construction time. The second oracle has size O(n<sup>2.5</sup>), but the construction time is only <i>Õ</i>(mn<sup>1.5</sup>). We present two new oracles that substantially improve upon both of these results. Both oracles are constructed with randomized, Monte Carlo algorithms. For directed graphs with non-negative edge weights, we present an oracle of size <i>Õ</i>(n<sup>2</sup>), which has an O(1) query time, and an <i>Õ</i>(n<sup>2</sup>√<i>m</i>) construction time. For unweighted graphs, we achieve a more general construction time of <i>Õ</i>(√n<sup>3</sup> · <i>APSP</i> + <i>mn</i>), where APSP is the time it takes to compute all pairs shortest paths in an aribtrary subgraph of G.

DONE