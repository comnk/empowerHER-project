Exploring the representation capabilities of the HOG descriptor
Object recognition strategies are increasingly based on regional descriptors such as SIFT or HOG at a sparse set of points or on a dense grid of points. Despite their success on databases such as PASCAL and CALTECH, the capability of such a representation in capturing the essential object content of the image is not well-understood: How large is the equivalence class of images sharing the same HOG descriptor? Are all these images from the same object category, and if not, do the non-category images resemble random images which cannot generically arise from imaged scenes? How frequently do images from two categories share the same HOG-based representation? These questions are increasingly more relevant as very large databases such as ImageNet and LabelMe are being developed where the current object recognition strategies show limited success. We examine these questions by introducing the metameric class of moments of HOG which allows for a target image to be morphed into an impostor image sharing the HOG representation of a source image while retaining the initial visual appearance. We report that two distinct images can be made to share the same HOG representation when the overlap between HOG patches is minimal, and the success of this method falls with increasing overlap. This paper is therefore a step in the direction of developing a sampling theorem for representing images by HOG features.

Bicycle chain shape models
In this paper we introduce landmark-based pre-shapes which allow mixing of anatomical landmarks and pseudo-landmarks, constraining consecutive pseudo-landmarks to satisfy planar equidistance relations. This defines naturally a structure of Riemannian manifold on these preshapes, with a natural action of the group of planar rotations. Orbits define the shapes. We develop a geodesic generalized procrustes analysis procedure for a sample set on such a preshape spaces and use it to compute principal geodesic analysis. We demonstrate it on an elementary synthetic example as well on a dataset of manually annotated vertebra shapes from x-ray. We re-landmark them consistently and show that PGA captures the variability of the dataset better than its linear counterpart, PCA.

A novel active contour model for texture segmentation
Affine interpolation in a lie group framework
Affine transformations are of vital importance in many tasks pertaining to motion design and animation. Interpolation of affine transformations is non-trivial. Typically, the given affine transformation is decomposed into simpler components which are easier to interpolate. This may lead to unintuitive results, while in some cases, such solutions may not work. In this work, we propose an interpolation framework which is based on a Lie group representation of the affine transformation. The Lie group representation decomposes the given transformation into simpler and meaningful components, on which computational tools like the exponential and logarithm maps are available in closed form. Interpolation exists for all affine transformations while preserving a few characteristics of the original transformation. A detailed analysis and several experiments of the proposed framework are included.

Lie bodies based 3d shape morphing and interpolation
Morphing and Interpolation algorithms aim to support the animator by automating animation in two scenarios. While morphing algorithms generate intermediate objects given two (or more) objects belonging to different classes, interpolation generates intermediate objects given two deformations of the same object.
 In this paper, we propose a framework for morphing and interpolation based on the Lie Bodies representation of triangular meshes. Without any physics based constraints on allowable deformations, the Lie group of transformations involved are able to handle both, morphing as well as interpolation. The Lie Bodies framework is known to fail in case of large deformations. In such cases, we propose to segment the mesh and use the Lie Bodies framework on individual components. Our segmentation scheme is based on detecting parts of meshes undergoing large deformations. The Lie bodies framework is thus able to handle large deformations, is able to produce any intermediate interpolation result directly, and is efficient due to the independent treatment of triangles in the mesh. We provide several interpolation and morphing results in support of our framework.

Active contour models for manifold valued image segmentation
A graph downsampling technique based on graph fourier transform
In this paper, we provide a Graph Fourier Transform based approach to downsample signals on graphs. For bandlimited signals on a graph, a test is provided to identify whether signal reconstruction is possible from the given downsampled signal. Moreover, if the signal is not bandlimited, we provide a quality measure for comparing different downsampling schemes. Using this quality measure, we propose a greedy downsampling algorithm. Most of the prevailing approaches consider undirected graphs, and exploit the topological properties of the graph in order to downsample the grid, while the proposed method exploits spectral properties of graph signals, and is applicable to directed graphs, undirected graphs, and graphs with negative edge-weights. We provide several experiments demonstrating our downsampling scheme, and compare our quality measure with measures like normalized cuts.

On restricting planar curve evolution to finite dimensional implicit subspaces with non-euclidean metric
Signal processing on graphs: structure preserving maps
Signal processing on graphs using adjacency matrix (as opposed to more traditional graph Laplacian) results in an algebraic framework for graph signals and shift invariant filters. This can be seen as an example of the algebraic signal processing theory. In this study, the authors examine the concepts of homomorphism and isomorphism between two graphs from a signal processing point of view and refer to them as GSP isomorphism and GSP homomorphism, respectively. Collectively, they refer to these concepts as structure preserving maps (SPMs). The fact that linear combination of signals and linear transforms on signals are meaningful operations has implications on the GSP isomorphism and GSP homomorphism, which diverges from the topological interpretations of the same concepts (i.e. graph isomorphism and graph homomorphism). When SPMs exist between two graphs, signals and filters can be mapped between them while preserving spectral properties. They examine conditions on adjacency matrices for such maps to exist. They also show that isospectral graphs form a special case of GSP isomorphism and that GSP isomorphism and GSP homomorphism is intrinsic to resampling and downsampling process.

Downsampling on bipartite graphs: An algebraic perspective
Research on downsampling of signals on graphs has gained momentum in recent years. As there is no natural ordering of vertices for arbitrary graphs, the downsampling process is not trivial. In this paper, we show that downsampling process on bipartite graphs share many properties, such as spectral and aliasing behavior, with that for classical signals. We establish optimality of bipartite graph downsampling using various optimization parameters as well as using different existing algorithms for downsampling. We also examine the bipartite graphs in the framework of Algebraic Signal Processing Theory for graphs and derive key properties for algebra of filters and module of signals.

Curve evolution in subspaces
Interval-Based Least Squares for Uncertainty-Aware Learning in Human-Centric Multimedia Systems
Machine learning (ML) methods are popular in several application areas of multimedia signal processing. However, most existing solutions in the said area, including the popular least squares, rely on penalizing predictions that deviate from the target ground-truth values. In other words, uncertainty in the ground-truth data is simply ignored. As a result, optimization and validation overemphasize a single-target value when, in fact, human subjects themselves did not unanimously agree to it. This leads to an unreasonable scenario where the trained model is not allowed the benefit of the doubt in terms of prediction accuracy. The problem becomes even more significant in the context of more recent human-centric and immersive multimedia systems where user feedback and interaction are influenced by higher degrees of freedom (leading to higher levels of uncertainty in the ground truth). To ameliorate this drawback, we propose an uncertainty aware loss function (referred to as $\text {MSE}^{*}$ ) that explicitly accounts for data uncertainty and is useful for both optimization (training) and validation. As examples, we demonstrate the utility of the proposed method for blind estimation of perceptual quality of audiovisual signals, panoramic images, and images affected by camera-induced distortions. The experimental results support the theoretical ideas in terms of reducing prediction errors. The proposed method is also relevant in the context of more recent paradigms, such as crowdsourcing, where larger uncertainty in ground truth is expected.

Analysis of Downsampling of DCT-Graphs
The discrete cosine transform (DCT) finds variety of applications in the field of signal processing. Due to its significant data compression properties, it forms a key component of image compression algorithms. Algebraic Signal Processing theory provides a graph representation for the DCT. In this representation, the DCT becomes a special case of Graph Fourier Transform. In this paper, we analyze the DCT in the context of downsampling using various graph-downsampling techniques. We provide proofs for various downsampling outcomes and validate our findings with experiments on 1-D and 2-D data.

Estimating graph topology from sparse graph signals with an application to image denoising
Graph signal processing is a framework that allows us to work with general unstructured discrete data that cannot be handled with classical Discrete signal processing. The underlying graph topology plays a crucial role in determining the definition of Fourier transform on graphs. Graph topology for a given graph signal is not always available and may also not be unique. In this paper we address the problem of estimating graph topology from signals that are sparse in the frequency domain. We estimate the graph Laplacian matrix in an optimization framework that minimizes errors in relations known to exist between the graph signals and their Fourier transforms. We also propose to use this algorithm for adapting an existing graph based non-local image denoising algorithm, which is known to perform well only for piece-wise smooth images. We provide results on natural, texture and smooth images that support our claim that with our topology estimation algorithm the denoising algorithm is able to adapt to different image structures. We compare our results with the graph based non-local method and the state-of-art BM3D algorithm, using different performance measures.

Affine Interpolation in a Lie Group Framework
Affine transformations are of vital importance in many tasks pertaining to motion design and animation. Interpolation of affine transformations is non-trivial. Typically, the given affine transformation is decomposed into simpler components which are easier to interpolate. This may lead to unintuitive results, while in some cases, such solutions may not work. In this work, we propose an interpolation framework which is based on a Lie group representation of the affine transformation. The Lie group representation decomposes the given transformation into simpler and meaningful components, on which computational tools like the exponential and logarithm maps are available in closed form. Interpolation exists for all affine transformations while preserving a few characteristics of the original transformation. A detailed analysis and several experiments of the proposed framework are included.

Lie Bodies Based Deformation Transfer
In computer animation, meshes corresponding to different objects may have to undergo similar deformations. Deformation transfer tries to mimic the deformation between given source mesh examples, onto given target meshes. In this paper, we propose a deformation transfer approach based on the Lie bodies representation of triangular meshes. The approach does not assume that the target and source meshes are of the same resolution or are registered. Due to the Lie bodies representation, the non-linearity in the space of transformations is accurately modeled which enables the framework to handle large deformations. Furthermore, under certain assumptions stated in the paper, the approach can also be used for the task of pose transfer.

Tangled Splines
Extracting shape information from object boundaries is a well studied problem in vision, and has found tremendous use in applications like object recognition. Conversely, studying the space of shapes represented by curves satisfying certain constraints is also intriguing. In this paper, we model and analyze the space of shapes represented by a 3D curve (space curve) formed by connecting n pieces of quarter of a unit circle. Such a space curve is what we call a Tangle, the name coming from a toy built on the same principle. We provide two models for the shape space of n-link open and closed tangles, and we show that tangles are a subset of trigonometric splines of a certain order. We give algorithms for curve approximation using open/closed tangles, computing geodesics on these shape spaces, and to find the deformation that takes one given tangle to another given tangle, i.e., the Log map. The algorithms provided yield tangles upto a small and acceptable tolerance, as shown by the results given in the paper.

A Novel Active Contour Model for Texture Segmentation
Differential geometry and image processing
Curve Evolution in Subspaces and Exploring the Metameric Class of Histogram of Gradient Orientation based Features using Nonlinear Projection Methods
DONE